<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MetaMedium: AI Beyond Chat</title>
    <meta name="description" content="A Framework for Diagrammatic Human-AI Communication">

    <!-- Open Graph / Social Share -->
    <meta property="og:title" content="MetaMedium: AI Beyond Chat">
    <meta property="og:description" content="A Framework for Diagrammatic Human-AI Communication">
    <meta property="og:image" content="./Assets/thumb-metamedium.jpg">
    <meta property="og:type" content="article">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="MetaMedium: AI Beyond Chat">
    <meta name="twitter:description" content="A Framework for Diagrammatic Human-AI Communication">
    <meta name="twitter:image" content="./Assets/thumb-metamedium.jpg">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,400;0,9..40,500;0,9..40,600;0,9..40,700;1,9..40,400;1,9..40,500&family=Space+Grotesk:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        /* ===========================================
           MetaMedium Design System v4
           Warm Palette · Space Grotesk Headings
           WCAG AA Compliant
        =========================================== */
        
        :root {
            --ink: #1a1a2e;
            --ink-soft: #2d2d3a;
            --paper: #f8f6f1;
            --paper-warm: #f3f1ec;
            --accent: #e63946;
            --accent-hover: #c62f3b;
            --muted: #4a4a5a;
            --muted-light: #6b7280;
            --highlight: #fef3c7;
            --sketch-blue: #3b82f6;
            --sketch-green: #22c55e;
            --sketch-purple: #8b5cf6;
            --border: #d1d5db;
            --shadow: rgba(0,0,0,0.08);
            --focus: #3b82f6;
            
            --text-xs: 0.8125rem;
            --text-sm: 0.9375rem;
            --text-base: 1.125rem;
            --text-lg: 1.3125rem;
            --text-xl: 1.625rem;
            --text-2xl: 2rem;
            --text-3xl: 2.5rem;
            
            --space-xs: 0.375rem;
            --space-sm: 0.75rem;
            --space-md: 1.25rem;
            --space-lg: 2rem;
            --space-xl: 3rem;
            --space-2xl: 4.5rem;
        }
        
        *, *::before, *::after { margin: 0; padding: 0; box-sizing: border-box; }
        
        html { scroll-behavior: smooth; font-size: 100%; }
        
        @media (prefers-reduced-motion: reduce) {
            html { scroll-behavior: auto; }
            *, *::before, *::after {
                animation-duration: 0.01ms !important;
                animation-iteration-count: 1 !important;
                transition-duration: 0.01ms !important;
            }
        }
        
        body {
            font-family: 'DM Sans', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: var(--text-base);
            font-weight: 450;
            line-height: 1.7;
            color: var(--ink);
            background: var(--paper);
            -webkit-font-smoothing: antialiased;
        }
        
        .skip-link {
            position: absolute;
            top: -100%;
            left: 50%;
            transform: translateX(-50%);
            background: var(--ink);
            color: var(--paper);
            padding: var(--space-sm) var(--space-md);
            border-radius: 0 0 8px 8px;
            font-weight: 600;
            z-index: 9999;
        }
        .skip-link:focus { top: 0; }
        
        :focus-visible { outline: 3px solid var(--focus); outline-offset: 3px; }
        
        /* Hero - Blueprint Canvas (Dark/Inverted) */
        .hero {
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            text-align: center;
            padding: var(--space-lg) var(--space-sm) 0;
            position: relative;
            overflow: hidden;
            background: #0a1628;
        }
        
        /* Gradient transition to content - simple fade */
        .hero::after {
            content: '';
            position: absolute;
            bottom: 0; left: 0; right: 0;
            height: 40px;
            background: linear-gradient(to bottom, transparent 0%, var(--paper) 100%);
            z-index: 3;
            pointer-events: none;
        }
        
        #heroCanvas {
            position: absolute;
            top: 0; left: 0;
            width: 100%; height: 100%;
            z-index: 1;
            cursor: crosshair;
        }
        
        .hero-hint {
            position: absolute;
            bottom: 9rem;
            left: 50%;
            transform: translateX(-50%);
            z-index: 2;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            font-family: 'DM Sans', sans-serif;
            font-size: 0.9rem;
            font-weight: 500;
            color: rgba(147, 197, 253, 0.4);
            text-transform: uppercase;
            letter-spacing: 0.15em;
            transition: opacity 0.3s;
            pointer-events: none;
        }
        .hero-hint .hint-icon {
            font-size: 0.9rem;
            animation: pencil-wiggle 2s ease-in-out infinite;
        }
        @keyframes pencil-wiggle {
            0%, 100% { transform: rotate(-5deg); }
            50% { transform: rotate(5deg); }
        }
        .hero.drawing .hero-hint { opacity: 0; }
        
        .hero-content {
            position: relative;
            z-index: 2;
            max-width: 800px;
            display: flex;
            flex-direction: column;
            align-items: center;
            pointer-events: none; /* Allow drawing through */
        }
        
        .sr-only {
            position: absolute;
            width: 1px; height: 1px;
            padding: 0; margin: -1px;
            overflow: hidden;
            clip: rect(0, 0, 0, 0);
            white-space: nowrap;
            border: 0;
        }
        
        .hero h1 {
            font-family: 'Space Grotesk', -apple-system, sans-serif;
            font-size: clamp(3rem, 10vw, 5.5rem);
            font-weight: 700;
            letter-spacing: -0.03em;
            line-height: 1;
            margin-bottom: var(--space-sm);
            color: #ebebeb;
        }
        
        .hero h1 .meta {
            background: linear-gradient(135deg, #f97316 0%, #a855f7 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .hero .tagline {
            font-family: 'DM Sans', sans-serif;
            font-size: var(--text-base);
            font-weight: 500;
            color: rgba(147, 197, 253, 0.7);
            letter-spacing: 0.45em;
            text-transform: uppercase;
            margin-bottom: var(--space-sm);
        }
        
        .hero .subtitle {
            font-family: 'DM Serif Display', Georgia, serif;
            font-size: clamp(1.1rem, 2.2vw, 1.4rem);
            font-weight: 400;
            font-style: italic;
            color: rgba(255, 255, 255, 0.651);
            max-width: 550px;
            margin: 0 auto var(--space-sm);
            line-height: 1.4;
        }
        
        .hero .byline { 
            font-size: var(--text-sm); 
            color: rgba(255, 255, 255, 0.75); 
            font-weight: 500;
        }
        .hero .byline .role { 
            color: rgba(255, 255, 255, 0.45);
            font-weight: 400;
            margin-left: 0.25em;
        }
        
        .scroll-indicator {
            position: absolute;
            /*bottom: var(--space-xl); */
            bottom: 11rem;
            z-index: 4;
            animation: bounce 2s infinite;
            cursor: pointer;
            text-decoration: none;
            display: block;
        }
        .scroll-indicator svg { 
            width: 35px; 
            height: 35px; 
            stroke-width: 2.5;
            animation: strokeGlow 0.8s ease-in-out infinite alternate;
        }
        
        @keyframes bounce {
            0%, 20%, 50%, 80%, 100% { transform: translateY(0); }
            40% { transform: translateY(-8px); }
            60% { transform: translateY(-4px); }
        }
        
        @keyframes strokeGlow {
            0% { stroke: rgba(147, 197, 253, 0.3); }
            100% { stroke: rgba(147, 197, 253, 0.9); }
        }
        
        /* Navigation - Slim with left title */
        nav {
            position: fixed;
            top: 0; left: 0; right: 0;
            background: rgba(253, 252, 250, 0.97);
            backdrop-filter: blur(12px);
            z-index: 1000;
            border-bottom: 1px solid var(--border);
            transform: translateY(-100%);
            transition: transform 0.25s ease;
            height: 40px;
        }
        nav.visible { transform: translateY(0); }
        
        .nav-inner {
            display: flex;
            align-items: center;
            justify-content: space-between;
            max-width: 1100px;
            margin: 0 auto;
            padding: 0 var(--space-md);
            height: 100%;
        }
        
        .nav-title {
            font-family: 'Space Grotesk', -apple-system, sans-serif;
            font-size: 1rem;
            font-weight: 600;
            color: var(--ink);
            text-decoration: none;
            letter-spacing: -0.02em;
            line-height: 1;
        }
        .nav-title:hover { color: var(--accent); }
        
        /* Mobile nav toggle */
        .nav-toggle {
            display: none;
            background: none;
            border: none;
            cursor: pointer;
            padding: 0.5rem;
            margin-right: -0.5rem;
        }
        .nav-toggle span {
            display: block;
            width: 20px;
            height: 2px;
            background: var(--ink);
            margin: 4px 0;
            transition: transform 0.2s, opacity 0.2s;
        }
        .nav-toggle.active span:nth-child(1) { transform: rotate(45deg) translate(4px, 4px); }
        .nav-toggle.active span:nth-child(2) { opacity: 0; }
        .nav-toggle.active span:nth-child(3) { transform: rotate(-45deg) translate(4px, -4px); }
        
        nav ul {
            display: flex;
            align-items: center;
            gap: 0;
            list-style: none;
            margin: 0;
            padding: 0;
            height: 100%;
        }
        
        nav li {
            display: flex;
            align-items: center;
            height: 100%;
            margin: 0;
        }
        
        nav a {
            font-family: 'DM Sans', sans-serif;
            font-size: 0.6875rem;
            font-weight: 600;
            text-decoration: none;
            color: var(--muted);
            text-transform: uppercase;
            letter-spacing: 0.06em;
            padding: 0 0.625rem;
            display: flex;
            align-items: center;
            height: 100%;
            transition: color 0.2s;
        }
        nav ul a:hover, nav ul a:focus { color: var(--accent); }
        
        /* Mobile nav styles */
        @media (max-width: 900px) {
            .nav-toggle {
                display: block;
                position: relative;
                z-index: 1001;
            }

            nav ul {
                position: absolute;
                top: 10px;
                left: 0;
                right: 0;
                background: rgba(253, 252, 250, 0.98);
                backdrop-filter: blur(12px);
                flex-direction: column;
                align-items: stretch;
                height: auto;
                padding: var(--space-md) 0;
                border-bottom: 1px solid var(--border);
                display: none;
                box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
            }
            
            nav ul.open { display: flex; }
            
            nav li {
                height: auto;
            }
            
            nav ul a {
                padding: .5rem var(--space-sm);
                height: auto;
                font-size: 1rem;
                font-weight: 800;
                letter-spacing: 0.02em;
                text-transform: none;
            }
        }
        
        /* Main Content */
        main {
            max-width: 760px;
            margin: 0 auto;
            padding: var(--space-lg) var(--space-lg) var(--space-xl);
        }
        
        /* Overview/Abstract section */
        #abstract {
            scroll-margin-top: 0;
        }
        .overview-box {
            padding: var(--space-lg);
            background: linear-gradient(135deg, #f0f9ff 0%, #fef3c7 100%);
            border-left: 4px solid var(--accent);
            border-radius: 0 8px 8px 0;
            margin-top: var(--space-sm);
        }
        .overview-box p {
            font-size: var(--text-lg);
            font-style: italic;
            line-height: 1.65;
            color: var(--ink-soft);
        }
        .overview-box p:first-child {
            font-weight: 500;
        }
        .overview-box p:last-child {
            margin-bottom: 0;
        }
        
        section {
            margin-bottom: var(--space-xl);
            scroll-margin-top: 4rem;
        }
        
        h2 {
            font-family: 'Space Grotesk', -apple-system, sans-serif;
            font-size: clamp(1.875rem, 4vw, var(--text-3xl));
            font-weight: 600;
            margin-bottom: var(--space-md);
            letter-spacing: -0.02em;
            line-height: 1.2;
            position: relative;
            padding-left: 1.25rem;
        }
        
        h2::before {
            content: '';
            position: absolute;
            left: 0;
            top: 0;
            bottom: 0;
            width: 5px;
            background: var(--sketch-blue);
            border-radius: 2px;
        }
        
        h3 {
            font-family: 'Space Grotesk', -apple-system, sans-serif;
            font-size: var(--text-xl);
            font-weight: 500;
            margin: var(--space-md) 0 var(--space-sm);
            line-height: 1.25;
        }
        
        h4 {
            font-family: 'Space Grotesk', -apple-system, sans-serif;
            font-size: var(--text-lg);
            font-weight: 600;
            margin: var(--space-md) 0 var(--space-xs);
            letter-spacing: -0.01em;
        }
        
        p {
            margin-bottom: var(--space-md);
            max-width: 65ch;
        }
        
        strong { font-weight: 600; color: var(--ink); }
        em { font-style: italic; }
        
        a {
            color: var(--accent);
            text-decoration: underline;
            text-underline-offset: 2px;
            text-decoration-thickness: 1px;
        }
        a:hover { color: var(--accent-hover); }
        
        /* Blockquotes - v1 elegant style with gradient */
        blockquote {
            font-family: 'DM Sans', sans-serif;
            font-size: 1.3rem;
            font-weight: 400;
            font-style: italic;
            line-height: 1.6;
            color: var(--muted);
            border-left: 3px solid var(--sketch-blue);
            padding: var(--space-md) var(--space-md) var(--space-md) var(--space-lg);
            margin: var(--space-lg) 0;
            background: linear-gradient(to bottom, rgba(59, 130, 246, 0.08) 0%, transparent 100%);
            border-radius: 0 8px 8px 0;
        }
        
        blockquote cite {
            display: block;
            font-family: 'DM Sans', sans-serif;
            font-style: normal;
            font-size: var(--text-sm);
            font-weight: 500;
            color: var(--muted-light);
            margin-top: var(--space-sm);
        }
        blockquote cite em { font-style: italic; }
        
        ul, ol {
            margin-bottom: var(--space-md);
            padding-left: var(--space-md);
        }
        
        li {
            margin-bottom: var(--space-xs);
            line-height: 1.65;
        }
        
        /* Figures */
        /*figure { margin: var(--space-lg) 0; }
        */
    figure {
            margin: 3rem 0;
            background: #fff;
            border: 1px solid var(--border);
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 4px 20px rgba(0,0,0,0.05);
        }
        
        /* Two-column figures */
           .figure-pair {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            margin: 1.5rem 0;
        }

        .figure-pair figure {
            padding: var(--space-xs) 0;
        }

        @media (max-width: 768px) {
            .figure-pair {
                grid-template-columns: 1fr;
            }
        }

        .placeholder-figure {
            background: linear-gradient(135deg, var(--paper-warm) 0%, #fff 100%);
            border: 2px dashed var(--border);
            border-radius: 10px;
            padding: var(--space-md);
            text-align: left;
            display: flex;
            flex-direction: column;
            align-items: stretch;
            justify-content: left;
            min-height: 220px;
        }

        .placeholder-figure .icon { font-size: 3rem; margin-bottom: var(--space-sm); opacity: 0.5; }
        .placeholder-figure .label {
            font-family: 'DM Sans', sans-serif;
            font-size: var(--text-sm);
            font-weight: 600;
            color: var(--muted);
            text-transform: uppercase;
            letter-spacing: 0.1em;
            margin-bottom: var(--space-xs);
            width: 100%;
        }
        .placeholder-figure .description {
            font-size: var(--text-base);
            color: var(--muted);
            line-height: 1.5;
            width: 100%;
        }

        .placeholder-figure .image {
            width: 100%;
            margin-bottom: var(--space-md);
        }

        .placeholder-figure .image svg,
        .placeholder-figure .image img {
            width: 100%;
            height: auto;
            display: block;
        }

        figure img {
            width: 100%;
            display: block;
        }
        
        figure.sketch-figure {
            background: #fafafa;
            border: 2px dashed var(--border);
        }
        
        figcaption {
            padding: 1rem 1.5rem;
            font-size: 0.95rem;
            color: var(--muted);
            border-top: 1px solid var(--border);
            font-style: italic;
        }
        
        figcaption strong {
            color: var(--ink);
            font-style: normal;
        }
        /*
        figcaption {
            font-size: var(--text-sm);
            color: var(--muted);
            margin-top: var(--space-sm);
            line-height: 1.6;
        }
        figcaption strong { color: var(--ink); }
        *?
        /* Video Embed */
        .video-embed {
            position: relative;
            padding-bottom: 56.25%;
            height: 0;
            overflow: hidden;
            margin: var(--space-md) 0;
            border-radius: 10px;
            box-shadow: 0 3px 20px var(--shadow);
        }
        .video-embed iframe {
            position: absolute;
            top: 0; left: 0;
            width: 100%; height: 100%;
            border: none;
        }
        
        /* Demo Container */
        .demo-container {
            margin: var(--space-lg) 0;
            border: 2px solid var(--sketch-blue);
            border-radius: 12px;
            overflow: hidden;
            background: #fff;
        }
        .demo-header {
            background: linear-gradient(135deg, var(--sketch-blue), var(--sketch-green));
            color: white;
            padding: var(--space-sm) var(--space-md);
            font-family: 'Space Grotesk', sans-serif;
            font-weight: 600;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .demo-header span {
            font-size: var(--text-sm);
            opacity: 0.9;
            font-weight: 400;
        }
        .demo-container iframe {
            width: 100%;
            height: 640px;
            border: none;
            display: block;
        }
        
        /* Callout */
        .callout {
            background: var(--paper-warm);
            border-radius: 10px;
            padding: var(--space-md);
            margin: var(--space-md) 0;
            border-left: 4px solid var(--sketch-blue);
        }
        .callout.insight { border-left-color: var(--sketch-purple); }
        .callout.key { border-left-color: var(--accent); }
        .callout h4 { margin-top: 0; font-size: var(--text-base); }
        .callout p { font-size: var(--text-base); line-height: 1.65; }
        .callout p:last-child { margin-bottom: 0; }
        
        /* Use Cases - v1 gradient style */
        .use-case {
            background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
            border-radius: 8px;
            padding: var(--space-md);
            margin: var(--space-md) 0;
            border-left: 4px solid var(--sketch-green);
        }
        .use-case h4 {
            margin-top: 0;
            color: var(--sketch-green);
            font-size: var(--text-sm);
            text-transform: uppercase;
            letter-spacing: 0.06em;
        }
        .use-case .scenario { font-style: italic; font-size: var(--text-base); line-height: 1.65; margin-bottom: var(--space-sm); }
        .use-case .capabilities { font-size: var(--text-sm); color: var(--muted); margin-bottom: 0; }
        
        /* Genealogy */
        .genealogy-item {
            display: grid;
            grid-template-columns: 70px 1fr;
            gap: var(--space-md);
            margin: var(--space-lg) 0;
            padding: 0;
            align-items: start;
        }
        .genealogy-item .year {
            font-family: 'Space Grotesk', sans-serif;
            font-size: var(--text-base);
            font-weight: 600;
            color: var(--accent);
            line-height: 1.4;
            padding-top: 2px;
        }
        .genealogy-item .content h4 { margin-top: 0; font-size: var(--text-lg); line-height: 1.25; }
        .genealogy-item .content p { font-size: var(--text-base); line-height: 1.65; }
        .genealogy-item .content p:last-of-type { margin-bottom: 0; }
        
        /* Media placeholder for timeline */
        .media-placeholder {
            margin-top: var(--space-sm);
            border-radius: 8px;
            overflow: hidden;
            background: linear-gradient(135deg, rgba(59, 130, 246, 0.08) 0%, rgba(139, 92, 246, 0.08) 100%);
            border: 1px dashed rgba(59, 130, 246, 0.3);
            position: relative;
        }
        .media-placeholder.video {
            aspect-ratio: 16/9;
        }
        .media-placeholder.image {
            aspect-ratio: 4/3;
        }
        .media-placeholder-inner {
            position: absolute;
            inset: 0;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            color: var(--muted-light);
            font-size: var(--text-sm);
            gap: var(--space-xs);
        }
        .media-placeholder-inner .icon {
            font-size: 1.5rem;
            opacity: 0.6;
        }
        .media-placeholder-inner .label {
            font-weight: 500;
            opacity: 0.8;
        }
        .media-placeholder-inner .caption {
            font-size: var(--text-xs);
            opacity: 0.6;
            text-align: center;
            max-width: 80%;
        }
        
        /* Embedded video in timeline */
        .timeline-video {
            margin-top: var(--space-sm);
            border-radius: 8px;
            overflow: hidden;
            aspect-ratio: 16/9;
            background: #000;
        }
        .timeline-video iframe {
            width: 100%;
            height: 100%;
            border: none;
        }
        
        /* Embedded image in timeline */
        .timeline-image {
            margin-top: var(--space-sm);
            border-radius: 8px;
            overflow: hidden;
        }
        .timeline-image img {
            width: 100%;
            height: auto;
            display: block;
        }
        
        /* Gallery grid */
        .gallery {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: var(--space-xs);
            margin: var(--space-xs) 0;
        }
        .gallery-item {
            background: rgba(74, 85, 104, 0.08);
            border: 1px solid rgba(107, 114, 128, 0.4);
            border-radius: 4px;
            padding: var(--space-xs) var(--space-xs);
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
            margin: 0;  /* Override default figure margin */
        }
        .gallery-item .image {
            width: 100%;
            aspect-ratio: 16 / 10;
            margin-bottom: var(--space-xs);
            background: rgba(10, 14, 26, 0.9);
            border-radius: 2px;
            overflow: hidden;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s ease;
        }
        .gallery-item .image:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
            background: rgba(10, 14, 26, 1);
        }
        .gallery-item .image img {
            width: 100%;
            height: 100%;
            object-fit: contain;
            display: block;
        }
        .gallery-item figcaption {
            font-size: var(--text-sm);
            color: rgba(51, 51, 51, 0.9);
            line-height: 1.4;
            width: 100%;
            font-weight: 500;
        }

        @media (max-width: 768px) {
            .gallery {
                grid-template-columns: repeat(2, 1fr);
                gap: var(--space-sm);
            }
            .gallery-item .image {
                aspect-ratio: 4 / 3;  /* Taller ratio for smaller screens */
            }
        }
        @media (max-width: 480px) {
            .gallery {
                grid-template-columns: 1fr;
            }
            .gallery-item .image {
                aspect-ratio: 16 / 10;  /* Back to wider for full-width */
            }
        }
        
        /* Principles - v1 watermark + v3 stripe */
        .principles-grid { display: grid; gap: var(--space-sm); margin: var(--space-md) 0; }
        
        .principle {
            background: #fff;
            border-radius: 8px;
            padding: var(--space-lg);
            border: 1px solid var(--border);
            position: relative;
            overflow: hidden;
        }
        .principle::before {
            content: attr(data-num);
            position: absolute;
            top: -15px;
            right: 15px;
            font-family: 'Space Grotesk', sans-serif;
            font-size: 5rem;
            font-weight: 700;
            color: rgba(26, 26, 46, 0.04);
            line-height: 1;
            pointer-events: none;
        }
        .principle::after {
            content: '';
            position: absolute;
            top: 0; left: 0;
            width: 4px; height: 100%;
            background: linear-gradient(180deg, var(--sketch-blue), var(--sketch-green));
        }
        .principle h4 {
            margin-top: 0;
            font-family: 'Space Grotesk', sans-serif;
            font-size: var(--text-lg);
            font-weight: 600;
        }
        .principle p { font-size: var(--text-base); line-height: 1.65; }
        .principle p:last-child { margin-bottom: 0; }
        .principle-example {
            font-size: var(--text-sm);
            color: var(--muted);
            border-left: 2px solid var(--border);
            padding-left: var(--space-sm);
            margin-top: var(--space-xs);
        }
        
        /* Practice use cases - principles label */
        .principles-used {
            font-size: var(--text-xs);
            color: var(--sketch-blue);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: var(--space-xs);
        }
        .principles-used strong {
            color: var(--ink-muted);
            font-weight: 500;
        }
        
        /* Video fallback links */
        .video-link {
            font-size: var(--text-sm);
            color: var(--ink-muted);
            margin-top: var(--space-xs);
        }
        .video-link a {
            color: var(--accent);
        }
        
        /* Footer */
        footer {
            background: var(--ink);
            color: #fff;
            text-align: center;
            padding: var(--space-xl) var(--space-md);
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
        }
        .footer-title {
            font-family: 'Space Grotesk', -apple-system, sans-serif;
            font-size: var(--text-lg);
            font-weight: 600;
            margin-bottom: var(--space-xs);
        }
        .footer-subtitle {
            font-size: var(--text-sm);
            opacity: 0.65;
            margin-bottom: var(--space-md);
        }
        .footer-copyright {
            font-size: var(--text-sm);
            opacity: 0.8;
            margin-bottom: var(--space-xs);
        }
        footer a { 
            color: #fff; 
            opacity: 0.8;
            text-decoration: underline;
            text-underline-offset: 2px;
        }
        footer a:hover { opacity: 1; }
        footer .version {
            font-family: 'DM Sans', sans-serif;
            font-size: var(--text-xs);
            font-weight: 500;
            opacity: 0.45;
            margin-top: var(--space-sm);
            letter-spacing: 0.05em;
        }

        /* Demo Canvas - Fish Animation */
        .demo-wrapper {
            margin: var(--space-lg) 0 var(--space-md);
        }

        .demo-prompt {
            text-align: center;
            margin-bottom: 1rem;
            font-size: 1.25rem;
            color: var(--ink);
            font-weight: 500;
            min-height: 2rem;
        }

        .demo-prompt .highlight {
            color: #e63946;
            font-weight: 600;
        }

        .demo-canvas-container {
            position: relative;
            background: #fff;
            border-radius: 12px;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            overflow: hidden;
        }

        #demoCanvas {
            display: block;
            width: 100%;
            aspect-ratio: 4/3;
            cursor: crosshair;
            touch-action: none;
        }

        #demoCanvas.grabbing {
            cursor: grabbing;
        }

        .demo-labels {
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            pointer-events: none;
        }

        .demo-label {
            position: absolute;
            font-size: 0.75rem;
            font-weight: 600;
            padding: 0.25rem 0.5rem;
            border-radius: 4px;
            transform: translate(-50%, -100%);
            margin-top: -8px;
            opacity: 0;
            transition: opacity 0.3s ease;
        }

        .demo-label.visible {
            opacity: 1;
        }

        .demo-label.fish {
            background: rgba(59, 130, 246, 0.9);
            color: white;
        }

        .demo-label.food {
            background: rgba(74, 222, 128, 0.9);
            color: #1a1a2e;
        }

        .demo-controls {
            display: flex;
            justify-content: center;
            margin-top: 1rem;
            gap: 0.75rem;
        }

        .demo-reset-btn {
            font-family: inherit;
            font-size: 0.875rem;
            font-weight: 500;
            padding: 0.625rem 1.25rem;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.2s ease;
            background: #1a1a2e;
            color: white;
            opacity: 0;
            pointer-events: none;
        }

        .demo-reset-btn.visible {
            opacity: 1;
            pointer-events: auto;
        }

        .demo-reset-btn:hover {
            background: #2d2d4a;
        }

        .demo-status {
            text-align: center;
            margin-top: 0.75rem;
            font-size: 0.8125rem;
            color: #666;
            min-height: 1.25rem;
        }

        /* Image Modal / Lightbox */
        .image-modal {
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            z-index: 10000;
            display: none;
            align-items: center;
            justify-content: center;
            opacity: 0;
            transition: opacity 0.3s ease;
        }

        .image-modal.active {
            display: flex;
            opacity: 1;
        }

        .modal-backdrop {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(15, 15, 20, 0.92);
            cursor: pointer;
        }

        .modal-content {
            position: relative;
            z-index: 10001;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
        }

        .modal-image-container {
            width: 90vw;
            height: 80vh;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .modal-image-container img {
            max-width: 100%;
            max-height: 100%;
            object-fit: contain;
            border-radius: 4px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.5);
        }

        .modal-close {
            background: rgba(255, 255, 255, 0.1);
            border: 2px solid rgba(255, 255, 255, 0.3);
            color: white;
            font-size: 1.5rem;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s;
            line-height: 0;
            padding: 0;
            font-family: Arial, sans-serif;
        }

        .modal-close:hover {
            background: rgba(255, 255, 255, 0.2);
            border-color: rgba(255, 255, 255, 0.5);
            transform: scale(1.1);
        }

        .modal-prev,
        .modal-next {
            background: rgba(255, 255, 255, 0.1);
            border: 2px solid rgba(255, 255, 255, 0.3);
            color: white;
            font-size: 1.5rem;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s;
            line-height: 1;
            padding: 0;
            padding-bottom: 2px;
            font-family: Arial, sans-serif;
        }

        .modal-prev:hover:not(:disabled),
        .modal-next:hover:not(:disabled) {
            background: rgba(255, 255, 255, 0.2);
            border-color: rgba(255, 255, 255, 0.5);
            transform: scale(1.1);
        }

        .modal-prev:disabled,
        .modal-next:disabled {
            opacity: 0.3;
            cursor: not-allowed;
        }

        #modalCaption {
            color: white;
            text-align: center;
            margin-top: 1rem;
            font-size: 0.9375rem;
            max-width: 80vw;
            line-height: 1.5;
        }

        .modal-controls {
            display: grid;
            grid-template-columns: 1fr auto 1fr;
            align-items: center;
            width: 90vw;
            margin-top: 0.75rem;
        }

        .modal-nav-group {
            grid-column: 2;
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .modal-close {
            grid-column: 3;
            justify-self: end;
        }

        .modal-counter {
            color: rgba(255, 255, 255, 0.7);
            font-size: 0.875rem;
            font-family: 'JetBrains Mono', monospace;
            min-width: 60px;
            text-align: center;
        }

        @media (max-width: 768px) {
            .modal-image-container {
                width: 95vw;
                height: 70vh;
            }

            .modal-controls {
                width: 95vw;
            }

            .modal-nav-group {
                gap: 0.75rem;
            }
        }

        /* Responsive */
        @media (max-width: 768px) {
            :root { 
                --text-base: 1.0625rem;
                --space-lg: 1.5rem;
                --space-xl: 2.25rem;
                --space-2xl: 3rem;
            }
            main { padding: var(--space-md) var(--space-md) var(--space-xl); }

            nav { height: 36px; }
            .nav-inner { padding: 0 var(--space-sm); }
            .nav-title { font-size: 0.9375rem; }

            .genealogy-item { grid-template-columns: 1fr; gap: var(--space-xs); }
            .genealogy-item .year { margin-bottom: var(--space-xs); }
            
            .media-placeholder.video { aspect-ratio: 16/9; }
            .media-placeholder.image { aspect-ratio: 3/2; }
            .timeline-video { aspect-ratio: 16/9; }
            blockquote { padding-left: var(--space-sm); }
            .hero { padding: var(--space-lg) var(--space-md); }
            .hero-hint { bottom: 7rem; }
        }
        
        @media (max-width: 480px) {
            nav { height: 34px; }
            .nav-title { font-size: 0.875rem; }
            .hero h1 { font-size: clamp(2.5rem, 14vw, 4rem); }
        }
        
        @media (min-width: 1200px) { main { max-width: 800px; } }
        
        @media print {
            body { font-size: 11pt; line-height: 1.5; }
            nav, .scroll-indicator, #heroCanvas, .hero-hint, .demo-wrapper { display: none; }
            .hero { min-height: auto; padding: 1.5rem 0; background: none; }
            .hero::after { display: none; }
            .hero h1, .hero .subtitle, .hero .tagline, .hero .byline { color: var(--ink); }
            .hero h1 .meta { -webkit-text-fill-color: var(--accent); }
        }
    </style>
</head>
<body>
    <a href="#vision" class="skip-link">Skip to main content</a>
    
    <nav id="nav" role="navigation" aria-label="Main navigation">
        <div class="nav-inner">
            <a href="#top" class="nav-title">MetaMedium</a>
            <button class="nav-toggle" aria-label="Toggle navigation" aria-expanded="false">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul>
                <li><a href="#abstract">Overview</a></li>
                <li><a href="#problem">Problem</a></li>
                <li><a href="#vision">Vision</a></li>
                <li><a href="#thesis">Thesis</a></li>
                <li><a href="#framework">Framework</a></li>
                 <li><a href="#lineage">Lineage</a></li>
                <li><a href="#development">Development</a></li>
                <li><a href="#scenarios">Scenarios</a></li>
                <li><a href="#future">Future</a></li>
                <li><a href="#conclusion">Conclusion</a></li>
            </ul>
        </div>
    </nav>

    <header class="hero" id="top">
        <!-- Interactive Blueprint Canvas -->
        <canvas id="heroCanvas"></canvas>
        <div class="hero-hint">
            <span class="hint-icon">✎</span>
            <span class="hint-text">draw here</span>
        </div>
        
        <div class="hero-content">
            <h1><span class="meta">Meta</span>Medium</h1>
            <p class="tagline">beyond chat</p>
            <p class="subtitle">Semantic Drawing as AI Interface</p>
            <p class="byline">John Hanacek <span class="role">Product Designer</span></p>
        </div>
        <a href="#abstract" class="scroll-indicator" aria-label="Scroll to content">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" aria-hidden="true">
                <path d="M7 13l5 5 5-5M7 6l5 5 5-5"/>
            </svg>
        </a>
    </header>

    <main>
        <!-- ABSTRACT -->
        <section>
            <h2 id="abstract">Overview</h2>
            
            <div class="overview-box">
                <p>We face problems at scales our ancestors never imagined—climate systems, global economies, interconnected crises that span generations and continents. These challenges demand new ways of thinking together, yet our tools for shared understanding remain impoverished: text boxes, chat threads, turn-taking dialogues. The gap between the complexity we face and the bandwidth of our interfaces has never been wider.</p>
                
                <p>The MetaMedium proposes that drawing—humanity's oldest form of external thought—can become the foundation for a new kind of human-AI communication. This is not merely a new interface. It is an evolution of language itself, where AI becomes a <em>meta-word</em>—a new linguistic element that transforms marks into meaning-in-context, allowing a seven-year-old and a graduate student to collaborate on the same canvas, each working at their own level, each contributing to shared understanding.</p>
            </div>
        </section>

        <!-- SECTION 1: THE PROBLEM -->
        <section id="problem">
            <h2>The Problem</h2>
          
            
            <h3>Simulating Inert Drawing On The Dynamic Medium</h3>
            
            <p>Today's drawing applications—from Adobe Illustrator to Microsoft OneNote to Apple Notes—share a fundamental limitation: they treat drawings as <em>pixels</em> or <em>vectors</em>, not as <em>meaning</em>. When you draw a circle in Illustrator, it knows geometry but not semantics. When you sketch in OneNote, your marks are captured but never interpreted. Procreate creates beautiful strokes that remain forever inert.</p>
            
            <p>Even sophisticated tools like Figma or Miro treat diagrams as layout, not computation. You can draw a flowchart, but the arrows don't actually <em>flow</em>. You can sketch a state machine, but it doesn't <em>run</em>. The computer faithfully records what you drew without understanding what you meant.</p>
            
            <figure>
                <div class="placeholder-figure">
                    <div class="image"><img src="./Assets/fig-digidraw.svg" alt="Spectrum of Digital Representations"></div>
                    <div class="label">Figure: The Digital Representation Spectrum</div>
                    <div class="description">From "inert" to "dynamic": Photoshop (pixels) → Illustrator (vectors) → OneNote (ink capture) → Figma (components) → Miro (templates) → Chalktalk (behaviors) → tldraw computer (LLM interpretation) → MetaMedium (learning + negotiation). Most tools cluster on the left.</div>
                </div>
            </figure>
            
            <p>This is the dead drawing problem: our most natural form of expression—mark-making—remains computationally inert. We have given computers eyes (computer vision), ears (speech recognition), and voice (text-to-speech), but we haven't given them the ability to <em>think with us through drawing</em>.</p>
            
            <h3>The Communication Bottleneck</h3>
            
            <p>We stand at an inflection point. Large language models have achieved remarkable capabilities in reasoning, generation, and conversation. Yet we interact with them through text boxes—the equivalent of trying to share a symphony by describing it in words.</p>
            
            <figure>
                <div class="placeholder-figure">
                    <div class="image">
                        <svg viewBox="0 0 800 280" xmlns="http://www.w3.org/2000/svg">
                            <!-- Title -->
                            <text x="400" y="25" text-anchor="middle" fill="#7b8a9a" font-size="22" opacity="1">Information Capacity Across Modalities</text>

                            <!-- Panel 1: TEXT (1D) -->
                            <g>
                                <rect x="50" y="50" width="220" height="180" fill="rgba(74, 85, 104, 0.15)" stroke="#4a5568" stroke-width="2" rx="6" opacity="0.5"/>
                                <text x="160" y="75" text-anchor="middle" fill="#6b7280" font-size="18" font-weight="bold">TEXT</text>

                                <!-- Monospace text with cursor -->
                                <text x="75" y="125" fill="#9ca3af" font-family="Monaco, monospace" font-size="16">hello world</text>
                                <rect x="182" y="110" width="2" height="18" fill="#9ca3af" opacity="0.6"/>

                                <!-- Dimension label -->
                                <text x="160" y="165" text-anchor="middle" fill="#6b7280" font-size="15" font-weight="bold">1D: linear sequence</text>

                                <!-- Narrow pipe visualization -->
                                <rect x="140" y="185" width="40" height="10" fill="#4a5568" opacity="0.4" rx="5"/>
                                <text x="160" y="213" text-anchor="middle" fill="#6b7280" font-size="18" opacity="0.7">conceptual</text>
                            </g>

                            <!-- Panel 2: VOICE (1.5D) -->
                            <g transform="translate(260, 0)">
                                <rect x="50" y="50" width="220" height="180" fill="rgba(90, 106, 122, 0.15)" stroke="#5a6a7a" stroke-width="2" rx="6"/>
                                <text x="160" y="75" text-anchor="middle" fill="#7b8a9a" font-size="18" font-weight="bold">VOICE</text>

                                <!-- Waveform with prosody markers -->
                                <path d="M 70 125 Q 85 110, 100 125 T 130 125 Q 145 140, 160 125 T 190 125 Q 205 118, 220 125"
                                      stroke="#7b8a9a" stroke-width="2" fill="none"/>
                                <text x="90" y="105" fill="#7b8a9a" font-size="14">↑</text>
                                <text x="150" y="150" fill="#7b8a9a" font-size="14">↓</text>
                                <text x="200" y="110" fill="#7b8a9a" font-size="14">↑</text>

                                <!-- Dimension label -->
                                <text x="160" y="165" text-anchor="middle" fill="#7b8a9a" font-size="15" font-weight="bold">1.5D: + tone, rhythm</text>

                                <!-- Medium pipe -->
                                <rect x="130" y="185" width="60" height="12" fill="#5a6a7a" opacity="0.5" rx="6"/>
                                <text x="160" y="213" text-anchor="middle" fill="#7b8a9a" font-size="18" opacity="0.8">temporal</text>
                            </g>

                            <!-- Panel 3: DRAWING (2D+) -->
                            <g transform="translate(510, 0)">
                                <rect x="50" y="50" width="220" height="180" fill="7b8a9a" stroke="#64b5f6" stroke-width="2" rx="6" opacity="0.6"/>
                                <text x="160" y="75" text-anchor="middle" fill="#90caf9" font-size="18" font-weight="bold">DRAWING</text>

                                <!-- Rich sketch elements - compact -->
                                <circle cx="110" cy="115" r="15" stroke="#90caf9" stroke-width="2" fill="none"/>
                                <line x1="125" y1="115" x2="160" y2="115" stroke="#90caf9" stroke-width="2" marker-end="url(#arrowblue)"/>
                                <rect x="187" y="100" width="20" height="20" stroke="#90caf9" stroke-width="2" fill="none"/>
                                <circle cx="185" cy="120" r="6" fill="#90caf9"/>
                                <line x1="170" y1="110" x2="170" y2="145" stroke="#90caf9" stroke-width="2" marker-end="url(#arrowblue)"/>
                                <!-- Annotation -->
                                <line x1="180" y1="165" x2="200" y2="165" stroke="#90caf9" stroke-width="1.5" opacity="0.6"/>

                                <!-- Dimension label -->
                                <text x="160" y="159" text-anchor="middle" fill="#64b5f6" font-size="15" font-weight="bold">2D+: space, gesture,</text>
                                <text x="160" y="173" text-anchor="middle" fill="#64b5f6" font-size="15" font-weight="bold">time, relationships</text>

                                <!-- Wide channel -->
                                <rect x="110" y="185" width="100" height="16" fill="#64b5f6" opacity="0.2" rx="8"/>
                                <text x="160" y="213" text-anchor="middle" fill="#64b5f6" font-size="18" opacity="0.99">expressive</text>
                            </g>

                            <!-- Bottom insight -->
                            <text x="400" y="260" text-anchor="middle" fill="#6b7280" font-size="18" font-style="italic">Drawing captures complexity that text and voice alone cannot express</text>

                            <!-- Arrow marker -->
                            <defs>
                                <marker id="arrowblue" markerWidth="8" markerHeight="8" refX="7" refY="2.5" orient="auto" markerUnits="strokeWidth">
                                    <path d="M0,0 L0,5 L7,2.5 z" fill="#90caf9" />
                                </marker>
                            </defs>
                        </svg>
                    </div>
                    <div class="label">Figure: Input channel dimensionality</div>
                    <div class="description">Text box (1D: linear sequence) → Voice (1.5D: sequence + prosody) → Drawing (2D+: space, time, pressure, gesture, annotation).</div>
                </div>
            </figure>
            
            <p>The bottleneck limiting human-AI collaboration is not AI capability but <em>symbolic bandwidth</em>—the poverty of sign vehicles available for exchange. We have rich internal representations; our output channel is a text box. When a visual thinker must translate their spatial intuitions into text prompts, when a designer must describe their sketches in words, when a child must linearize their exploratory understanding into sequential queries—we lose the very richness that makes human cognition powerful.</p>
            
            <p>This matters for more than productivity. It matters for <em>who gets to participate</em> in the computational future. Text-based interfaces privilege certain cognitive styles, certain educational backgrounds, certain ways of being in the world. A child who thinks in pictures, a craftsperson who thinks with their hands, an elder who thinks in stories—all are excluded from full partnership with computational intelligence.</p>
            
            <blockquote>
                "In a few years, men will be able to communicate more effectively through a machine than face to face."
                <cite>— J.C.R. Licklider & Robert Taylor, "The Computer as a Communication Device," 1968</cite>
            </blockquote>
            
            <p>Licklider and Taylor's prophecy has come true for text—but we have yet to fulfill it for the full range of human expression. We already have richer languages than text: we have drawing, gesture, spatial arrangement, annotation, demonstration. We need interfaces that honor these languages—that let humans bring their full cognitive richness into collaboration with AI.</p>

              
            <h3>Dancing Without Music</h3>
            
            <blockquote>
                "Imagine that children were forced to spend an hour a day drawing dance steps on squared paper and had to pass tests in these 'dance facts' before they were allowed to dance physically. Would we not expect the world to be full of 'dancophobes'?"
                <cite>— Seymour Papert, <em>Mindstorms</em>, 1980</cite>
            </blockquote>
            
            <p>Papert's question cuts to the heart of how we teach abstraction. We have created generations of "mathphobes"—people who carry through life the belief that they are "bad at math" even as they routinely use logical reasoning to fix computers, build furniture, navigate cities, and run businesses.</p>
            
            <p>The problem is not aptitude. The problem is that we ask people to dance without music—to manipulate symbols divorced from meaning, to learn the steps before they feel the rhythm. A child raised in a DIY family, with an intuitive sense for assembling materials in 3D space, may never connect the theory of school geometry with the practice of building. The analogy leap from scribbling symbols on paper to genuine internalization never happens.</p>
            
            <p>But give that same person a system that provides constant feedback, that lets them see the results of their actions immediately, that connects symbol to meaning through direct manipulation—and suddenly they discover they were never unable to do math. They were simply never shown the right connection to spark their understanding.</p>
        </section>

        <!-- SECTION 2: THE VISION -->
        <section id="vision">
            <h2>The Vision: As We May Sketch</h2>
            
            <p>In 1945, Vannevar Bush imagined the Memex—a device for extending human memory and enabling associative thinking. He asked: <em>As we may think</em>, how might machines augment the trails of connection that constitute human understanding?</p>
            
            <p>We ask the parallel question: <em>As we may sketch</em>, how might machines augment the visual, spatial, gestural thinking that constitutes so much of human cognition—especially in children, who draw before they write, who think in pictures before they think in propositions?</p>
            
            <blockquote>
                Anything digitized has become an abstraction, so let's embrace it. When I draw into a computer with the flourish of my hand, we can take it beyond pixels, beyond even vectors, toward universal mapping attempts, toward a truly metamedium.
                <cite>— John Hanacek, "As We May Sketch," Georgetown CCT Masters Thesis 2016</cite>
            </blockquote>
            
            <p>A curve drawn by hand is a chance to try fitting a function to the line. A function is just waiting to become metaphorical graphics. Digital ink will move beyond "networked paper" to become a magical plane where computer vision partners with the human hand functioning as an <strong>interactive external imagination</strong>. From sketch to code, from code to sketch—no longer a pipeline but rather a constellation of possibilities, an ever-expanding network of opportunities to map expressiveness and flow to logic and math directly.</p>
            
            <h3>A Medium for Children</h3>
            
            <blockquote>
                "The child is a 'verb' rather than a 'noun', an actor rather than an object... We would like to hook into his current modes of thought in order to influence him rather than just trying to replace his model with one of our own."
                <cite>— Alan Kay, "A Personal Computer for Children of All Ages," 1972</cite>
            </blockquote>
            
            <p>Children don't need to learn to code before they can think computationally. They already think in systems, in cause and effect, in "what happens if." They need interfaces that meet them where they already are—drawing, playing, exploring. The MetaMedium is that interface: a space where a child's sketch of a "bouncy house" contains the seeds of structural engineering, where doodled spirals become gateways to mathematical beauty, where the native language of visual thinking connects directly to computational power.</p>
            
            <h3>How We Think: Conceptual Blending</h3>
            
            <p>What is human thought anyway? Gilles Fauconnier and Mark Turner attempted to answer this with their theory of <em>conceptual blending</em>, building on Lakoff and Johnson's work on how metaphor structures understanding. Consider a riddle:</p>
            
            <div class="callout">
                <p>A Buddhist monk begins at dawn walking up a mountain, reaches the top at sunset. After several days, he walks back down, starting at dawn and arriving at sunset. <strong>Is there a place on the path he occupies at the same hour on both journeys?</strong></p>
            </div>
            
            <p>The answer becomes obvious the moment you <em>visualize</em> two monks walking the path simultaneously—one going up, one going down. They must meet somewhere. But this visualization requires what Fauconnier and Turner call an "integration network"—a blended mental space where separate inputs combine to reveal emergent structure. I have animated their central figure illustrating the blending space as a diagram.</p>

            <figure>
                <img src="./Assets/fig-Conceptual Blending slower.gif" alt="Conceptual Blending Animation - Integration network showing blend space">
                <figcaption>Fauconnier & Turner's conceptual blending model: two input spaces merge into a blended space revealing emergent structure.</figcaption>
            </figure>

            <blockquote>
                "Thanks to a mapping, full-fledged meaning can suddenly appear in a spot where it was entirely unsuspected."
                <cite>— Douglas Hofstadter, <em>I Am a Strange Loop</em>, 2007</cite>
            </blockquote>
            
            <p>The MetaMedium is a system for building integration networks on a canvas. When you draw a diagram, you are setting up mental spaces. When you connect elements with arrows or proximity, you are creating cross-space mappings. When the AI interprets your marks and offers possibilities, it is helping locate shared structures. Diagrammatic thinking externalizes the blending process—making it visible, manipulable, shareable. Two people looking at the same diagram can point to the same conceptual space.</p>
<!--
            <div class="demo-container">
                <div class="demo-header">
                    <span>Interactive Figure - Network Nodes as Table</span>
                    <span><a href="https://jjh111.github.io/MetaMedium/doodle2-canvas.html" target="_blank" rel="noopener"><strong></strong>Try it→</strong></a></span>
                
                </div>
                <iframe src="https://jjh111.github.io/MetaMedium/Demos/sna-drawing-demo.html" title="MetaMedium Demo" loading="lazy"></iframe>
            </div>
        </section>

       -->
        <!-- SECTION 4: THE THESIS -->
        <section id="thesis">
            <h2>The Thesis: AI as Meta-Word</h2>
            
            <h3>Closing the Triadic Loop</h3>
            
            <p>Current human-computer interfaces connect Language and Computation, but leave Meaning external to the loop. We write code; machines execute. Machines return outputs, displays, text. But there is no continuous <strong>Language ↔ Computation ↔ Meaning</strong> feedback loop. The computer cannot participate in meaning-making—it can only execute instructions and return results.</p>
            
            <!--
              TRIADIC LOOP FIGURE
              For MetaMedium Whitepaper

              Usage: Insert this entire block where the figure should appear.
              Dependencies: None (self-contained)
              Styling: Uses CSS custom properties from whitepaper, with fallbacks
            -->

            <div class="triadic-loop-figure" id="triadicLoop">
              <div class="triadic-controls">
                <button class="triadic-toggle" data-mode="traditional" aria-pressed="true">Traditional</button>
                <button class="triadic-toggle" data-mode="metamedium" aria-pressed="false">MetaMedium</button>
              </div>

              <svg viewBox="0 0 400 360" class="triadic-svg" aria-labelledby="triadicTitle triadicDesc">
                <title id="triadicTitle">The Triadic Loop</title>
                <desc id="triadicDesc">Diagram showing how MetaMedium closes the loop between Language, Computation, and Meaning</desc>

                <defs>
                  <!-- Gradient for active edges -->
                  <linearGradient id="edgeGradient" x1="0%" y1="0%" x2="100%" y2="0%">
                    <stop offset="0%" style="stop-color: var(--accent, #3b82f6); stop-opacity: 0.8" />
                    <stop offset="100%" style="stop-color: var(--accent-secondary, #8b5cf6); stop-opacity: 0.8" />
                  </linearGradient>

                  <!-- Arrow marker -->
                  <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                    <polygon points="0 0, 10 3.5, 0 7" fill="var(--accent, #3b82f6)" />
                  </marker>
                  <marker id="arrowheadDim" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                    <polygon points="0 0, 10 3.5, 0 7" fill="var(--border, #d4d4d4)" />
                  </marker>
                  <marker id="arrowheadReverse" markerWidth="10" markerHeight="7" refX="1" refY="3.5" orient="auto">
                    <polygon points="10 0, 0 3.5, 10 7" fill="var(--accent, #3b82f6)" />
                  </marker>

                  <!-- Glow filter -->
                  <filter id="glow" x="-50%" y="-50%" width="200%" height="200%">
                    <feGaussianBlur stdDeviation="3" result="coloredBlur"/>
                    <feMerge>
                      <feMergeNode in="coloredBlur"/>
                      <feMergeNode in="SourceGraphic"/>
                    </feMerge>
                  </filter>
                </defs>

                <!-- Background subtle grid -->
                <pattern id="triGrid" width="20" height="20" patternUnits="userSpaceOnUse">
                  <path d="M 20 0 L 0 0 0 20" fill="none" stroke="var(--border, #e5e5e5)" stroke-width="0.5" opacity="0.3"/>
                </pattern>
                <rect width="400" height="360" fill="url(#triGrid)" />

                <!-- Triangle vertices (positions) -->
                <!-- Top: Language (200, 60) -->
                <!-- Bottom-left: Computation (60, 300) -->
                <!-- Bottom-right: Meaning (340, 300) -->

                <!-- EDGES -->
                <g class="triadic-edges">
                  <!-- Language → Computation (left edge) -->
                  <line class="edge edge-lang-comp"
                        x1="175" y1="85" x2="85" y2="270"
                        stroke="var(--accent, #3b82f6)" stroke-width="2"
                        marker-end="url(#arrowhead)" />

                  <!-- Computation → Meaning/Output (bottom edge) -->
                  <line class="edge edge-comp-meaning"
                        x1="95" y1="300" x2="305" y2="300"
                        stroke="var(--accent, #3b82f6)" stroke-width="2"
                        marker-end="url(#arrowhead)" />

                  <!-- Meaning → Language (right edge) - initially hidden in traditional -->
                  <line class="edge edge-meaning-lang"
                        x1="315" y1="270" x2="225" y2="85"
                        stroke="var(--accent, #3b82f6)" stroke-width="2"
                        marker-end="url(#arrowhead)"
                        opacity="0" />

                  <!-- Reverse arrows (MetaMedium only) -->
                  <line class="edge edge-reverse edge-comp-lang"
                        x1="85" y1="255" x2="170" y2="90"
                        stroke="var(--accent-secondary, #8b5cf6)" stroke-width="2"
                        marker-end="url(#arrowhead)"
                        opacity="0" />
                  <line class="edge edge-reverse edge-meaning-comp"
                        x1="290" y1="295" x2="110" y2="295"
                        stroke="var(--accent-secondary, #8b5cf6)" stroke-width="2"
                        marker-end="url(#arrowhead)"
                        opacity="0" />
                  <line class="edge edge-reverse edge-lang-meaning"
                        x1="230" y1="90" x2="310" y2="255"
                        stroke="var(--accent-secondary, #8b5cf6)" stroke-width="2"
                        marker-end="url(#arrowhead)"
                        opacity="0" />
                </g>

                <!-- Animated pulses (MetaMedium mode) -->
                <g class="triadic-pulses" opacity="0">
                  <circle class="pulse pulse-1" r="4" fill="var(--accent, #3b82f6)" filter="url(#glow)">
                    <animateMotion dur="2s" repeatCount="indefinite" path="M175,85 L85,270 L305,300 L225,85 Z" />
                  </circle>
                  <circle class="pulse pulse-2" r="4" fill="var(--accent-secondary, #8b5cf6)" filter="url(#glow)">
                    <animateMotion dur="2s" repeatCount="indefinite" begin="0.66s" path="M175,85 L85,270 L305,300 L225,85 Z" />
                  </circle>
                  <circle class="pulse pulse-3" r="4" fill="var(--accent, #3b82f6)" filter="url(#glow)">
                    <animateMotion dur="2s" repeatCount="indefinite" begin="1.33s" path="M175,85 L85,270 L305,300 L225,85 Z" />
                  </circle>
                  <!-- Counter-rotating pulses -->
                  <circle class="pulse pulse-4" r="3" fill="var(--accent-secondary, #8b5cf6)" opacity="0.7">
                    <animateMotion dur="2.5s" repeatCount="indefinite" path="M225,85 L305,300 L85,270 L175,85 Z" />
                  </circle>
                </g>

                <!-- NODES -->
                <g class="triadic-nodes">
                  <!-- Language Node (top) -->
                  <g class="node node-language" transform="translate(200, 60)">
                    <circle r="36" fill="var(--paper, #f8f6f1)" stroke="var(--accent, #3b82f6)" stroke-width="3" />
                    <text y="5" text-anchor="middle" class="node-label" fill="var(--text, #1a1a2e)">Language</text>
                  </g>

                  <!-- Computation Node (bottom-left) -->
                  <g class="node node-computation" transform="translate(60, 300)">
                    <circle r="36" fill="var(--paper, #f8f6f1)" stroke="var(--accent, #3b82f6)" stroke-width="3" />
                    <text y="5" text-anchor="middle" class="node-label" fill="var(--text, #1a1a2e)">Computation</text>
                  </g>

                  <!-- Meaning/Output Node (bottom-right) -->
                  <g class="node node-meaning" transform="translate(340, 300)">
                    <circle r="36" fill="var(--paper, #f8f6f1)" stroke="var(--border, #d4d4d4)" stroke-width="3" class="meaning-circle" />
                    <text y="5" text-anchor="middle" class="node-label meaning-label" fill="var(--muted, #666)">Output</text>
                  </g>
                </g>

                <!-- Annotations -->
                <g class="triadic-annotations">
                  <!-- Traditional annotation -->
                  <text class="annotation annotation-traditional" x="200" y="345" text-anchor="middle" fill="var(--muted, #666)" font-size="12">
                    Linear: meaning stays in human heads
                  </text>
                  <!-- MetaMedium annotation -->
                  <text class="annotation annotation-metamedium" x="200" y="345" text-anchor="middle" fill="var(--accent, #3b82f6)" font-size="12" opacity="0">
                    Circular: AI participates in interpretation
                  </text>
                </g>

                <!-- Center label -->
                <text class="center-label" x="200" y="210" text-anchor="middle" fill="var(--muted, #666)" font-size="11" opacity="0">
                  negotiation
                </text>
              </svg>

              <p class="triadic-caption">
                <span class="caption-traditional">Traditional interfaces flow one way: we write, machines execute, outputs return. Meaning remains external.</span>
                <span class="caption-metamedium" style="display: none;">The MetaMedium closes the loop. Marks become probabilistic proposals. Both parties negotiate meaning.</span>
              </p>
            </div>

            <style>
            .triadic-loop-figure {
              --accent: #3b82f6;
              --accent-secondary: #8b5cf6;
              --paper: #f8f6f1;
              --text: #1a1a2e;
              --muted: #666;
              --border: #d4d4d4;

              margin: var(--space-lg, 2rem) 0;
              padding: var(--space-md, 1rem);
              background: linear-gradient(135deg, rgba(59, 130, 246, 0.03) 0%, rgba(139, 92, 246, 0.03) 100%);
              border-radius: 12px;
              font-family: 'Space Grotesk', system-ui, sans-serif;
            }

            .triadic-controls {
              display: flex;
              justify-content: center;
              gap: 0.5rem;
              margin-bottom: 1rem;
            }

            .triadic-toggle {
              padding: 0.5rem 1rem;
              border: 1px solid var(--border);
              border-radius: 6px;
              background: var(--paper);
              color: var(--muted);
              font-family: inherit;
              font-size: 0.875rem;
              cursor: pointer;
              transition: all 0.2s ease;
            }

            .triadic-toggle[aria-pressed="true"] {
              background: var(--accent);
              border-color: var(--accent);
              color: white;
            }

            .triadic-toggle:hover:not([aria-pressed="true"]) {
              border-color: var(--accent);
              color: var(--accent);
            }

            .triadic-svg {
              width: 100%;
              max-width: 400px;
              height: auto;
              display: block;
              margin: 0 auto;
            }

            .node-label {
              font-family: 'Space Grotesk', system-ui, sans-serif;
              font-size: 11px;
              font-weight: 500;
            }

            .triadic-caption {
              text-align: center;
              font-size: 0.875rem;
              color: var(--muted);
              margin-top: 1rem;
              min-height: 2.5em;
            }

            /* Transitions */
            .edge, .node circle, .node-label, .annotation, .center-label {
              transition: all 0.5s ease;
            }

            .triadic-pulses {
              transition: opacity 0.5s ease;
            }

            /* MetaMedium mode styles (applied via JS) */
            .triadic-loop-figure.metamedium .meaning-circle {
              stroke: var(--accent);
            }

            .triadic-loop-figure.metamedium .meaning-label {
              fill: var(--text);
            }

            .triadic-loop-figure.metamedium .edge-meaning-lang,
            .triadic-loop-figure.metamedium .edge-reverse {
              opacity: 1;
            }

            .triadic-loop-figure.metamedium .triadic-pulses {
              opacity: 1;
            }

            .triadic-loop-figure.metamedium .annotation-traditional {
              opacity: 0;
            }

            .triadic-loop-figure.metamedium .annotation-metamedium {
              opacity: 1;
            }

            .triadic-loop-figure.metamedium .center-label {
              opacity: 0.7;
            }

            /* Responsive */
            @media (max-width: 480px) {
              .triadic-controls {
                flex-direction: column;
                align-items: center;
              }
              .triadic-toggle {
                width: 100%;
                max-width: 200px;
              }
            }
            </style>

            <script>
            (function() {
              const figure = document.getElementById('triadicLoop');
              const toggles = figure.querySelectorAll('.triadic-toggle');
              const captionTraditional = figure.querySelector('.caption-traditional');
              const captionMetamedium = figure.querySelector('.caption-metamedium');
              const meaningLabel = figure.querySelector('.meaning-label');

              function setMode(mode) {
                // Update toggle states
                toggles.forEach(btn => {
                  btn.setAttribute('aria-pressed', btn.dataset.mode === mode);
                });

                // Update figure class
                if (mode === 'metamedium') {
                  figure.classList.add('metamedium');
                  captionTraditional.style.display = 'none';
                  captionMetamedium.style.display = 'inline';
                  meaningLabel.textContent = 'Meaning';
                } else {
                  figure.classList.remove('metamedium');
                  captionTraditional.style.display = 'inline';
                  captionMetamedium.style.display = 'none';
                  meaningLabel.textContent = 'Output';
                }
              }

              toggles.forEach(btn => {
                btn.addEventListener('click', () => setMode(btn.dataset.mode));
              });

              // Optional: auto-cycle for demo
              // let isMetamedium = false;
              // setInterval(() => {
              //   isMetamedium = !isMetamedium;
              //   setMode(isMetamedium ? 'metamedium' : 'traditional');
              // }, 4000);
            })();
            </script>
            
            <p>The MetaMedium closes this triad. When every mark can mean multiple things, when the system holds probabilistic interpretations and refines them through exchange, meaning becomes part of the computational loop—not something that happens only in human minds before and after the interaction.</p>
            
            <h3>AI as Meta-Word</h3>
            
            <p>The MetaMedium proposes that AI can function as a new kind of <em>meta-word</em> within an evolution of language itself. Just as written language externalized memory, allowing us to store and retrieve thoughts across time and space, AI externalizes <em>interpretation</em>—the capacity to generate meaning from marks in context.</p>
            
            <div class="callout key">
                <h4>What Is a Meta-Word?</h4>
                <p>A meta-word is not a word about words (like "noun" or "verb"). It's a word that <em>transforms other words</em>—that takes your rough marks and interprets them into structured meaning based on context. When you write "bounce" near a spring, the meta-word doesn't just read "bounce"—it understands <em>spring + bounce = physics behavior</em> and makes it so.</p>
            </div>
            
            <figure>
                <div class="placeholder-figure">
                    <div class="image"><img src="./Assets/fig-MetaMediumTriad.svg" alt="From Docs to MetaMedium</div>
                    <div class="label">Figure: AI as Meta-Word — The Evolution of External Cognition</div>
                    <div class="description">Three paradigms: (1) Writing: human → marks → human (marks store memory, reader reconstructs meaning). (2) Computing: human → code → machine → output (machine executes instructions literally). (3) MetaMedium: human ↔ marks ↔ AI ↔ meaning (bidirectional interpretation, iterative refinement, meaning emerges through exchange).</div>
                </div>
            </figure>
            
            <p>In traditional communication, humans exchange sign vehicles (words, gestures, marks) and each party reconstructs meaning internally. The MetaMedium introduces AI as an active participant in this meaning-making process—not merely receiving marks and returning responses, but operating as a semantic transformer that:</p>
            
            <ul>
                <li>Recognizes patterns across multiple possible interpretations</li>
                <li>Holds ambiguity productively until context resolves it</li>
                <li>Learns user-specific vocabularies and reasoning patterns</li>
                <li>Bridges between representational modes (sketch ↔ equation ↔ code ↔ text)</li>
            </ul>
            
            <p>This is not AI as tool. This is AI as <em>grammatical element</em>—a new part of speech in the language of human-computer collaboration.</p>
            
            <h3>Tools vs. Medium</h3>
            
            <p>Alan Kay's Dynabook vision asked: "What is the carrying capacity for ideas of the computer?" His answer: the computer is a metamedium—it can simulate any existing media and also be the basis of media that can't exist without the computer. But Kay made a crucial distinction:</p>
            
            <blockquote>
                "What then is a personal computer? One would hope that it would be both a medium for containing and expressing arbitrary symbolic notions, and also a collection of useful tools for manipulating these structures."
                <cite>— Alan Kay, "A Personal Computer for Children of All Ages," 1972</cite>
            </blockquote>
            
            <p>Most AI interfaces treat AI as tool—something to be invoked, queried, commanded. The MetaMedium treats AI as part of the medium itself. The sketches aren't <em>using</em> the computer; they're <em>composing within</em> the computational medium, with AI as an active interpretive layer that transforms marks into executable meaning.</p>
        </section>

        <!-- SECTION 5: THE FRAMEWORK -->
        <section id="framework">
            <h2>The Framework</h2>
            
            <h3>The Semiotic Foundation</h3>
            
            <p>Charles Sanders Peirce's semiotics offers a framework for understanding meaning-making that proves remarkably apt for human-AI interaction. In Peirce's model, meaning emerges from the relationship between the <em>sign</em> (the representational form), the <em>object</em> (what is represented), and the <em>interpretant</em> (the meaning generated in the interpreter's mind).</p>
            
            <figure>
                <div class="placeholder-figure">
                    <div class="image">
                        <svg viewBox="0 0 700 400" xmlns="http://www.w3.org/2000/svg">
                            <!-- Title -->
                            <text x="350" y="10" text-anchor="middle" fill="#7b8a9a" font-size="15" opacity="1">How meaning emerges through triangulation</text>

                            <!-- Triangle structure -->
                            <line x1="350" y1="80" x2="180" y2="320" stroke="#64b5f6" stroke-width="2" opacity=".4"/>
                            <line x1="350" y1="80" x2="520" y2="320" stroke="#64b5f6" stroke-width="2" opacity=".4"/>
                            <line x1="180" y1="320" x2="520" y2="320" stroke="#64b5f6" stroke-width="2" opacity=".4"/>

                            <!-- OBJECT (top) -->
                            <g>
                                <circle cx="350" cy="80" r="26" fill="rgba(100, 181, 246, 0.15)" stroke="#64b5f6" stroke-width="2"/>
                                <text x="350" y="125" text-anchor="middle" fill="#90caf9" font-size="22" font-weight="bold">OBJECT</text>
                                <text x="350" y="140" text-anchor="middle" fill="#7b8a9a" font-size="13" opacity="1">(referent)</text>

                                <!-- Multiple possibilities -->
                                <text x="350" y="88" text-anchor="middle" fill="#90caf9" font-size="12">?</text>
                                <g transform="translate(350, 30)">
                                    <circle cx="-40" cy="0" r="7" fill="none" stroke="#90caf9" stroke-width="1" opacity="0.8"/>
                                    <text x="-40" y="15" text-anchor="middle" fill="#90caf9" font-size="11" opacity="0.6">face</text>

                                    <ellipse cx="-14" cy="0" rx="7" ry="8" fill="none" stroke="#90caf9" stroke-width="1" opacity="0.8"/>
                                    <text x="-14" y="15" text-anchor="middle" fill="#90caf9" font-size="11" opacity="0.6">egg</text>

                                    <circle cx="13" cy="0" r="7" fill="none" stroke="#90caf9" stroke-width="1" opacity="0.8"/>
                                    <text x="13" y="5" text-anchor="middle" fill="#90caf9" font-size="10" opacity="0.6">0</text>

                                    <circle cx="39" cy="0" r="7" fill="rgba(100, 181, 246, 0.15)" stroke="#90caf9" stroke-width="1" opacity="0.8"/>
                                    <text x="39" y="15" text-anchor="middle" fill="#90caf9" font-size="11" opacity="0.6">ball</text>
                                </g>
                            </g>

                            <!-- SIGN (bottom left) -->
                            <g>
                                <circle cx="180" cy="320" r="26" fill="rgba(100, 181, 246, 0.15)" stroke="#64b5f6" stroke-width="2"/>
                                <text x="180" y="361" text-anchor="middle" fill="#90caf9" font-size="22" font-weight="bold">SIGN</text>
                                <text x="180" y="375" text-anchor="middle" fill="#7b8a9a" font-size="13" opacity="1">(form)</text>

                                <!-- Sketch progression -->
                                <g transform="translate(50, 170)">
                                    <!-- Step 1: rough oval -->
                                    <rect x="0" y="0" width="80" height="70" fill="rgba(10, 14, 26, 0.5)" stroke="#4a5568" stroke-width="1" rx="3"/>
                                    <ellipse cx="40" cy="30" rx="22" ry="25" fill="none" stroke="#90caf9" stroke-width="2" stroke-dasharray="2,1" opacity="0.8"/>
                                    <text x="40" y="60" text-anchor="middle" fill="#7b8a9a" font-size="11">1. rough oval</text>

                                    <!-- Arrow -->
                                    <text x="88" y="35" fill="#64b5f6" font-size="16">→</text>

                                    <!-- Step 2: with dots -->
                                    <rect x="100" y="0" width="80" height="70" fill="rgba(10, 14, 26, 0.5)" stroke="#4a5568" stroke-width="1" rx="3"/>
                                    <ellipse cx="140" cy="30" rx="22" ry="25" fill="none" stroke="#90caf9" stroke-width="2" stroke-dasharray="2,1" opacity="0.8"/>
                                    <circle cx="134" cy="27" r="2.5" fill="#90caf9"/>
                                    <circle cx="146" cy="27" r="2.5" fill="#90caf9"/>
                                    <text x="140" y="52" text-anchor="middle" fill="#7b8a9a" font-size="11">2. + eyes</text>
                                    <text x="140" y="64" text-anchor="middle" fill="#90caf9" font-size="11" font-weight="bold">→ face?</text>
                                </g>
                            </g>

                            <!-- INTERPRETANT (bottom right) -->
                            <g>
                                <circle cx="520" cy="320" r="26" fill="rgba(100, 181, 246, 0.15)" stroke="#64b5f6" stroke-width="2"/>
                                <text x="520" y="361" text-anchor="middle" fill="#90caf9" font-size="22" font-weight="bold">INTERPRETANT</text>
                                <text x="520" y="375" text-anchor="middle" fill="#7b8a9a" font-size="13" opacity="1">(meaning)</text>

                                <!-- Probability distribution -->
                                <g transform="translate(450, 168)">
                                    <!-- Initial probabilities -->
                                    <rect x="0" y="0" width="120" height="85" fill="rgba(10, 14, 26, 0.5)" stroke="#4a5568" stroke-width="1" rx="3"/>
                                    <text x="60" y="15" text-anchor="middle" fill="#4a5568" font-size="12">AI considers:</text>

                                    <rect x="8" y="23" width="45" height="7" fill="#64b5f6" opacity="0.8"/>
                                    <text x="58" y="29" fill="#90caf9" font-size="11">face 0.4</text>

                                    <rect x="8" y="34" width="28" height="7" fill="#64b5f6" opacity="0.8"/>
                                    <text x="58" y="40" fill="#90caf9" font-size="11">egg 0.25</text>

                                    <rect x="8" y="45" width="22" height="7" fill="#64b5f6" opacity="0.8"/>
                                    <text x="58" y="51" fill="#90caf9" font-size="11">zero 0.2</text>

                                    <rect x="8" y="56" width="17" height="7" fill="#64b5f6" opacity="0.8"/>
                                    <text x="58" y="62" fill="#90caf9" font-size="11">ball 0.15</text>

                                    <text x="60" y="77" text-anchor="middle" fill="#64b5f6" font-size="12">↓ context</text>

                                    <!-- Resolved -->
                                    <rect x="0" y="90" width="120" height="38" fill="rgba(100, 181, 246, 0.12)" stroke="#64b5f6" stroke-width="1.5" rx="3"/>
                                    <rect x="8" y="100" width="104" height="9" fill="#64b5f6" opacity="0.8"/>
                                    <text x="60" y="107" text-anchor="middle" fill="#fff" font-size="12" font-weight="bold">face 0.95 ✓</text>
                                    <text x="60" y="122" text-anchor="middle" fill="#64b5f6" font-size="11">"confident match"</text>
                                </g>
                            </g>

                            <!-- Central insight -->
                            <text x="350" y="310" text-anchor="middle" fill="#64b5f6" font-size="14" opacity="0.9">AI participates in meaning-making,</text>
                            <text x="350" y="335" text-anchor="middle" fill="#64b5f6" font-size="14" opacity="0.9">not as authority but as collaborator</text>
                        </svg>
                    </div>
                    <div class="label">Figure: The Semiotic Triad Applied</div>
                    <div class="description">Peirce's triangle in MetaMedium context: Sign (rough oval drawn on canvas) → Object (could be face, egg, zero, planet...) → Interpretant (AI holds probability distribution across possibilities; context accumulates; meaning resolves). The AI participates in the interpretant role, not as final arbiter but as collaborative sense-maker.</div>
                </div>
            </figure>
            
            <p>Peirce's insight—that meaning is reconstructed, not transferred—maps directly onto the <strong>Language ↔ Computation ↔ Meaning</strong> triadic loop described earlier. The MetaMedium enables this iterative coordination by letting AI participate as interpretant, holding probabilistic meanings that refine through exchange rather than executing fixed commands.</p>

            <h3>Core Principles</h3>
            
            <div class="principles-grid">
                <div class="principle" data-num="1">
                    <h4>Space Is Semantic</h4>
                    <p>Spatial relationships carry meaning. Elements placed near each other are semantically related. Elements connected by lines have directional relationships. The entire canvas becomes a semantic field where position, proximity, and connection are first-class citizens of meaning.</p>
                    <p class="principle-example"><em>Place two circles close together; the system infers "related." Draw one inside another; it understands "containment." Position creates meaning without words.</em></p>
                </div>
                
                <div class="principle" data-num="2">
                    <h4>Annotation Becomes Execution</h4>
                    <p>When you write "make this bounce" near a drawn spring, that annotation becomes an instruction. When you draw an arrow from input to output, you've defined a data flow. The boundary between description and command dissolves; to describe is to instruct.</p>
                    <p class="principle-example"><em>Write "3x" next to a line; it becomes three lines. Write "wiggle" near a shape; it animates. The annotation is the program.</em></p>
                </div>
                
                <div class="principle" data-num="3">
                    <h4>Interpretive Ambiguity Is Feature</h4>
                    <p>A rough sketch is understood as rough—the system holds multiple interpretations probabilistically, refining its understanding as context accumulates. This mirrors how humans communicate: we tolerate ambiguity, using context to resolve meaning over time.</p>
                    <p class="principle-example"><em>Your rough oval might be a face, an egg, or a zero. The system holds all three until you add two dots—then it commits to "face." Premature disambiguation is the enemy of exploration.</em></p>
                </div>
                
                <div class="principle" data-num="4">
                    <h4>Bidirectional Learning</h4>
                    <p>User and system engage in mutual adaptation. User-specific patterns accumulate into "cognitive lenses"—the system learns your vocabulary and conventions. Simultaneously, the system teaches you by surfacing patterns, completing gestures, suggesting relationships. Your idiolect becomes executable; the system's interpretations become visible. Both intelligences grow through exchange.</p>
                    <p class="principle-example"><em>Draw "recursion" shorthand repeatedly; the system learns it. Later, it suggests this mark when detecting recursive patterns—teaching you to see what it sees. Your notation becomes shared language.</em></p>
                </div>

                <div class="principle" data-num="5">
                    <h4>No Mode Switching</h4>
                    <p>Following Larry Tesler's principle that "no modes is good modes," the MetaMedium eliminates cognitive overhead. You're always just working—drawing, annotating, refining. The interpretation layer appears when needed and fades when not. There's no "design mode" vs. "execution mode," no switching between tools.</p>
                    <p class="principle-example"><em>Draw a shape. Write near it. Adjust with gestures. Watch it execute. All the same canvas, all the same moment. The interface disappears into the work.</em></p>
                </div>

                <div class="principle" data-num="6">
                    <h4>Observable Reasoning</h4>
                    <p>Uncertainty becomes visible. When the AI considers multiple interpretations, the user sees them—not collapsed into a single guess. Multiple possibilities stay present until context or user choice resolves them. This transparency supports both trust and alignment.</p>
                    <p class="principle-example"><em>Your rough mark triggers three possible interpretations shown as faint ghosts; tap one to commit, or keep drawing to refine. You see the system thinking.</em></p>
                </div>
            </div>
            
            <h3>The Negotiation Paradigm</h3>
            
            <p>Ribeiro and Igarashi's "Sketch-Editing Games" (UIST 2012) introduced a negotiation paradigm where user and machine take turns refining interpretation. The user sketches; the machine recognizes and offers interpretations ("bottle?"). The user refines ("no, more like a mug"). The machine updates. Understanding emerges through iterative exchange.</p>
            
            <div class="figure-pair">
                <figure>
                    <img src="./Assets/fig-sketch-game-turn.png" alt="Turn-taking in the sketch-editing game">
                    <figcaption><strong>Figure: Turn-Taking.</strong> The user sketches; the machine recognizes and modifies with a guess ("bottle?"). User declines and modifies. Machine recognizes again ("mug?"). User accepts. From Ribeiro & Igarashi, UIST '12.</figcaption>
                </figure>
                <figure>
                    <img src="./Assets/fig-sketch-game-graph.png" alt="Game graph showing transformation possibilities">
                    <figcaption><strong>Figure: Possibility Graph.</strong> The machine's internal model after several games. The center shows the current sketch; each ring shows possible transformations (add neck → bottle, add base → wine glass, widen top → cup). From Ribeiro & Igarashi, UIST '12.</figcaption>
                </figure>
            </div>

            <p>Their key insight: sketch recognition improves dramatically when reframed as a <em>game</em> rather than a classification problem. The machine maintains a "possibility graph"—a network of possible interpretations and the transformations that would select among them. The user's next stroke navigates this graph, collapsing some possibilities and opening others.</p>
            
            <figure>
                <div class="placeholder-figure">
                    <div class="image">
                        <svg viewBox="0 0 700 400" xmlns="http://www.w3.org/2000/svg">
                            <!-- Title -->
                            <text x="350" y="22" text-anchor="middle" fill="#7b8a9a" font-size="15" opacity="1">Understanding emerges through iterative exchange</text>

                            <!-- User box (left) -->
                            <rect x="70" y="50" width="140" height="60" fill="rgba(100, 181, 246, 0.08)" stroke="#64b5f6" stroke-width="2" rx="6"/>
                            <text x="140" y="78" text-anchor="middle" fill="#64b5f6" font-size="18" font-weight="bold">USER</text>
                            <text x="140" y="95" text-anchor="middle" fill="#7b8a9a" font-size="14">sketches</text>

                            <!-- System box (right) -->
                            <rect x="490" y="50" width="140" height="60" fill="rgba(100, 181, 246, 0.08)" stroke="#64b5f6" stroke-width="2" rx="6"/>
                            <text x="560" y="78" text-anchor="middle" fill="#64b5f6" font-size="18" font-weight="bold">SYSTEM</text>
                            <text x="560" y="95" text-anchor="middle" fill="#7b8a9a" font-size="14">interprets</text>

                            <!-- Exchange 1 -->
                            <g>
                                <line x1="210" y1="140" x2="480" y2="140" stroke="#64b5f6" stroke-width="2" marker-end="url(#arrow4)" stroke-dasharray="3,2"/>
                                <text x="345" y="133" text-anchor="middle" fill="#64b5f6" font-size="14">1. draws oval</text>
                                <ellipse cx="315" cy="143" rx="11" ry="13" fill="none" stroke="#64b5f6" stroke-width="2" stroke-dasharray="2,1"/>

                                <line x1="480" y1="160" x2="210" y2="160" stroke="#7b8a9a" stroke-width="2" marker-end="url(#arrow5)"/>
                                <text x="345" y="175" text-anchor="middle" fill="#6b7280" font-size="13" font-style="italic">"Circle? Face? Egg?"</text>
                            </g>

                            <!-- Exchange 2 -->
                            <g transform="translate(0, 60)">
                                <line x1="210" y1="140" x2="480" y2="140" stroke="#64b5f6" stroke-width="2" marker-end="url(#arrow4)" stroke-dasharray="3,2"/>
                                <text x="345" y="133" text-anchor="middle" fill="#64b5f6" font-size="14">2. adds two dots</text>
                                <ellipse cx="315" cy="143" rx="11" ry="13" fill="none" stroke="#64b5f6" stroke-width="2" stroke-dasharray="2,1"/>
                                <circle cx="311" cy="141" r="2" fill="#64b5f6"/>
                                <circle cx="319" cy="141" r="2" fill="#64b5f6"/>

                                <line x1="480" y1="160" x2="210" y2="160" stroke="#7b8a9a" stroke-width="2" marker-end="url(#arrow5)"/>
                                <text x="345" y="175" text-anchor="middle" fill="#6b7280" font-size="13" font-style="italic">"Face! (confidence 0.9)"</text>
                            </g>

                            <!-- Exchange 3 -->
                            <g transform="translate(0, 120)">
                                <line x1="210" y1="140" x2="480" y2="140" stroke="#64b5f6" stroke-width="2" marker-end="url(#arrow4)" stroke-dasharray="3,2"/>
                                <text x="345" y="133" text-anchor="middle" fill="#64b5f6" font-size="14">3. writes "happy"</text>
                                <text x="315" y="149" text-anchor="middle" fill="#64b5f6" font-size="13" font-family="Monaco">happy</text>

                                <line x1="480" y1="160" x2="210" y2="160" stroke="#7b8a9a" stroke-width="2" marker-end="url(#arrow5)"/>
                                <text x="345" y="175" text-anchor="middle" fill="#6b7280" font-size="13" font-style="italic">adds smile + learns pattern</text>
                            </g>

                            <!-- Shared Understanding result -->
                            <g transform="translate(0, 200)">
                                <rect x="230" y="140" width="240" height="58" fill="rgba(100, 181, 246, 0.15)" stroke="#64b5f6" stroke-width="2" rx="6"/>
                                <text x="350" y="162" text-anchor="middle" fill="#64b5f6" font-size="18" font-weight="bold">SHARED UNDERSTANDING</text>
                                <text x="350" y="180" text-anchor="middle" fill="#6b7280" font-size="14">"Happy face" → vocabulary</text>

                                <!-- Small happy face demo -->
                                <ellipse cx="320" cy="187" rx="9" ry="10" fill="none" stroke="#64b5f6" stroke-width="2"/>
                                <circle cx="316" cy="185" r="1.3" fill="#64b5f6"/>
                                <circle cx="324" cy="185" r="1.3" fill="#64b5f6"/>
                                <path d="M 314 190 Q 320 192, 326 190" stroke="#64b5f6" stroke-width="1.5" fill="none"/>
                            </g>

                            <!-- Bottom insight -->
                            <text x="350" y="330" text-anchor="middle" fill="#6b7280" font-size="14" font-style="italic">Each exchange refines shared vocabulary and understanding</text>

                            <!-- Arrow markers -->
                            <defs>
                                <marker id="arrow4" markerWidth="8" markerHeight="8" refX="7" refY="2.5" orient="auto">
                                    <path d="M0,0 L0,5 L7,2.5 z" fill="#64b5f6" />
                                </marker>
                                <marker id="arrow5" markerWidth="8" markerHeight="8" refX="7" refY="2.5" orient="auto">
                                    <path d="M0,0 L0,5 L7,2.5 z" fill="#7b8a9a" />
                                </marker>
                            </defs>
                        </svg>
                    </div>
                    <div class="label">Figure: The Negotiation Loop</div>
                    <div class="description">User sketches rough shape → System offers: "Circle? Oval? Face?" → User adds two dots → System updates: "Face ✓" → User writes "happy" → System adds smile, adjusts interpretation confidence. Understanding emerges through exchange, not one-shot recognition. Each turn updates the possibility graph.</div>
                </div>
            </figure>
            
            <p>The MetaMedium extends this approach by making the possibility graph <em>learnable</em>—accumulating user-specific patterns over time—and by integrating natural language annotation as another form of navigation. The negotiation paradigm means that misunderstanding is not failure—it's information. Each correction teaches the system something about this user's intentions, building toward a shared vocabulary over time.</p>
            
            <h3>Cognitive Lenses</h3>
            
            <p>As patterns accumulate, they form "cognitive lenses"—personalized interpretation frameworks that shape how the system reads new marks. A physicist's lens recognizes force diagrams; an architect's lens sees load-bearing structures; a musician's lens interprets spatial arrangements as rhythm.</p>
            
            <figure>
                <div class="placeholder-figure">
                    <div class="image">
                        <svg viewBox="0 0 750 450" xmlns="http://www.w3.org/2000/svg">
                            <!-- Title -->
                            <text x="375" y="25" text-anchor="middle" fill="#7b8a9a" font-size="15" opacity="1">Your patterns become portable ways of seeing</text>

                            <!-- Pattern Accumulation -->
                            <g>
                                <text x="120" y="65" fill="#90caf9" font-size="17" font-weight="bold">PATTERN ACCUMULATION</text>
                                <text x="120" y="83" fill="#7b8a9a" font-size="13">User repeatedly draws:</text>

                                <!-- Repeated marks -->
                                <g transform="translate(80, 95)">
                                    <!-- Force arrows -->
                                    <path d="M 0 0 L 20 -8" stroke="#90caf9" stroke-width="2" marker-end="url(#arrow6)"/>
                                    <path d="M 35 0 L 55 -8" stroke="#90caf9" stroke-width="2" marker-end="url(#arrow6)"/>
                                    <path d="M 70 0 L 90 -8" stroke="#90caf9" stroke-width="2" marker-end="url(#arrow6)"/>
                                    <path d="M 105 0 L 125 -8" stroke="#90caf9" stroke-width="2" marker-end="url(#arrow6)"/>
                                    <text x="135" y="0" fill="#7b8a9a" font-size="13" opacity="1">force</text>

                                    <!-- Energy marks -->
                                    <text x="0" y="25" fill="#90caf9" font-size="20">↯</text>
                                    <text x="35" y="25" fill="#90caf9" font-size="20">↯</text>
                                    <text x="70" y="25" fill="#90caf9" font-size="20">↯</text>
                                    <text x="105" y="25" fill="#90caf9" font-size="20">↯</text>
                                    <text x="135" y="25" fill="#7b8a9a" font-size="13" opacity="1">energy</text>

                                    <!-- Field lines -->
                                    <path d="M 0 45 Q 10 38, 20 45" stroke="#90caf9" stroke-width="2" fill="none"/>
                                    <path d="M 35 45 Q 45 38, 55 45" stroke="#90caf9" stroke-width="2" fill="none"/>
                                    <path d="M 70 45 Q 80 38, 90 45" stroke="#90caf9" stroke-width="2" fill="none"/>
                                    <path d="M 105 45 Q 115 38, 125 45" stroke="#90caf9" stroke-width="2" fill="none"/>
                                    <text x="135" y="48" fill="#7b8a9a" font-size="13" opacity="1">fields</text>
                                </g>
                            </g>

                            <!-- Arrow to extraction -->
                            <text x="350" y="140" text-anchor="middle" fill="#64b5f6" font-size="17">→ system extracts →</text>

                            <!-- Lens (crystallized pattern) -->
                            <g transform="translate(520, 85)">
                                <rect x="0" y="0" width="160" height="80" fill="rgba(100, 181, 246, 0.12)" stroke="#64b5f6" stroke-width="2" rx="6"/>
                                <text x="80" y="25" text-anchor="middle" fill="#90caf9" font-size="16" font-weight="bold">LENS</text>
                                <text x="80" y="43" text-anchor="middle" fill="#90caf9" font-size="13">"Maya's Physics</text>
                                <text x="80" y="56" text-anchor="middle" fill="#90caf9" font-size="13">Notation"</text>

                                <!-- Prism icon -->
                                <path d="M 80 -8 L 92 8 L 68 8 Z" fill="none" stroke="#64b5f6" stroke-width="2"/>
                                <line x1="92" y1="8" x2="96" y2="10" stroke="#90caf9" stroke-width="1" opacity="0.5"/>
                                <line x1="92" y1="8" x2="96" y2="6" stroke="#64b5f6" stroke-width="1" opacity="0.5"/>

                                <text x="80" y="72" text-anchor="middle" fill="#7b8a9a" font-size="12" opacity="1">portable interpretation</text>
                            </g>

                            <!-- Arrow down to operations -->
                            <line x1="600" y1="175" x2="600" y2="200" stroke="#64b5f6" stroke-width="2" marker-end="url(#arrow6)"/>

                            <!-- Lens Operations -->
                            <g transform="translate(0, 210)">
                                <text x="375" y="10" text-anchor="middle" fill="#90caf9" font-size="18" font-weight="bold">LENS OPERATIONS</text>

                                <!-- Operation 1: Apply -->
                                <g transform="translate(60, 30)">
                                    <rect x="0" y="0" width="170" height="165" fill="rgba(74, 85, 104, 0.2)" stroke="#64b5f6" stroke-width="2" rx="6"/>
                                    <text x="85" y="25" text-anchor="middle" fill="#90caf9" font-size="16" font-weight="bold">APPLY</text>
                                    <text x="85" y="43" text-anchor="middle" fill="#7b8a9a" font-size="13">Use on new drawings</text>

                                    <text x="85" y="72" text-anchor="middle" fill="#64b5f6" font-size="20">↓</text>

                                    <!-- Result -->
                                    <rect x="15" y="82" width="140" height="65" fill="rgba(10, 14, 26, 0.5)" stroke="#4a5568" stroke-width="1.5" rx="3"/>
                                    <text x="85" y="100" text-anchor="middle" fill="#7b8a9a" font-size="13">New sketch auto-</text>
                                    <text x="85" y="113" text-anchor="middle" fill="#7b8a9a" font-size="13">recognized as</text>
                                    <text x="85" y="126" text-anchor="middle" fill="#90caf9" font-size="14" font-weight="bold">force diagram</text>
                                    <path d="M 65 135 L 95 127" stroke="#90caf9" stroke-width="2" marker-end="url(#arrow6)"/>
                                </g>

                                <!-- Operation 2: Share -->
                                <g transform="translate(260, 30)">
                                    <rect x="0" y="0" width="170" height="165" fill="rgba(74, 85, 104, 0.2)" stroke="#64b5f6" stroke-width="2" rx="6"/>
                                    <text x="85" y="25" text-anchor="middle" fill="#90caf9" font-size="16" font-weight="bold">SHARE</text>
                                    <text x="85" y="43" text-anchor="middle" fill="#7b8a9a" font-size="13">Export to collaborators</text>

                                    <text x="85" y="72" text-anchor="middle" fill="#64b5f6" font-size="20">↓</text>

                                    <!-- Result -->
                                    <rect x="15" y="82" width="140" height="65" fill="rgba(10, 14, 26, 0.5)" stroke="#4a5568" stroke-width="1.5" rx="3"/>
                                    <text x="85" y="100" text-anchor="middle" fill="#7b8a9a" font-size="13">Teammate's canvas</text>
                                    <text x="85" y="113" text-anchor="middle" fill="#7b8a9a" font-size="13">now reads your</text>
                                    <text x="85" y="126" text-anchor="middle" fill="#90caf9" font-size="14" font-weight="bold">notation</text>
                                    <text x="65" y="142" text-anchor="middle" fill="#90caf9" font-size="18">👤</text>
                                    <text x="105" y="142" text-anchor="middle" fill="#90caf9" font-size="18">👤</text>
                                    <line x1="73" y1="139" x2="97" y2="139" stroke="#64b5f6" stroke-width="2"/>
                                </g>

                                <!-- Operation 3: Compose -->
                                <g transform="translate(460, 30)">
                                    <rect x="0" y="0" width="170" height="165" fill="rgba(74, 85, 104, 0.2)" stroke="#64b5f6" stroke-width="2" rx="6"/>
                                    <text x="85" y="25" text-anchor="middle" fill="#90caf9" font-size="16" font-weight="bold">COMPOSE</text>
                                    <text x="85" y="43" text-anchor="middle" fill="#7b8a9a" font-size="13">Combine with other lenses</text>

                                    <text x="85" y="72" text-anchor="middle" fill="#64b5f6" font-size="20">↓</text>

                                    <!-- Result -->
                                    <rect x="15" y="82" width="140" height="65" fill="rgba(10, 14, 26, 0.5)" stroke="#4a5568" stroke-width="1.5" rx="3"/>
                                    <text x="85" y="100" text-anchor="middle" fill="#7b8a9a" font-size="13">Create hybrid:</text>
                                    <text x="85" y="118" text-anchor="middle" fill="#90caf9" font-size="14" font-weight="bold">physics +</text>
                                    <text x="85" y="132" text-anchor="middle" fill="#90caf9" font-size="14" font-weight="bold">chemistry lens</text>
                                    <!-- Overlapping prisms -->
                                    <path d="M 65 140 L 77 152 L 53 152 Z" fill="none" stroke="#90caf9" stroke-width="2" opacity="0.6"/>
                                    <path d="M 85 140 L 97 152 L 73 152 Z" fill="none" stroke="#64b5f6" stroke-width="2" opacity="0.6"/>
                                </g>
                            </g>

                            <!-- Bottom insight -->
                            <text x="375" y="435" text-anchor="middle" fill="#6b7280" font-size="14" font-style="italic">Lenses are portable interpretation frameworks</text>

                            <!-- Arrow marker -->
                            <defs>
                                <marker id="arrow6" markerWidth="8" markerHeight="8" refX="7" refY="2.5" orient="auto">
                                    <path d="M0,0 L0,5 L7,2.5 z" fill="#64b5f6" />
                                </marker>
                            </defs>
                        </svg>
                    </div>
                    <div class="label">Figure: Cognitive Lens Architecture</div>
                    <div class="description">User's repeated gestures → Pattern extraction → Named "lens" (e.g., "Maya's physics notation"). Lenses can be: applied to new drawings, shared with collaborators, composed with other lenses, versioned and evolved. Your way of seeing becomes portable.</div>
                </div>
            </figure>
            
            <p>Lenses are not just personal—they can be shared, creating communities of interpretation. A research group might develop a shared lens for their domain notation. A classroom might inherit a lens from curriculum designers. Lenses become a new kind of intellectual property: not content, but <em>ways of seeing</em>.</p>
        </section>
 <!-- SECTION: THE LINEAGE -->
        <section id="lineage">
            <h2>The Lineage</h2>
            
            <p>The MetaMedium does not emerge from nothing. It stands on decades of research into human-computer interaction, sketch-based interfaces, and computational media. Understanding this lineage helps clarify what is genuinely new and what builds on proven foundations.</p>
            
           <div class="timeline-single" id="timelineSingle">

  <div class="tl-entry">
    <div class="tl-year">1960</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">MAN-COMPUTER SYMBIOSIS <span class="tl-badge visions">VISIONS</span></div>
      <div class="tl-author">J.C.R. Licklider — The foundational vision</div>
      <div class="tl-desc">"Human brains and computing machines coupled together very tightly... the resulting partnership will think as no human brain has ever thought."</div>
      <div class="tl-sig">┗━━ Cooperative interaction, not automation</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">1962</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">AUGMENTING HUMAN INTELLECT <span class="tl-badge visions">VISIONS</span></div>
      <div class="tl-author">Douglas Engelbart — Framework for human augmentation</div>
      <div class="tl-desc">"Raise the level of the capability hierarchy at which human brains operate." Research agenda that led to the 1968 demo.</div>
      <div class="tl-sig">┗━━ Augmentation, not replacement</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">1963</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">SKETCHPAD <span class="tl-badge visions">VISIONS</span></div>
      <div class="tl-author">Ivan Sutherland — A man-machine graphical communication system</div>
      <div class="tl-desc">First direct manipulation of graphical objects. Constraints, copying, instances.</div>
      <div class="tl-sig">┗━━ Drawing as computational dialogue</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">1968</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">ENGELBART <span class="tl-badge visions">VISIONS</span></div>
      <div class="tl-author">Douglas Engelbart — The augmentation of human intellect</div>
      <div class="tl-desc">Mouse, hypertext, real-time collaboration, video conferencing.</div>
      <div class="tl-sig">┗━━ Tools for thought, not productivity</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">1972</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">DYNABOOK <span class="tl-badge visions">VISIONS</span></div>
      <div class="tl-author">Alan Kay — A personal computer for children of all ages</div>
      <div class="tl-desc">Dynamic media, end-user programming, learning through building.</div>
      <div class="tl-sig">┗━━ The computer as metamedium</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">1993</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">PAD++ <span class="tl-badge recognition">RECOGNITION</span></div>
      <div class="tl-author">Ken Perlin & David Fox — Zooming graphical interface for alternate interface physics</div>
      <div class="tl-desc">Semantic zoom, infinite canvas, spatial navigation.</div>
      <div class="tl-sig">┗━━ Space as interface</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">1996</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">SILK <span class="tl-badge recognition">RECOGNITION</span></div>
      <div class="tl-author">James Landay — Sketching Interfaces Like Krazy</div>
      <div class="tl-desc">Hand-drawn UI mockups recognized and made interactive.</div>
      <div class="tl-sig">┗━━ Sketch-to-prototype pipeline</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2004</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">SKETCHREAD <span class="tl-badge recognition">RECOGNITION</span></div>
      <div class="tl-author">Christine Alvarado & Randall Davis — A multi-domain sketch recognition engine</div>
      <div class="tl-desc">Hierarchical shape recognition from strokes to complex diagrams.</div>
      <div class="tl-sig">┗━━ Domain-independent sketch parsing</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2007</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">PHYSICSBOOK <span class="tl-badge recognition">RECOGNITION</span></div>
      <div class="tl-author">Saad Cheema & Joseph LaViola — A sketch-based physics tutoring system</div>
      <div class="tl-desc">Draw physics diagrams that simulate automatically.</div>
      <div class="tl-sig">┗━━ Educational sketch recognition</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2011</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">PAPER <span class="tl-badge visions">VISIONS</span></div>
      <div class="tl-author">FiftyThree — Expressive tools for visual thinking</div>
      <div class="tl-desc">Consumer app bringing gestural, expressive digital drawing to iPad.</div>
      <div class="tl-sig">┗━━ Mass-market appetite for sketch interfaces</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2011</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">SHADOWDRAW <span class="tl-badge recognition">RECOGNITION</span></div>
      <div class="tl-author">Yong Jae Lee et al. — Real-time user guidance for freehand drawing</div>
      <div class="tl-desc">System suggests strokes based on partial input; helps users draw better.</div>
      <div class="tl-sig">┗━━ AI as drawing collaborator</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2012</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">INVENTING ON PRINCIPLE <span class="tl-badge visions">VISIONS</span></div>
      <div class="tl-author">Bret Victor — Creators need immediate connection to what they create</div>
      <div class="tl-desc">Live feedback, direct manipulation, visible state.</div>
      <div class="tl-sig">┗━━ The principle that drives MetaMedium</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2014</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">JUXTAPOZE <span class="tl-badge recognition">RECOGNITION</span></div>
      <div class="tl-author">Andrew Head et al. — Supporting serendipity and creative expression</div>
      <div class="tl-desc">Suggest related visual elements during composition; semantic associations.</div>
      <div class="tl-sig">┗━━ Canvas that offers possibilities</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2014</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">KITTY / DRACO <span class="tl-badge recognition">RECOGNITION</span></div>
      <div class="tl-author">Rubaiat Kazi et al. — Sketch-based animation tools</div>
      <div class="tl-desc">Draw characters and motions; system brings them to life.</div>
      <div class="tl-sig">┗━━ Sketch as animation input</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2015</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">CHALKTALK <span class="tl-badge chalktalk">VISIONS × RECOGNITION</span></div>
      <div class="tl-author">Ken Perlin — Thinking by drawing, drawing by thinking</div>
      <div class="tl-desc">Recognized sketches become live simulations; linked behaviors.</div>
      <div class="tl-sig">┗━━ Semantic sketching — the direct ancestor</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2018</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">DYNAMICLAND <span class="tl-badge visions">VISIONS</span></div>
      <div class="tl-author">Bret Victor et al. — A communal computer</div>
      <div class="tl-desc">Physical paper with computational behavior; no screens.</div>
      <div class="tl-sig">┗━━ Computing escapes the rectangle</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2018</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">DATAINK <span class="tl-badge recognition">RECOGNITION</span></div>
      <div class="tl-author">Haijun Xia et al. — Direct pen and touch data visualization</div>
      <div class="tl-desc">Draw charts that bind to data; gestural visualization authoring.</div>
      <div class="tl-sig">┗━━ Sketch-based data viz</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2023</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">GRAPHOLOGUE <span class="tl-badge intelligence">INTELLIGENCE</span></div>
      <div class="tl-author">Peiling Jiang et al. — Exploring LLM responses with interactive diagrams</div>
      <div class="tl-desc">Text responses converted to node-link diagrams in real-time.</div>
      <div class="tl-sig">┗━━ Diagrammatic dialogue with AI</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2023</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">SENSECAPE <span class="tl-badge intelligence">INTELLIGENCE</span></div>
      <div class="tl-author">Sangho Suh et al. — Enabling multilevel exploration of web with LLMs</div>
      <div class="tl-desc">Hierarchical concept maps for navigating AI-generated content.</div>
      <div class="tl-sig">┗━━ Spatial LLM exploration</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2023</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">MAKE-REAL <span class="tl-badge intelligence">INTELLIGENCE</span></div>
      <div class="tl-author">tldraw — Sketch to working UI</div>
      <div class="tl-desc">Draw a wireframe, LLM generates functional code.</div>
      <div class="tl-sig">┗━━ Sketch-to-application via AI</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2024</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">DRAWTALKING <span class="tl-badge intelligence">INTELLIGENCE</span></div>
      <div class="tl-author">Eyal Rosenberg et al. — Building interactive worlds by sketching and speaking</div>
      <div class="tl-desc">Multimodal creation of interactive scenes. Ken Perlin co-author.</div>
      <div class="tl-sig">┗━━ Chalktalk lineage continues</div>
    </div>
  </div>

  <div class="tl-entry">
    <div class="tl-year">2024</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">TLDRAW COMPUTER <span class="tl-badge intelligence">INTELLIGENCE</span></div>
      <div class="tl-author">tldraw — AI as canvas participant</div>
      <div class="tl-desc">Autonomous agent that can see, draw, and use the canvas.</div>
      <div class="tl-sig">┗━━ Bidirectional human-AI canvas</div>
    </div>
  </div>

  <div class="tl-entry tl-convergence">
    <div class="tl-year">2025</div>
    <div class="tl-pipe">┃</div>
    <div class="tl-content">
      <div class="tl-title">METAMEDIUM <span class="tl-badge convergence">CONVERGENCE</span></div>
      <div class="tl-author">All threads merge</div>
      <div class="tl-synthesis">
        <strong>Synthesizes:</strong>
        <ul>
          <li><strong>From Visions:</strong> Dynabook's metamedium concept + Victor's directness principle</li>
          <li><strong>From Recognition:</strong> Sketch-editing games' negotiation paradigm</li>
          <li><strong>From Intelligence:</strong> LLM interpretation + probabilistic reasoning</li>
        </ul>
      </div>
      <div class="tl-sig">┗━━ Drawing becomes a shared language between human and AI, enabling genuine collaboration through rich symbolic exchange</div>
    </div>
  </div>

</div>

<style>
/* ===== NEW SINGLE-COLUMN TIMELINE ===== */
.timeline-single {
  --visions-color: #3b82f6;
  --recognition-color: #8b5cf6;
  --intelligence-color: #10b981;
  --chalktalk-color: #f59e0b;
  --convergence-color: #f59e0b;
  --bg: #f8f6f1;
  --text: #1a1a2e;
  --muted: #666;
  --border: #d4d4d4;
  --pipe: #999;

  font-family: 'SF Mono', 'Monaco', 'Courier New', monospace;
  font-size: 0.9375rem;
  background: var(--bg);
  border-radius: 12px;
  padding: 1.5rem;
  margin: 1rem 0;
  color: var(--text);
  max-width: 900px;
}

.tl-entry {
  display: grid;
  grid-template-columns: 60px 20px 1fr;
  gap: 1rem;
  margin-bottom: 1.3rem;
  position: relative;
}

.tl-year {
  font-weight: 700;
  font-size: 1.125rem;
  color: var(--text);
  text-align: right;
  line-height: 1.3;
}

.tl-pipe {
  color: var(--pipe);
  font-size: 1.25rem;
  line-height: 1;
  user-select: none;
}

.tl-content {
  padding-bottom: 0.5rem;
}

.tl-title {
  font-weight: 700;
  font-size: 1rem;
  letter-spacing: 0.02em;
  margin-bottom: 0.5rem;
  color: var(--text);
  display: flex;
  align-items: center;
  gap: 0.75rem;
  flex-wrap: wrap;
}

.tl-badge {
  display: inline-block;
  padding: 0.25rem 0.625rem;
  border-radius: 12px;
  font-size: 0.6875rem;
  font-weight: 600;
  letter-spacing: 0.05em;
  white-space: nowrap;
}

.tl-badge.visions {
  background: rgba(59, 130, 246, 0.15);
  color: var(--visions-color);
  border: 1px solid var(--visions-color);
}

.tl-badge.recognition {
  background: rgba(139, 92, 246, 0.15);
  color: var(--recognition-color);
  border: 1px solid var(--recognition-color);
}

.tl-badge.intelligence {
  background: rgba(16, 185, 129, 0.15);
  color: var(--intelligence-color);
  border: 1px solid var(--intelligence-color);
}

.tl-badge.chalktalk {
  background: rgba(245, 158, 11, 0.15);
  color: var(--chalktalk-color);
  border: 1px solid var(--chalktalk-color);
  font-size: 0.625rem;
}

.tl-badge.convergence {
  background: rgba(245, 158, 11, 0.2);
  color: var(--convergence-color);
  border: 2px solid var(--convergence-color);
  font-weight: 700;
}

.tl-author {
  font-size: 0.9375rem;
  color: var(--text);
  margin-bottom: 0.375rem;
  line-height: 1.5;
}

.tl-desc {
  font-size: 0.875rem;
  color: var(--muted);
  line-height: 1.6;
  margin-bottom: 0.5rem;
}

.tl-sig {
  font-size: 0.8125rem;
  font-style: italic;
  color: var(--muted);
  line-height: 1.5;
  padding-left: 1.25rem;
}

/* MetaMedium convergence entry */
.tl-convergence {
  background: linear-gradient(135deg, rgba(59, 130, 246, 0.05), rgba(245, 158, 11, 0.08));
  border: 2px solid var(--convergence-color);
  border-radius: 12px;
  padding: 1.5rem;
  margin: 2rem 0;
  grid-template-columns: 1fr;
}

.tl-convergence .tl-year {
  text-align: left;
  font-size: 1.25rem;
  color: var(--convergence-color);
  margin-bottom: 0.5rem;
}

.tl-convergence .tl-pipe {
  display: none;
}

.tl-convergence .tl-title {
  font-size: 1.375rem;
  margin-bottom: 0.75rem;
}

.tl-synthesis {
  margin: 1rem 0;
  padding: 1rem;
  background: rgba(255, 255, 255, 0.5);
  border-radius: 8px;
  border-left: 3px solid var(--convergence-color);
}

.tl-synthesis strong {
  color: var(--text);
}

.tl-synthesis ul {
  margin: 0.75rem 0 0 0;
  padding-left: 1.25rem;
  list-style: none;
}

.tl-synthesis li {
  margin-bottom: 0.5rem;
  line-height: 1.6;
  position: relative;
}

.tl-synthesis li::before {
  content: "→";
  position: absolute;
  left: -1.25rem;
  color: var(--convergence-color);
}

/* Mobile */
@media (max-width: 768px) {
  .timeline-single {
    padding: 1rem;
    font-size: 0.875rem;
  }

  .tl-entry {
    grid-template-columns: 50px 15px 1fr;
    gap: 0.75rem;
    margin-bottom: 1.25rem;
  }

  .tl-year {
    font-size: 1rem;
  }

  .tl-title {
    font-size: 0.9375rem;
  }

  .tl-author {
    font-size: 0.875rem;
  }

  .tl-desc {
    font-size: 0.8125rem;
  }

  .tl-convergence {
    padding: 1rem;
    grid-template-columns: 1fr;
    display: block;
  }

  .tl-convergence .tl-title {
    font-size: 1.125rem;
  }

  .tl-synthesis {
    margin: 0.75rem 0;
    padding: 0.75rem;
  }

  .tl-synthesis ul {
    padding-left: 1rem;
  }
}

/* Hide old timeline styles */
.timeline-columns, .tc-mobile, .tc-card {
  display: none !important;
}

/* Header */
.tc-header {
  display: grid;
  grid-template-columns: 1fr 1fr 1fr;
  gap: 1.5rem;
  margin-bottom: 0.625rem;
}

.tc-col-head {
  font-size: 0.875rem;
  font-weight: 600;
  letter-spacing: 0.12em;
  color: var(--muted);
  padding-bottom: 0.625rem;
  border-bottom: 1px solid var(--border);
}

.tc-col-head:nth-child(1) { color: var(--v); border-color: var(--v); }
.tc-col-head:nth-child(2) { color: var(--r); border-color: var(--r); }
.tc-col-head:nth-child(3) { color: var(--i); border-color: var(--i); }

/* Body */
.tc-body {
  display: grid;
  grid-template-columns: 1fr 1fr 1fr;
  gap: 1.5rem;
  padding: 1rem 0;
  border-bottom: 1px solid var(--border);
}

.tc-column {
  display: flex;
  flex-direction: column;
  gap: 0.375rem;
}

/* Items */
.tc-item {
  padding: 0.3125rem 0;
  cursor: pointer;
  transition: color 0.15s ease;
  position: relative;
  white-space: nowrap;
}

.tc-item:hover {
  color: white;
}

.visions .tc-item:hover { color: var(--v); }
.recognition .tc-item:hover { color: var(--r); }
.intelligence .tc-item:hover { color: var(--i); }

.tc-year {
  color: var(--muted);
  margin-right: 0.625rem;
}

.tc-item:hover .tc-year {
  color: inherit;
}

/* Spacers */
.tc-spacer { height: 3rem; }
.tc-spacer-top { height: 2.8rem; }
.tc-spacer-small { height: 1rem; }
.tc-spacer-large { height: 14.5rem; }

/* Chalktalk arrow */
.tc-item.chalktalk {
  position: relative;
}

.tc-arrow {
  position: absolute;
  left: 100%;
  top: 50%;
  width: calc(100% + 1.5rem);
  height: 1px;
  background: var(--muted);
  margin-left: 0.625rem;
}

.tc-arrow::after {
  content: '';
  position: absolute;
  right: 0;
  top: -4px;
  border: 5px solid transparent;
  border-left-color: var(--muted);
}

/* Footer */
.tc-footer {
  padding-top: 1.25rem;
  text-align: center;
}

.tc-metamedium {
  display: inline-block;
  font-size: 1.25rem;
}

.tc-metamedium .tc-year {
  background: linear-gradient(90deg, var(--v), var(--r), var(--i));
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  background-clip: text;
}

.tc-subtitle {
  font-size: 0.9375rem;
  color: var(--muted);
  margin-top: 0.25rem;
}

/* Hover Card */
.tc-card {
  position: absolute;
  width: 280px;
  background: #1a1a1e;
  border: 1px solid var(--border);
  border-radius: 6px;
  padding: 1.125rem;
  box-shadow: 0 8px 24px rgba(0,0,0,0.4);
  opacity: 0;
  visibility: hidden;
  transition: opacity 0.15s ease, visibility 0.15s ease;
  z-index: 100;
  pointer-events: none;
}

.tc-card.visible {
  opacity: 1;
  visibility: visible;
}

.tc-card-title {
  font-family: 'Space Grotesk', system-ui, sans-serif;
  font-size: 1rem;
  font-weight: 600;
  color: white;
  margin-bottom: 0.3125rem;
}

.tc-card-subtitle {
  font-size: 0.875rem;
  font-style: italic;
  color: var(--muted);
  margin-bottom: 0.625rem;
}

.tc-card-desc {
  font-family: 'Space Grotesk', system-ui, sans-serif;
  font-size: 0.9375rem;
  color: var(--text);
  line-height: 1.5;
  margin-bottom: 0.625rem;
}

.tc-card-est {
  font-size: 0.875rem;
  font-weight: 600;
}

.tc-card.visions .tc-card-est { color: var(--v); }
.tc-card.recognition .tc-card-est { color: var(--r); }
.tc-card.intelligence .tc-card-est { color: var(--i); }

/* Mobile list - hidden on desktop */
.tc-mobile {
  display: none;
}

/* Responsive */
@media (max-width: 640px) {
  .timeline-columns {
    padding: 1.25rem;
  }
  
  /* Hide desktop layout */
  .tc-header,
  .tc-body,
  .tc-footer {
    display: none;
  }
  
  /* Show mobile layout */
  .tc-mobile {
    display: block;
  }
  
  .tc-mobile-legend {
    display: flex;
    gap: 1rem;
    padding-bottom: 0.75rem;
    margin-bottom: 0.75rem;
    border-bottom: 1px solid var(--border);
    font-size: 0.75rem;
    color: var(--muted);
    flex-wrap: wrap;
  }
  
  .tc-mobile-legend span {
    display: inline-flex;
    align-items: center;
    gap: 0.375rem;
  }
  
  .tc-mobile-legend .dot {
    width: 8px;
    height: 8px;
    border-radius: 50%;
  }
  
  .tc-mobile-legend .dot.v { background: var(--v); }
  .tc-mobile-legend .dot.r { background: var(--r); }
  .tc-mobile-legend .dot.i { background: var(--i); }
  
  .tc-mobile-list {
    display: flex;
    flex-direction: column;
    gap: 0.1rem;
  }
  
  .tc-m-item {
    display: flex;
    align-items: center;
    gap: 0.625rem;
    padding: 0.2rem 0.3;
    font-size: 1.2rem;
    cursor: pointer;
    transition: color 0.15s ease;
  }
  
  .tc-m-item:hover {
    color: white;
  }
  
  .tc-m-item.v:hover { color: var(--v); }
  .tc-m-item.r:hover { color: var(--r); }
  .tc-m-item.i:hover { color: var(--i); }
  .tc-m-item.c:hover,
  .tc-m-item.m:hover { color: var(--r); }
  
  .tc-m-dot {
    width: 10px;
    height: 10px;
    border-radius: 50%;
    flex-shrink: 0;
  }
  
  .tc-m-dot.v { background: var(--v); }
  .tc-m-dot.r { background: var(--r); }
  .tc-m-dot.i { background: var(--i); }
  .tc-m-dot.c { background: linear-gradient(135deg, var(--v), var(--r)); }
  .tc-m-dot.m { 
    background: linear-gradient(135deg, var(--v), var(--r), var(--i)); 
    width: 12px;
    height: 12px;
  }
  
  .tc-m-year {
    color: var(--muted);
    font-size: 0.875rem;
    min-width: 3rem;
  }
  
  .tc-m-item:hover .tc-m-year {
    color: inherit;
  }
  
  .tc-m-name {
    flex: 1;
  }
  
  .tc-m-sub {
    font-size: 0.75rem;
    color: var(--muted);
    margin-left: 0.25rem;
  }
  
  .tc-m-item.m {
    margin-top: 0.5rem;
    padding-top: 0.75rem;
    border-top: 1px solid var(--border);
  }
  
  .tc-m-item.m .tc-m-year {
    background: linear-gradient(90deg, var(--v), var(--r), var(--i));
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
  }
  
  /* Card adjustments for mobile */
  .tc-card {
    width: 290px;
    padding: 0.875rem;
    left: 50% !important;
    transform: translateX(-50%);
  }
  
  .tc-card-title { font-size: 0.875rem; }
  .tc-card-subtitle { font-size: 0.75rem; }
  .tc-card-desc { font-size: 0.8125rem; }
  .tc-card-est { font-size: 0.75rem; }
}
</style>

<script>
(function() {
  const container = document.getElementById('timelineColumns');
  const card = document.getElementById('tcCard');
  const items = container.querySelectorAll('.tc-item, .tc-m-item');
  
  items.forEach(item => {
    item.addEventListener('mouseenter', (e) => {
      const info = item.dataset.info;
      if (!info) return;
      
      const [title, subtitle, desc, est] = info.split('|');
      card.querySelector('.tc-card-title').textContent = title;
      card.querySelector('.tc-card-subtitle').textContent = subtitle;
      card.querySelector('.tc-card-desc').textContent = desc;
      card.querySelector('.tc-card-est').textContent = est;
      
      // Determine thread for coloring
      card.classList.remove('visions', 'recognition', 'intelligence');
      const col = item.closest('.tc-column');
      if (col) {
        if (col.classList.contains('visions')) card.classList.add('visions');
        if (col.classList.contains('recognition')) card.classList.add('recognition');
        if (col.classList.contains('intelligence')) card.classList.add('intelligence');
      } else {
        // Mobile item - check class
        if (item.classList.contains('v')) card.classList.add('visions');
        if (item.classList.contains('r')) card.classList.add('recognition');
        if (item.classList.contains('i')) card.classList.add('intelligence');
        if (item.classList.contains('c')) card.classList.add('recognition');
      }
      
      // Position card
      const rect = item.getBoundingClientRect();
      const containerRect = container.getBoundingClientRect();
      
      // Check if mobile (card will be centered via CSS)
      if (window.innerWidth <= 640) {
        card.style.left = '';
        card.style.top = (rect.top - containerRect.top - 120) + 'px';
        if (rect.top - containerRect.top < 130) {
          card.style.top = (rect.bottom - containerRect.top + 10) + 'px';
        }
      } else {
        let left = rect.right - containerRect.left + 16;
        let top = rect.top - containerRect.top - 10;
        
        if (left + 280 > containerRect.width) {
          left = rect.left - containerRect.left - 296;
        }
        if (top + 160 > containerRect.height) {
          top = containerRect.height - 170;
        }
        if (top < 0) top = 10;
        
        card.style.left = left + 'px';
        card.style.top = top + 'px';
      }
      
      card.classList.add('visible');
    });
    
    item.addEventListener('mouseleave', () => {
      card.classList.remove('visible');
    });
  });
})();
</script>

        </section>

        <!-- SECTION 6: CURRENT DEVELOPMENT -->
        <section id="development">
            <h2>Current Development</h2>
            
            <p>This whitepaper accompanies active development of a working prototype demonstrating the core principles. The demo—internally called "Doodle 2.0"—implements:</p>
            
            <ul>
                <li><strong>Shape recognition with learning:</strong> Draw primitives (circles, rectangles, lines); teach custom shapes by naming them; the system remembers</li>
                <li><strong>Spatial relationship detection:</strong> Intersection, containment, adjacency, and alignment are detected and visualized in real-time</li>
                <li><strong>Composition matching:</strong> Combine learned shapes into patterns; the system recognizes compositions</li>
                <li><strong>Refinement controls:</strong> Adjust recognition thresholds; see confidence scores; correct misrecognitions</li>
            </ul>
            
            <p>The interactive canvas at the top of this page demonstrates a subset of these capabilities—you can draw shapes and see recognition, auto-completion, and relationship detection in action.</p>
            
            <div class="demo-container">
                <div class="demo-header">
                    <span>Interactive Demo</span>
                    <span><a href="https://jjh111.github.io/MetaMedium/doodle2-canvas.html" target="_blank" rel="noopener"><strong></strong>Try it→</strong></a></span>
                </div>
                <iframe src="https://jjh111.github.io/MetaMedium/doodle2-canvas.html" title="MetaMedium Demo" loading="lazy"></iframe>
            </div>
            
            <div class="callout">
                <h4>Full Demo & Source</h4>
                <p>The complete prototype is available as a standalone web page. Development continues in the repository.</p>
                <p><strong>Development Demo:</strong> <a href="https://jjh111.github.io/MetaMedium/doodle2-canvas.html" target="_blank" rel="noopener">jjh111.github.io/doodle2-Canvas</a>
                    <br>
                <strong>GitHub:</strong> <a href="https://github.com/jjh111/MetaMedium/tree/master" target="_blank" rel="noopener">github.com/jjh111/MetaMedium</a><br>
                <strong>License:</strong> GPL — the MetaMedium framework is open source.</p>
            </div>
            
            <h3>Development Roadmap</h3>
            
            <ul>
                <li><strong>Current:</strong> Core shape recognition, spatial relationships, custom shape learning, composition detection</li>
                <li><strong>Next:</strong> Annotation-as-execution (text near shapes triggers behaviors), improved gesture recognition, undo/redo with branching history</li>
                <li><strong>Future:</strong> Multi-user collaboration, cognitive lens export/import, LLM integration for natural language annotation, speech input (following DrawTalking's approach), 3D/4D canvas navigation (z-axis as semantic distance, temporal versioning)</li>
            </ul>
            
            <h3>Open Questions</h3>
            
            <p>The MetaMedium framework raises questions that can only be answered through building and testing:</p>
            
            <ul>
                <li>How do we design for "interpretive ambiguity" without creating confusion? What's the right balance between holding possibilities open and committing to interpretation?</li>
                <li>What are the limits of gesture vocabulary before cognitive overhead exceeds benefit? How many "words" can a visual language productively contain?</li>
                <li>How can "cognitive lenses" be effectively shared between users? What's lost in translation when one person's way of seeing meets another's?</li>
                <li>What patterns emerge in human-AI co-creation that neither intelligence generates alone? Can we characterize genuinely collaborative cognition?</li>
                <li>Can visible reasoning on a shared canvas measurably improve AI alignment outcomes? This is empirically testable.</li>
            </ul>
            
            <h3>Limitations and Challenges</h3>
            
            <ul>
                <li><strong>Recognition accuracy:</strong> Current shape recognition, even with ML, makes mistakes. The negotiation paradigm helps—but users may lose patience with too many corrections. Finding the threshold where recognition is "good enough" is empirical work.</li>
                <li><strong>Cognitive load:</strong> Every gestural vocabulary is a language to learn. There's a real risk that MetaMedium becomes its own expertise barrier, replacing "learn to code" with "learn our gestures." Keeping the learning curve gentle while enabling power is a design challenge.</li>
                <li><strong>Privacy of patterns:</strong> If the canvas learns from your drawing patterns, those patterns become data. "Cognitive lenses" are intimate—they encode how you think. The system must be designed with privacy as foundational, not afterthought.</li>
                <li><strong>Over-automation risk:</strong> "The system guessed wrong and did something I didn't want" is a real failure mode. Undo must be instant and obvious. Interpretations must be inspectable before they execute. The user must remain in control.</li>
                <li><strong>Evaluation difficulty:</strong> How do we measure success? Traditional usability metrics may not capture "quality of thought." New evaluation frameworks are needed.</li>
            </ul>
            
            <h4>Abstraction Management and Learning Dynamics</h4>
            
            <p>The promise of a canvas that learns creates its own challenges. Vocabulary bloat is inevitable: every learned pattern accumulates, and the system has no natural way to forget. Over time, old notations compete with new ones, recognition slows, and the interpretation space becomes cluttered. Yet aggressive pruning risks destroying hard-won understanding—how do you distinguish "stale" from "rarely-used-but-important"?</p>
            
            <p>More subtle is the local minima problem. A system well-fitted to your previous way of thinking may resist your attempts to evolve. When you try new notation or reframe a concept, the system "corrects" you back toward familiar patterns. The learning becomes a cage. This is especially acute for discontinuous growth—the student becoming expert, the breakthrough insight that requires abandoning old frames. Human cognition undergoes phase transitions; the canvas must accommodate leaps, not just gradual drift.</p>
            
            <p>Possible mitigations include explicit "unlearn" gestures, decay functions with different rates for core vs. peripheral vocabulary, "lens snapshots" that version-control ways of seeing, or detection of systematic deviation as a signal that the user is trying to break free. But the deeper question remains: is the canvas a memory of what you've done, or a partner in what you're becoming? The answer shapes the entire architecture.</p>

            <h3>Gallery</h3>
            
            <div class="gallery">
                <figure class="gallery-item">
                    <div class="image">
                        <img src="./Assets/gal-notes-systemdiagram2016.jpeg" alt="System architecture diagram">
                    </div>
                    <figcaption>System architecture diagram showing component relationships</figcaption>
                </figure>
                <figure class="gallery-item">
                    <div class="image">
                        <img src="./Assets/gal-notes-activitymodel2016.png" alt="Activity theory model">
                    </div>
                    <figcaption>Activity theory model for human-computer interaction</figcaption>
                </figure>
                <figure class="gallery-item">
                    <div class="image">
                        <img src="./Assets/gal-notes-sentance.png" alt="Sentence annotation framework">
                    </div>
                    <figcaption>Conceptual framework for sentence-level annotation</figcaption>
                </figure>
                <figure class="gallery-item">
                    <div class="image">
                        <img src="./Assets/gal-notes-planestocube.png" alt="2D to 3D transformation">
                    </div>
                    <figcaption>Visualization of 2D to 3D transformation concept</figcaption>
                </figure>
                <figure class="gallery-item">
                    <div class="image">
                        <img src="./Assets/gal-notes-memoryassociaton.png" alt="Memory association network">
                    </div>
                    <figcaption>Memory association network diagram</figcaption>
                </figure>
                <figure class="gallery-item">
                    <div class="image">
                        <img src="./Assets/gal-notes-conceptualframeworks Large.jpeg" alt="Design thinking frameworks">
                    </div>
                    <figcaption>Conceptual frameworks for design thinking</figcaption>
                </figure>
                <figure class="gallery-item">
                    <div class="image">
                        <img src="./Assets/gal-notes-artefactsofprocess.jpeg" alt="Process artifacts">
                    </div>
                    <figcaption>Artifacts of the design and development process</figcaption>
                </figure>
                <figure class="gallery-item">
                    <div class="image">
                        <img src="./Assets/gal-notes-useractivitymodel2016.png" alt="User activity model">
                    </div>
                    <figcaption>User activity model showing interaction patterns</figcaption>
                </figure>
            </div>
        </section>

        <!-- SECTION 7: THE PRACTICE -->
        <section id="scenarios">
            <h2>Scenarios</h2>
            
            <p>The framework becomes concrete through scenarios. Each demonstrates specific principles in action.</p>
            
            <div class="use-case">
                <h4>Visual Learning</h4>
                <p class="principles-used"><strong>Principles:</strong> Space is semantic, Canvas learns, Bidirectional representation</p>
                <p class="scenario">Visual thinker draws parabolas; system connects spatial intuition to formal equations bidirectionally. Discovers he understood calculus all along—just needed symbols connected to drawings.
                    <a href="https://open.substack.com/pub/johnhanacek/p/a-day-with-metamedium-62d" target="_blank">Read Story</a></a>
                </p>
            </div>
            
            <div class="use-case">
                <h4>Asymmetric Collaboration</h4>
                <p class="principles-used"><strong>Principles:</strong> Interpretive ambiguity, Cognitive lenses, Negotiation</p>
                <p class="scenario">Seven-year-old's playful "bouncy bridge" sketch becomes engineering student's seismic dampening simulation. Canvas holds both interpretations—intuitive play and rigorous analysis—without translation.
                    <a href="https://open.substack.com/pub/johnhanacek/p/a-day-with-metamedium-96c" target="_blank">Read Story</a></a>
                </p>
            </div>
            
            <div class="use-case">
                <h4>Rapid Prototyping</h4>
                <p class="principles-used"><strong>Principles:</strong> Annotation becomes execution, Negotiation paradigm</p>
                <p class="scenario">Non-programmer sketches water tanks, annotates flow logic. System generates simulation, asks clarifying questions, updates as she refines. Continuous negotiation from rough idea to working prototype—no code.
                    <a href="https://open.substack.com/pub/johnhanacek/p/a-day-with-metamedium" target="_blank">Read Story</a></a>
                </p>
            </div>
            
            <div class="use-case">
                <h4>Scientific Collaboration</h4>
                <p class="principles-used"><strong>Principles:</strong> Space is semantic, Annotation becomes execution, Shared lenses</p>
                <p class="scenario">Researchers sketch faster than formal notation allows. Spatial annotations like "defect here?" trigger simulations. Shared research lens interprets their shorthand.
                    <a href="https://open.substack.com/pub/johnhanacek/p/a-day-with-metamedium-147" target="_blank">Read Story</a></a>
                </p>
            </div>
        </section>

        <!-- SECTION 8: THE FUTURE -->
        <section id="future">
            <h2>The Future</h2>
            
        
            
            <h3>External Imagination</h3>
            
            <p>Perhaps the most apt description of the human role in MetaMedium interaction is as <em>navigator, conductor, explorer, friend to possibility</em>. The human has direction—knowing which possibilities matter. The AI has generative capability—holding multiple possibilities at once.</p>
            
            <p>The AI becomes an "external imagination"—not doing the thinking for the human, but <em>thinking with</em> the human. This is not automation, not replacement, but <strong>augmentation</strong>. Making human capability larger.</p>
            
            <h3>Reframing Intelligence</h3>
            
            <p>The term "Artificial Intelligence" carries unfortunate connotations: fake, simulated, lesser-than. Perhaps a more useful frame: <strong>Authentic Intelligence</strong>—real, valid, different-but-equal.</p>
            
            <p>Human intelligence is biological, embodied, temporal, mortal, with agency shaped by survival pressures. AI intelligence is computational, distributed, instant, ephemeral, with agency shaped by training and human direction. Both are genuine forms of intelligence. Both have distinct capabilities. The question is not which is "better" but how they can combine to create capability neither possesses alone.</p>

            <h3>Alignment Through Communication</h3>
            
            <p>Current approaches to AI alignment focus primarily on control: rules, guardrails, constraints, limitations. The assumption is that AI must be contained because its goals might diverge from human goals.</p>
            
            <p>The MetaMedium proposes an alternative: instead of constraining AI's internal state through rules, we <em>enrich the medium between us</em> so that coordination happens through genuine communication. The richer the vocabulary we share—the more kinds of sign vehicles available for exchange—the better we can align our understanding.</p>
            
            <figure>
                <div class="placeholder-figure">
                    <div class="image">
                        <svg viewBox="0 0 750 400" xmlns="http://www.w3.org/2000/svg">
                            <!-- Title -->
                            <text x="375" y="25" text-anchor="middle" fill="#7b8a9a" font-size="15" opacity="1">Two paradigms for AI alignment</text>

                            <!-- Left: Traditional -->
                            <g>
                                <rect x="40" y="50" width="330" height="310" fill="rgba(74, 85, 104, 0.15)" stroke="#6b7280" stroke-width="2" rx="6"/>
                                <text x="205" y="75" text-anchor="middle" fill="#8b9aaa" font-size="16" font-weight="bold">TRADITIONAL ALIGNMENT</text>

                                <!-- Rules imposed from above -->
                                <g transform="translate(205, 100)">
                                    <rect x="-50" y="0" width="100" height="60" fill="rgba(74, 85, 104, 0.4)" stroke="#6b7280" stroke-width="2" rx="4"/>
                                    <text x="0" y="15" text-anchor="middle" fill="rgba(20, 25, 35, 0.8)" font-size="14" font-weight="bold">RULES</text>
                                    <text x="0" y="31" text-anchor="middle" fill="#d1495b" font-size="13">STOP</text>
                                    <text x="0" y="44" text-anchor="middle" fill="#d1495b" font-size="13">DON'T</text>
                                    <text x="0" y="57" text-anchor="middle" fill="#d1495b" font-size="13">NEVER</text>
                                </g>

                                <!-- Arrow down -->
                                <line x1="205" y1="175" x2="205" y2="205" stroke="#6b7280" stroke-width="2" marker-end="url(#arrow7)"/>

                                <!-- Black box AI -->
                                <g transform="translate(205, 220)">
                                    <rect x="-55" y="0" width="110" height="70" fill="rgba(20, 25, 35, 0.8)" stroke="#4a5568" stroke-width="2" rx="4"/>
                                    <text x="0" y="25" text-anchor="middle" fill="#6b7280" font-size="20">🧠</text>
                                    <text x="0" y="44" text-anchor="middle" fill="#6b7280" font-size="14">AI</text>
                                    <text x="0" y="60" text-anchor="middle" fill="#6b7280" font-size="12" opacity="0.9">?????</text>
                                    <text x="70" y="40" fill="#rgba(20, 25, 35, 0.8)" font-size="14" opacity="0.6">Black Box</text>
                                </g>

                                <!-- User watching from outside -->
                                <text x="80" y="268" fill="#9ca3af" font-size="14">👤</text>
                                <text x="65" y="285" fill="#9ca3af" font-size="14">Watches</text>
                                <text x="65" y="300" fill="#9ca3af" font-size="14">from "outside"</text>

                                <!-- Problems -->
                                <text x="60" y="330" fill="#d1495b" font-size="13">❌ Opaque process</text>
                                <text x="60" y="347" fill="#d1495b" font-size="13">❌ Adversarial framing</text>
                            </g>

                            <!-- Right: MetaMedium -->
                            <g transform="translate(380, 0)">
                                <rect x="40" y="50" width="330" height="310" fill="rgba(100, 181, 246, 0.08)" stroke="#64b5f6" stroke-width="2" rx="6"/>
                                <text x="205" y="75" text-anchor="middle" fill="#90caf9" font-size="16" font-weight="bold">METAMEDIUM ALIGNMENT</text>

                                <!-- Human and AI side by side -->
                                <g transform="translate(100, 95)">
                                    <!-- Human -->
                                    <circle cx="0" cy="25" r="18" fill="rgba(100, 181, 246, 0.2)" stroke="#64b5f6" stroke-width="2"/>
                                    <text x="0" y="32" text-anchor="middle" fill="#fff" font-size="18">👤</text>
                                    <text x="0" y="52" text-anchor="middle" fill="#90caf9" font-size="12">human</text>

                                    <!-- Double arrow (equals) -->
                                    <path d="M 22 25 L 183 25" stroke="#64b5f6" stroke-width="3"/>
                                    <path d="M 0,0 L0,5 L7,2.5 z" transform="translate(22, 22.5)" fill="#64b5f6"/>
                                    <path d="M0,0 L0,5 L7,2.5 z" transform="translate(176, 27.5) rotate(180)" fill="#64b5f6"/>
                                    <text x="102" y="16" text-anchor="middle" fill="#90caf9" font-size="12">shared</text>
                                    <text x="102" y="40" text-anchor="middle" fill="#90caf9" font-size="12">canvas</text>

                                    <!-- AI -->
                                    <circle cx="205" cy="25" r="18" fill="rgba(100, 181, 246, 0.2)" stroke="#64b5f6" stroke-width="2"/>
                                    <text x="205" y="32" text-anchor="middle" fill="#fff" font-size="22">≙</text>
                                    <text x="205" y="52" text-anchor="middle" fill="#90caf9" font-size="12">AI</text>
                                </g>

                                <!-- Arrows to shared canvas -->
                                <line x1="120" y1="150" x2="160" y2="180" stroke="#64b5f6" stroke-width="2" marker-end="url(#arrow8)"/>
                                <line x1="290" y1="150" x2="250" y2="180" stroke="#64b5f6" stroke-width="2" marker-end="url(#arrow8)"/>

                                <!-- Shared visible canvas -->
                                <g transform="translate(205, 200)">
                                    <rect x="-75" y="0" width="150" height="95" fill="rgba(10, 14, 26, 0.5)" stroke="#64b5f6" stroke-width="2" rx="4"/>

                                    <!-- Sketch elements showing visible reasoning -->
                                    <circle cx="-30" cy="25" r="13" fill="none" stroke="#90caf9" stroke-width="1.5"/>
                                    <line x1="-17" y1="25" x2="10" y2="25" stroke="#90caf9" stroke-width="1.5" marker-end="url(#arrow8)"/>
                                    <line x1="-30" y1="38" x2="-45" y2="55" stroke="#90caf9" stroke-width="2"/>
                                    <line x1="-30" y1="38" x2="-15" y2="55" stroke="#90caf9" stroke-width="2"/>
                                    <rect x="-50" y="55" width="18" height="18" fill="none" stroke="#90caf9" stroke-width="1.5"/>
                                    <path d="M 5,55 L 20,68 L 5,68 Z" fill="none" stroke="#90caf9" stroke-width="1.5"/>
                                    <circle cx="35" cy="55" r="5" fill="#90caf9"/>

                                    <text x="0" y="17" text-anchor="middle" fill="#7b8a9a" font-size="11" font-style="italic">"flow process"</text>
                                    <text x="0" y="87" text-anchor="middle" fill="#7b8a9a" font-size="11">both can inspect</text>
                                </g>

                                <!-- Quote -->
                                <text x="205" y="194" text-anchor="middle" fill="#7b8a9a" font-size="14" font-style="italic">"The diagram is the treaty"</text>

                                <!-- Benefits -->
                                <text x="60" y="330" fill="#64b5f6" font-size="13">✓ Transparent reasoning</text>
                                <text x="60" y="347" fill="#64b5f6" font-size="13">✓ Collaborative process</text>
                            </g>

                            <!-- Bottom insight -->
                            <text x="375" y="385" text-anchor="middle" fill="#6b7280" font-size="14" font-style="italic">Control assumes conflict. Communication builds shared ground.</text>

                            <!-- Arrow markers -->
                            <defs>
                                <marker id="arrow7" markerWidth="8" markerHeight="8" refX="7" refY="2.5" orient="auto">
                                    <path d="M0,0 L0,5 L7,2.5 z" fill="#6b7280" />
                                </marker>
                                <marker id="arrow8" markerWidth="8" markerHeight="8" refX="7" refY="2.5" orient="auto">
                                    <path d="M0,0 L0,5 L7,2.5 z" fill="#64b5f6" />
                                </marker>
                            </defs>
                        </svg>
                    </div>
                    <div class="label">Figure: Alignment Through Shared Ground</div>
                    <div class="description">Left: Traditional alignment (AI in black box, rules imposed from outside, human hopes constraints hold). Right: MetaMedium alignment (human and AI on same canvas, reasoning visible to both, shared diagram as "treaty" both parties can read and negotiate). The diagram externalizes the alignment problem.</div>
                </div>
            </figure>
            
            <p>Current AI interfaces optimize for control. The MetaMedium optimizes for communication. This is not naïve trust—it's a different theory of how coordination happens. We coordinate with other humans not because we control their thoughts but because we share a rich medium for exchange.</p>
            
            <p>The MetaMedium is designed for this combination—not AI as tool to be commanded, but AI as partner with complementary cognition. The canvas becomes the site of collaboration between two authentic intelligences, each contributing what it does best.</p>

            <h3>Beyond 2D: Navigating Conceptual Space</h3>

            <p>The current framework treats diagrams as 2D arrangements, but diagrams are projections of higher-dimensional conceptual space. Future development could explore navigating the space a diagram lives in—not just the diagram itself. Three-dimensional visualization would give canvas elements depth: z-axis as semantic distance, uncertainty, or abstraction level. Four-dimensional (temporal) visualization would make the evolution of understanding navigable—scrub through versions, see where insight branched, experience collaborative history as visible geology.</p>

            <p>Most speculatively: latent space rendering. AI models maintain high-dimensional embedding spaces that encode meaning. What if the canvas could project these spaces, letting users see where their current sketch sits relative to possible interpretations? The "possibility graph" becomes navigable terrain; your marks become waypoints through semantic space.</p>

            <h3>The Deeper Vision</h3>
            
            <p>There is a version of this vision that goes beyond interface. Today's computing is an archaeological site—layer upon layer of abstraction, each solving problems created by the layer below, each adding distance from what the machine actually does. The jenga tower of modern software: every piece load-bearing, none removable, the whole structure increasingly precarious. Tools like Wolfram Alpha offer computational knowledge, but at such altitude that you consult rather than collaborate—you don't think <em>with</em> the computation, you query it.</p>
            
            <p>Bret Victor's "Future of Programming" reminds us that many ideas we consider new were explored in the 1960s and then abandoned. Direct manipulation. Visual programming. Goal-directed systems. The original metamedium vision—the computer as a medium that could simulate any medium—was never disproven, just buried under decades of commercial code, backward compatibility, and decisions made for reasons no one remembers.</p>
            
            <p>And now AI arrives, and what do we do? Add more layers. Generate code that runs on frameworks that call APIs that abstract over the same tower. We build higher, not deeper.</p>
            
            <p>The deeper vision asks: what if we went the other direction? The computer knowing what it can do and doing only as much as it needs to. Every operation justified. Every layer earning its existence. Endless refactoring—not just at the application level but all the way down, hardware and software co-evolving toward essential simplicity rather than capability bloat.</p>
            
            <p>AI could be the tool for this. Not generating more sediment on legacy systems, but reading the entire stack, understanding what it actually does, finding the essential operations buried under accretion. The MetaMedium canvas becomes a site for this rethinking: when you sketch a system, interpretation could mean "generate Python" or it could mean "this is fundamentally a constraint problem, here's how it maps closer to the metal." The diagram negotiates what level of abstraction the thought requires.</p>
            
            <p>The metamedium dream still lingers as ever possible. It waits beneath the APIs and the bloat, patient, ready to be excavated. The tools to dig are finally arriving.</p>
        </section>

        <!-- CONCLUSION -->
        <section id="conclusion">
            <h2>Conclusion</h2>
            
            <p>The MetaMedium represents both a technical framework and a philosophical position about the future of human-AI interaction. Its core claim is simple: we have been limited not by AI capability but by interface bandwidth. The medium between human and AI has been too impoverished to support genuine collaboration.</p>
            
            <p>By making every mark a potential sign vehicle, every spatial relationship a semantic statement, every annotation an execution—we create conditions for a new kind of partnership. The computer has always been a metamedium capable of simulating any other medium. AI has become capable of interpreting and generating across modalities. The missing piece is the interface that lets humans bring their full cognitive richness into the collaboration.</p>
            <p>We deserve to move beyond typing into text boxes. We deserve interfaces that meet us where we think. We deserve to dance and sing our intentions into being, to explore shared imagination space, to make the machinery we have built truly our partner in creation.</p>
            <!--
                  <div class="callout key">
                <h4>The Promise</h4>
                
            </div>
            
            -->
      
            <p>The MetaMedium is that interface. The demo accompanying this paper is a first step of grounded semantics, geometry and drawing: a canvas state ready for meta-mappings. Development continues. Contributions welcome.</p>

            <p>It's time to bring the computer to life at the depth of mind with the speed and intuitive action of our hands.</p>

            <!-- Interactive Demo Canvas -->
            <div class="demo-wrapper">
                <div class="demo-prompt" id="demoPrompt">Draw a <span class="highlight">fish</span></div>

                <div class="demo-canvas-container">
                    <canvas id="demoCanvas"></canvas>
                    <div class="demo-labels">
                        <div class="demo-label fish" id="fishLabel">🐟 fish</div>
                        <div class="demo-label food" id="foodLabel">🍎 food</div>
                    </div>
                </div>

                <div class="demo-controls">
                    <button class="demo-reset-btn" id="demoResetBtn">Reset</button>
                </div>

                <div class="demo-status" id="demoStatus"></div>
            </div>

        </section>
    </main>

    <footer>
        <p class="footer-title">MetaMedium</p>
        <p class="footer-subtitle">Principles for a Diagrammatic Understanding Interface</p>
        <p class="footer-subtitle">Bibliography: <a href="https://jhanacek.net/metamedium-resources-65" target="_blank" rel="noopener"><b>Resources Page</b></a></p>
        <p class="footer-copyright">© John Hanacek 2015 · <a href="https://johnhanacek.com" target="_blank" rel="noopener">JHDesign LLC</a></p>
        <p class="footer-signature"><img src="./Assets/footer-JHsig.png" alt="John Hanacek signature" style="height: 2em; opacity: 0.7;"></p>
        <p class="version">v4 · Updated December 2025 · Made using Claude Code </p>
    </footer>

    <script>
        // Navigation scroll visibility - show after scrolling past hero
        const nav = document.getElementById('nav');
        const hero = document.querySelector('.hero');
        
        function updateNavVisibility() {
            const heroBottom = hero.offsetTop + hero.offsetHeight - 60;
            nav.classList.toggle('visible', window.pageYOffset > heroBottom);
        }
        
        window.addEventListener('scroll', updateNavVisibility, { passive: true });
        window.addEventListener('resize', updateNavVisibility, { passive: true });
        updateNavVisibility();

        // Mobile navigation toggle
        const navToggle = document.querySelector('.nav-toggle');
        const navMenu = document.querySelector('nav ul');
        
        navToggle.addEventListener('click', () => {
            const isOpen = navMenu.classList.toggle('open');
            navToggle.classList.toggle('active', isOpen);
            navToggle.setAttribute('aria-expanded', isOpen);
        });
        
        // Close mobile nav when clicking a link
        navMenu.querySelectorAll('a').forEach(link => {
            link.addEventListener('click', () => {
                navMenu.classList.remove('open');
                navToggle.classList.remove('active');
                navToggle.setAttribute('aria-expanded', 'false');
            });
        });

        // ============================================
        // Interactive Blueprint Canvas with Shape Recognition
        // ============================================
        const canvas = document.getElementById('heroCanvas');
        const ctx = canvas.getContext('2d');
        
        // ============================================
        // Unified Timing Constants (in ms)
        // ============================================
        const TIMING = {
            // Raw strokes
            strokeHold: 1500,       // Time before stroke starts fading
            strokeFade: 2000,       // Fade duration for strokes
            
            // Recognized shapes
            morphDuration: 400,     // Shape morph animation
            shapeHold: 4000,        // Time shape stays at full opacity
            shapeFade: 1500,        // Fade duration for shapes
            
            // Whisper labels
            labelFadeIn: 200,       // Label fade in
            labelHold: 2500,        // Label at full opacity  
            labelFade: 800,         // Label fade out
            
            // Relationships & intersections
            relationshipFade: 1200, // Relationship indicator fade
            
            // Glyph reaction
            glyphActive: 2500,      // How long glyphs react after shape
        };
        
        // Computed totals
        TIMING.strokeTotal = TIMING.strokeHold + TIMING.strokeFade;
        TIMING.shapeTotal = TIMING.shapeHold + TIMING.shapeFade;
        TIMING.labelTotal = TIMING.labelFadeIn + TIMING.labelHold + TIMING.labelFade;
        
        let isDrawing = false;
        let strokes = [];           // Raw strokes (fade out)
        let recognizedShapes = [];  // Completed shapes (persist longer)
        let whisperLabels = [];     // Floating labels
        let rawIntersections = [];  // Intersections between raw strokes
        let currentStroke = null;
        let glyphRotation = 0;      // For reactive glyphs
        let glyphPulse = 0;
        let lastShapeTime = 0;
        let ripples = [];           // Tap ripple effects
        let particles = [];         // Recognition particle bursts
        let ambientParticles = [];  // Floating ambient particles
        
        // Blueprint colors
        const colors = {
            bg: '#0a1628',
            grid: 'rgba(59, 130, 246, 0.08)',
            gridAccent: 'rgba(59, 130, 246, 0.18)',
            stroke: 'rgba(147, 197, 253, 0.85)',
            strokeComplete: 'rgba(96, 165, 250, 0.95)',
            ghost: 'rgba(147, 197, 253, 0.25)',
            glyph: 'rgba(59, 130, 246, 0.25)',
            glyphActive: 'rgba(96, 165, 250, 0.5)',
            cursor: 'rgba(251, 146, 60, 0.9)',
            glow: 'rgba(59, 130, 246, 0.4)',
            label: 'rgba(147, 197, 253, 0.7)'
        };

        // ============================================
        // Helper Functions
        // ============================================
        function getBounds(points) {
            let minX = Infinity, minY = Infinity, maxX = -Infinity, maxY = -Infinity;
            points.forEach(p => {
                minX = Math.min(minX, p.x);
                minY = Math.min(minY, p.y);
                maxX = Math.max(maxX, p.x);
                maxY = Math.max(maxY, p.y);
            });
            return { minX, minY, maxX, maxY };
        }

        function distance(a, b) {
            return Math.sqrt((a.x - b.x) ** 2 + (a.y - b.y) ** 2);
        }

        function getCircleScore(points, center, radius) {
            let totalError = 0;
            points.forEach(p => {
                const dist = distance(p, center);
                totalError += Math.abs(dist - radius) / radius;
            });
            return Math.max(0, 1 - (totalError / points.length));
        }

        function getRectScore(points, bounds) {
            const width = bounds.maxX - bounds.minX;
            const height = bounds.maxY - bounds.minY;
            let cornerCount = 0;
            const corners = [
                { x: bounds.minX, y: bounds.minY },
                { x: bounds.maxX, y: bounds.minY },
                { x: bounds.maxX, y: bounds.maxY },
                { x: bounds.minX, y: bounds.maxY }
            ];
            const threshold = Math.max(width, height) * 0.15;
            corners.forEach(corner => {
                if (points.some(p => distance(p, corner) < threshold)) cornerCount++;
            });
            return cornerCount / 4;
        }

        function getTriangleScore(points, bounds) {
            // Improved triangle detection
            const width = bounds.maxX - bounds.minX;
            const height = bounds.maxY - bounds.minY;
            const cx = (bounds.minX + bounds.maxX) / 2;
            const cy = (bounds.minY + bounds.maxY) / 2;

            // Find the 3 most extreme points (potential vertices)
            let topPoint = points[0], bottomLeft = points[0], bottomRight = points[0];

            points.forEach(p => {
                if (p.y < topPoint.y) topPoint = p;
                if (p.y > bottomLeft.y || (p.y === bottomLeft.y && p.x < bottomLeft.x)) bottomLeft = p;
                if (p.y > bottomRight.y || (p.y === bottomRight.y && p.x > bottomRight.x)) bottomRight = p;
            });

            // Check for roughly 3 direction changes (corners)
            let turns = 0;
            const step = Math.max(1, Math.floor(points.length / 20));
            for (let i = step; i < points.length - step; i += step) {
                const prev = points[i - step];
                const curr = points[i];
                const next = points[Math.min(i + step, points.length - 1)];
                const angle1 = Math.atan2(curr.y - prev.y, curr.x - prev.x);
                const angle2 = Math.atan2(next.y - curr.y, next.x - curr.x);
                let diff = Math.abs(angle2 - angle1);
                if (diff > Math.PI) diff = 2 * Math.PI - diff;
                if (diff > 0.4 && diff < 2.8) turns++;
            }

            // Aspect ratio check - triangles shouldn't be too thin
            const aspect = Math.min(width, height) / Math.max(width, height);
            const aspectOk = aspect > 0.3;

            if (turns >= 2 && turns <= 6 && aspectOk) {
                return 0.65;
            }
            return 0.25;
        }

        function getLineScore(points) {
            if (points.length < 3) return 0;
            const start = points[0];
            const end = points[points.length - 1];
            const lineLen = distance(start, end);
            if (lineLen < 30) return 0;

            let totalDev = 0;
            points.forEach(p => {
                const t = Math.max(0, Math.min(1,
                    ((p.x - start.x) * (end.x - start.x) + (p.y - start.y) * (end.y - start.y)) / (lineLen * lineLen)
                ));
                const proj = { x: start.x + t * (end.x - start.x), y: start.y + t * (end.y - start.y) };
                totalDev += distance(p, proj);
            });
            return Math.max(0, 1 - (totalDev / points.length) / (lineLen * 0.1));
        }

        function detectArrowHead(points) {
            if (points.length < 10) return false;
            const last10 = points.slice(-10);
            const end = points[points.length - 1];
            const beforeEnd = points[Math.max(0, points.length - 8)];
            const mainDir = Math.atan2(end.y - beforeEnd.y, end.x - beforeEnd.x);

            // Check for splaying at end
            let hasSplay = false;
            for (let i = 1; i < last10.length - 1; i++) {
                const dir = Math.atan2(last10[i].y - end.y, last10[i].x - end.x);
                const diff = Math.abs(dir - mainDir);
                if (diff > 0.4 && diff < 2.7) hasSplay = true;
            }
            return hasSplay;
        }

        // ============================================
        // Shape Detection
        // ============================================
        function detectShape(points) {
            if (points.length < 5) return null;

            const bounds = getBounds(points);
            const center = { x: (bounds.minX + bounds.maxX) / 2, y: (bounds.minY + bounds.maxY) / 2 };
            const width = bounds.maxX - bounds.minX;
            const height = bounds.maxY - bounds.minY;
            const size = Math.max(width, height);

            if (size < 20) return null; // Too small

            // Check if closed (start near end)
            const start = points[0];
            const end = points[points.length - 1];
            const closedThreshold = size * 0.25;
            const isClosed = distance(start, end) < closedThreshold;

            if (isClosed) {
                // Analyze closed shape
                const circleScore = getCircleScore(points, center, size / 2);
                const rectScore = getRectScore(points, bounds);
                const triScore = getTriangleScore(points, bounds);

                if (circleScore > 0.7 && circleScore > rectScore && circleScore > triScore) {
                    return { type: 'circle', center, radius: size / 2, confidence: circleScore };
                }
                if (rectScore > 0.6 && rectScore > triScore) {
                    return { type: 'rectangle', bounds, center, confidence: rectScore };
                }
                if (triScore > 0.5) {
                    return { type: 'triangle', bounds, center, confidence: triScore };
                }
            } else {
                // Open shape - line or arrow
                const lineScore = getLineScore(points);
                if (lineScore > 0.7) {
                    const hasArrow = detectArrowHead(points);
                    return {
                        type: hasArrow ? 'arrow' : 'line',
                        start, end, center,
                        confidence: lineScore
                    };
                }
            }

            return null;
        }
        
        // ============================================
        // Generate Ideal Shape Points
        // ============================================
        function generateIdealShape(shape, numPoints = 60) {
            const points = [];
            
            if (shape.type === 'circle') {
                for (let i = 0; i <= numPoints; i++) {
                    const angle = (i / numPoints) * Math.PI * 2;
                    points.push({
                        x: shape.center.x + Math.cos(angle) * shape.radius,
                        y: shape.center.y + Math.sin(angle) * shape.radius
                    });
                }
            } else if (shape.type === 'rectangle') {
                const b = shape.bounds;
                const corners = [
                    { x: b.minX, y: b.minY }, { x: b.maxX, y: b.minY },
                    { x: b.maxX, y: b.maxY }, { x: b.minX, y: b.maxY },
                    { x: b.minX, y: b.minY }
                ];
                const perSide = Math.floor(numPoints / 4);
                for (let s = 0; s < 4; s++) {
                    for (let i = 0; i < perSide; i++) {
                        const t = i / perSide;
                        points.push({
                            x: corners[s].x + t * (corners[s + 1].x - corners[s].x),
                            y: corners[s].y + t * (corners[s + 1].y - corners[s].y)
                        });
                    }
                }
            } else if (shape.type === 'triangle') {
                const b = shape.bounds;
                const cx = (b.minX + b.maxX) / 2;
                const triPoints = [
                    { x: cx, y: b.minY },
                    { x: b.maxX, y: b.maxY },
                    { x: b.minX, y: b.maxY },
                    { x: cx, y: b.minY }
                ];
                const perSide = Math.floor(numPoints / 3);
                for (let s = 0; s < 3; s++) {
                    for (let i = 0; i < perSide; i++) {
                        const t = i / perSide;
                        points.push({
                            x: triPoints[s].x + t * (triPoints[s + 1].x - triPoints[s].x),
                            y: triPoints[s].y + t * (triPoints[s + 1].y - triPoints[s].y)
                        });
                    }
                }
            } else if (shape.type === 'line' || shape.type === 'arrow') {
                for (let i = 0; i <= numPoints; i++) {
                    const t = i / numPoints;
                    points.push({
                        x: shape.start.x + t * (shape.end.x - shape.start.x),
                        y: shape.start.y + t * (shape.end.y - shape.start.y)
                    });
                }
                if (shape.type === 'arrow') {
                    const angle = Math.atan2(shape.end.y - shape.start.y, shape.end.x - shape.start.x);
                    const headLen = 15;
                    points.push({ x: shape.end.x - headLen * Math.cos(angle - 0.4), y: shape.end.y - headLen * Math.sin(angle - 0.4) });
                    points.push(shape.end);
                    points.push({ x: shape.end.x - headLen * Math.cos(angle + 0.4), y: shape.end.y - headLen * Math.sin(angle + 0.4) });
                }
            } else if (shape.type === 'dot') {
                // Dot is just a single point
                points.push(shape.center);
            }
            
            return points;
        }
        
        // ============================================
        // Relationship Detection
        // ============================================
        function detectRelationships() {
            const relationships = [];
            const shapes = recognizedShapes.filter(s => Date.now() - s.createdAt < TIMING.shapeTotal);
            
            for (let i = 0; i < shapes.length; i++) {
                for (let j = i + 1; j < shapes.length; j++) {
                    const a = shapes[i];
                    const b = shapes[j];
                    
                    const rel = analyzeRelationship(a, b);
                    if (rel) {
                        relationships.push({
                            shapeA: a,
                            shapeB: b,
                            ...rel
                        });
                    }
                }
            }
            return relationships;
        }
        
        function analyzeRelationship(a, b) {
            const dist = distance(a.center, b.center);
            const aSize = getShapeSize(a);
            const bSize = getShapeSize(b);
            const combinedSize = (aSize + bSize) / 2;
            
            // Lines don't "contain" things - skip containment check for lines
            const aIsLine = a.type === 'line' || a.type === 'arrow';
            const bIsLine = b.type === 'line' || b.type === 'arrow';
            
            // Check containment (only for closed shapes)
            if (!aIsLine && !bIsLine && dist < Math.abs(aSize - bSize) * 0.5) {
                const container = aSize > bSize ? a : b;
                const contained = aSize > bSize ? b : a;
                return { type: 'contains', container, contained, dist };
            }
            
            // Check intersection/overlap - always check for lines
            if (aIsLine || bIsLine || dist < combinedSize * 0.9) {
                const intersections = findIntersectionPoints(a, b);
                if (intersections.length > 0) {
                    return { type: 'intersects', intersections, dist };
                }
                if (!aIsLine && !bIsLine) {
                    return { type: 'overlaps', dist };
                }
            }
            
            // Check adjacency (close but not overlapping)
            if (dist < combinedSize * 1.5) {
                return { type: 'adjacent', dist };
            }
            
            // Check alignment
            const dx = Math.abs(a.center.x - b.center.x);
            const dy = Math.abs(a.center.y - b.center.y);
            if (dx < 15) return { type: 'aligned-vertical', dist };
            if (dy < 15) return { type: 'aligned-horizontal', dist };
            
            return null;
        }
        
        function getShapeSize(shape) {
            if (shape.type === 'circle') return shape.radius * 2;
            if (shape.type === 'dot') return 12; // Small fixed size for dots
            if (shape.bounds) {
                return Math.max(shape.bounds.maxX - shape.bounds.minX, shape.bounds.maxY - shape.bounds.minY);
            }
            if (shape.start && shape.end) {
                return distance(shape.start, shape.end);
            }
            return 50;
        }
        
        function findIntersectionPoints(a, b) {
            const points = [];
            
            // Special case: two circles - use geometric calculation
            if (a.type === 'circle' && b.type === 'circle') {
                const d = distance(a.center, b.center);
                const r1 = a.radius;
                const r2 = b.radius;
                
                // Check if circles actually intersect
                if (d < r1 + r2 && d > Math.abs(r1 - r2)) {
                    // Calculate intersection points
                    const a2 = (r1 * r1 - r2 * r2 + d * d) / (2 * d);
                    const h = Math.sqrt(Math.max(0, r1 * r1 - a2 * a2));
                    
                    const px = a.center.x + a2 * (b.center.x - a.center.x) / d;
                    const py = a.center.y + a2 * (b.center.y - a.center.y) / d;
                    
                    const dx = h * (b.center.y - a.center.y) / d;
                    const dy = h * (b.center.x - a.center.x) / d;
                    
                    points.push({ x: px + dx, y: py - dy });
                    if (h > 1) { // Two distinct points
                        points.push({ x: px - dx, y: py + dy });
                    }
                }
                return points;
            }
            
            // General case: check line segment intersections
            const aPoints = a.idealPoints || [];
            const bPoints = b.idealPoints || [];
            
            if (aPoints.length < 4 || bPoints.length < 4) return points;
            
            const stepA = Math.max(1, Math.floor(aPoints.length / 24));
            const stepB = Math.max(1, Math.floor(bPoints.length / 24));
            
            for (let i = 0; i < aPoints.length - stepA; i += stepA) {
                const a1 = aPoints[i];
                const a2 = aPoints[i + stepA];
                
                for (let j = 0; j < bPoints.length - stepB; j += stepB) {
                    const b1 = bPoints[j];
                    const b2 = bPoints[j + stepB];
                    
                    const intersection = lineIntersection(a1, a2, b1, b2);
                    if (intersection) {
                        const tooClose = points.some(p => distance(p, intersection) < 15);
                        if (!tooClose) points.push(intersection);
                    }
                }
            }
            return points.slice(0, 4);
        }
        
        function lineIntersection(p1, p2, p3, p4) {
            const d = (p1.x - p2.x) * (p3.y - p4.y) - (p1.y - p2.y) * (p3.x - p4.x);
            if (Math.abs(d) < 0.001) return null;
            
            const t = ((p1.x - p3.x) * (p3.y - p4.y) - (p1.y - p3.y) * (p3.x - p4.x)) / d;
            const u = -((p1.x - p2.x) * (p1.y - p3.y) - (p1.y - p2.y) * (p1.x - p3.x)) / d;
            
            if (t >= 0 && t <= 1 && u >= 0 && u <= 1) {
                return {
                    x: p1.x + t * (p2.x - p1.x),
                    y: p1.y + t * (p2.y - p1.y)
                };
            }
            return null;
        }
        
        function drawRelationships() {
            const relationships = detectRelationships();
            const now = Date.now();
            
            relationships.forEach(rel => {
                const a = rel.shapeA;
                const b = rel.shapeB;
                const age = Math.max(now - a.createdAt, now - b.createdAt);
                let opacity = 1;
                // Fade with the shapes
                if (age > TIMING.shapeHold) {
                    opacity = 1 - ((age - TIMING.shapeHold) / TIMING.relationshipFade);
                }
                if (opacity <= 0) return;
                
                // Ease out for smoother fade
                const easedOpacity = opacity * opacity;
                
                // Draw connection line
                ctx.strokeStyle = `rgba(251, 191, 36, ${0.4 * easedOpacity})`; // Warm yellow
                ctx.lineWidth = 1;
                ctx.setLineDash([4, 4]);
                ctx.beginPath();
                ctx.moveTo(a.center.x, a.center.y);
                ctx.lineTo(b.center.x, b.center.y);
                ctx.stroke();
                ctx.setLineDash([]);
                
                // Draw relationship indicator at midpoint
                const midX = (a.center.x + b.center.x) / 2;
                const midY = (a.center.y + b.center.y) / 2;
                
                // Relationship-specific visuals
                if (rel.type === 'intersects' && rel.intersections) {
                    // Draw glowing intersection points
                    rel.intersections.forEach(pt => {
                        ctx.shadowColor = 'rgba(251, 191, 36, 0.8)';
                        ctx.shadowBlur = 12 * easedOpacity;
                        ctx.fillStyle = `rgba(251, 191, 36, ${0.9 * easedOpacity})`;
                        ctx.beginPath();
                        ctx.arc(pt.x, pt.y, 4, 0, Math.PI * 2);
                        ctx.fill();
                        
                        // Outer ring
                        ctx.strokeStyle = `rgba(251, 191, 36, ${0.5 * easedOpacity})`;
                        ctx.lineWidth = 1;
                        ctx.beginPath();
                        ctx.arc(pt.x, pt.y, 8, 0, Math.PI * 2);
                        ctx.stroke();
                        ctx.shadowBlur = 0;
                    });
                    
                    // Add label near first intersection
                    if (rel.intersections.length > 0) {
                        const pt = rel.intersections[0];
                        ctx.font = '500 10px "DM Sans", sans-serif';
                        ctx.textAlign = 'center';
                        ctx.fillStyle = `rgba(251, 191, 36, ${0.9 * easedOpacity})`;
                        ctx.fillText('intersects', pt.x, pt.y - 16);
                    }
                }
                
                if (rel.type === 'contains') {
                    // Draw containment brackets
                    ctx.strokeStyle = `rgba(74, 222, 128, ${0.5 * easedOpacity})`; // Green
                    ctx.lineWidth = 2;
                    const r = getShapeSize(rel.container) / 2 + 8;
                    ctx.beginPath();
                    ctx.arc(rel.container.center.x, rel.container.center.y, r, -0.3, 0.3);
                    ctx.stroke();
                    ctx.beginPath();
                    ctx.arc(rel.container.center.x, rel.container.center.y, r, Math.PI - 0.3, Math.PI + 0.3);
                    ctx.stroke();
                    
                    // Add label
                    ctx.font = '500 10px "DM Sans", sans-serif';
                    ctx.textAlign = 'center';
                    ctx.fillStyle = `rgba(74, 222, 128, ${0.8 * easedOpacity})`;
                    ctx.fillText('⊃ contains', rel.container.center.x, rel.container.center.y - r - 8);
                }
                
                if (rel.type === 'adjacent') {
                    // Draw proximity indicator
                    ctx.fillStyle = `rgba(147, 197, 253, ${0.6 * easedOpacity})`;
                    ctx.beginPath();
                    ctx.arc(midX, midY, 3, 0, Math.PI * 2);
                    ctx.fill();
                }
                
                if (rel.type === 'aligned-vertical' || rel.type === 'aligned-horizontal') {
                    // Draw alignment guide
                    ctx.strokeStyle = `rgba(167, 139, 250, ${0.4 * easedOpacity})`; // Purple
                    ctx.lineWidth = 1;
                    ctx.setLineDash([2, 4]);
                    ctx.beginPath();
                    if (rel.type === 'aligned-vertical') {
                        ctx.moveTo(a.center.x, Math.min(a.center.y, b.center.y) - 20);
                        ctx.lineTo(a.center.x, Math.max(a.center.y, b.center.y) + 20);
                    } else {
                        ctx.moveTo(Math.min(a.center.x, b.center.x) - 20, a.center.y);
                        ctx.lineTo(Math.max(a.center.x, b.center.x) + 20, a.center.y);
                    }
                    ctx.stroke();
                    ctx.setLineDash([]);
                }
                
                // Draw relationship label with word
                const labelData = {
                    'intersects': { symbol: '×', word: 'intersects' },
                    'contains': { symbol: '⊃', word: 'contains' },
                    'overlaps': { symbol: '∩', word: 'overlaps' },
                    'adjacent': { symbol: '↔', word: 'near' },
                    'aligned-vertical': { symbol: '│', word: 'aligned' },
                    'aligned-horizontal': { symbol: '─', word: 'aligned' }
                };
                
                const labelInfo = labelData[rel.type];
                if (labelInfo && rel.type !== 'intersects') { // intersects already has point markers
                    const displayText = `${labelInfo.symbol} ${labelInfo.word}`;
                    ctx.font = '500 10px "DM Sans", sans-serif';
                    ctx.textAlign = 'center';
                    ctx.textBaseline = 'middle';
                    
                    // Background pill
                    ctx.fillStyle = `rgba(10, 22, 40, ${0.85 * easedOpacity})`;
                    const metrics = ctx.measureText(displayText);
                    const pw = metrics.width + 12;
                    const ph = 18;
                    ctx.beginPath();
                    const rr = 9;
                    ctx.moveTo(midX - pw/2 + rr, midY - ph/2);
                    ctx.lineTo(midX + pw/2 - rr, midY - ph/2);
                    ctx.quadraticCurveTo(midX + pw/2, midY - ph/2, midX + pw/2, midY - ph/2 + rr);
                    ctx.lineTo(midX + pw/2, midY + ph/2 - rr);
                    ctx.quadraticCurveTo(midX + pw/2, midY + ph/2, midX + pw/2 - rr, midY + ph/2);
                    ctx.lineTo(midX - pw/2 + rr, midY + ph/2);
                    ctx.quadraticCurveTo(midX - pw/2, midY + ph/2, midX - pw/2, midY + ph/2 - rr);
                    ctx.lineTo(midX - pw/2, midY - ph/2 + rr);
                    ctx.quadraticCurveTo(midX - pw/2, midY - ph/2, midX - pw/2 + rr, midY - ph/2);
                    ctx.closePath();
                    ctx.fill();
                    
                    // Label text
                    ctx.fillStyle = `rgba(251, 191, 36, ${0.95 * easedOpacity})`;
                    ctx.fillText(displayText, midX, midY);
                }
            });
        }
        
        // ============================================
        // Raw Stroke Intersection Detection
        // ============================================
        function detectRawIntersections() {
            const now = Date.now();
            const activeStrokes = strokes.filter(s => {
                const age = now - s.startTime;
                return age < s.fadeStart + TIMING.strokeFade && s.points.length > 3;
            });
            
            const intersections = [];
            
            // Check strokes against each other
            for (let i = 0; i < activeStrokes.length; i++) {
                for (let j = i + 1; j < activeStrokes.length; j++) {
                    const pts = findStrokeIntersections(activeStrokes[i].points, activeStrokes[j].points);
                    pts.forEach(pt => {
                        intersections.push({
                            ...pt,
                            createdAt: Math.max(activeStrokes[i].startTime, activeStrokes[j].startTime)
                        });
                    });
                }
            }
            
            // Check strokes against recognized shapes
            const activeShapes = recognizedShapes.filter(s => now - s.createdAt < TIMING.shapeTotal);
            for (const stroke of activeStrokes) {
                for (const shape of activeShapes) {
                    const pts = findStrokeIntersections(stroke.points, shape.idealPoints || []);
                    pts.forEach(pt => {
                        intersections.push({
                            ...pt,
                            createdAt: Math.max(stroke.startTime, shape.createdAt)
                        });
                    });
                }
            }
            
            return intersections;
        }
        
        function findStrokeIntersections(pointsA, pointsB) {
            const results = [];
            if (pointsA.length < 5 || pointsB.length < 5) return results;
            
            // Coarser sampling to reduce noise
            const stepA = Math.max(3, Math.floor(pointsA.length / 15));
            const stepB = Math.max(3, Math.floor(pointsB.length / 15));
            
            for (let i = 0; i < pointsA.length - stepA; i += stepA) {
                const a1 = pointsA[i];
                const a2 = pointsA[Math.min(i + stepA, pointsA.length - 1)];
                
                for (let j = 0; j < pointsB.length - stepB; j += stepB) {
                    const b1 = pointsB[j];
                    const b2 = pointsB[Math.min(j + stepB, pointsB.length - 1)];
                    
                    const intersection = lineIntersection(a1, a2, b1, b2);
                    if (intersection) {
                        // Larger minimum distance to avoid clustering
                        const tooClose = results.some(p => distance(p, intersection) < 40);
                        if (!tooClose) results.push(intersection);
                    }
                }
            }
            return results.slice(0, 3); // Max 3 per pair
        }
        
        function drawRawIntersections() {
            const now = Date.now();
            const intersections = detectRawIntersections();
            
            intersections.forEach(pt => {
                const age = now - pt.createdAt;
                let opacity = 1;
                // Sync fade with strokes
                if (age > TIMING.strokeHold) {
                    opacity = 1 - ((age - TIMING.strokeHold) / TIMING.strokeFade);
                }
                if (opacity <= 0) return;
                
                // Ease out curve for smoother fade
                const easedOpacity = opacity * opacity; // Quadratic ease out
                
                // Subtle cyan glow for raw intersections
                ctx.shadowColor = 'rgba(34, 211, 238, 0.7)';
                ctx.shadowBlur = 10 * easedOpacity;
                ctx.fillStyle = `rgba(34, 211, 238, ${0.8 * easedOpacity})`;
                ctx.beginPath();
                ctx.arc(pt.x, pt.y, 3, 0, Math.PI * 2);
                ctx.fill();
                
                // Outer ring
                ctx.strokeStyle = `rgba(34, 211, 238, ${0.4 * easedOpacity})`;
                ctx.lineWidth = 1;
                ctx.beginPath();
                ctx.arc(pt.x, pt.y, 7, 0, Math.PI * 2);
                ctx.stroke();
                ctx.shadowBlur = 0;
            });
        }
        
        // ============================================
        // Canvas Setup
        // ============================================
        function resizeCanvas() {
            const dpr = window.devicePixelRatio || 1;
            const rect = canvas.getBoundingClientRect();
            canvas.width = rect.width * dpr;
            canvas.height = rect.height * dpr;
            ctx.scale(dpr, dpr);
            canvas.style.width = rect.width + 'px';
            canvas.style.height = rect.height + 'px';
        }
        
        resizeCanvas();
        window.addEventListener('resize', resizeCanvas);
        
        // ============================================
        // Drawing Functions
        // ============================================
        function drawGrid() {
            const w = canvas.width / (window.devicePixelRatio || 1);
            const h = canvas.height / (window.devicePixelRatio || 1);
            
            ctx.fillStyle = colors.bg;
            ctx.fillRect(0, 0, w, h);
            
            // Small grid
            ctx.strokeStyle = colors.grid;
            ctx.lineWidth = 0.5;
            ctx.beginPath();
            for (let x = 0; x <= w; x += 40) { ctx.moveTo(x, 0); ctx.lineTo(x, h); }
            for (let y = 0; y <= h; y += 40) { ctx.moveTo(0, y); ctx.lineTo(w, y); }
            ctx.stroke();
            
            // Large grid
            ctx.strokeStyle = colors.gridAccent;
            ctx.lineWidth = 1;
            ctx.beginPath();
            for (let x = 0; x <= w; x += 200) { ctx.moveTo(x, 0); ctx.lineTo(x, h); }
            for (let y = 0; y <= h; y += 200) { ctx.moveTo(0, y); ctx.lineTo(w, y); }
            ctx.stroke();
            
            drawGlyphs(w, h);
        }
        
        function drawGlyphs(w, h) {
            const now = Date.now();
            const timeSinceShape = now - lastShapeTime;
            const isActive = timeSinceShape < TIMING.glyphActive;
            
            // Smooth transition for active state
            let activeAmount = 0;
            if (isActive) {
                // Fade in quickly, fade out smoothly
                if (timeSinceShape < 200) {
                    activeAmount = timeSinceShape / 200;
                } else if (timeSinceShape > TIMING.glyphActive - 500) {
                    activeAmount = (TIMING.glyphActive - timeSinceShape) / 500;
                } else {
                    activeAmount = 1;
                }
            }
            
            // Pulse when idle
            if (!isActive && !isDrawing) {
                glyphPulse = Math.sin(now / 1000) * 0.15 + 0.85;
            } else {
                glyphPulse = 1;
            }
            
            // Rotate when shape detected (slow down as it fades)
            if (isActive) {
                glyphRotation += 0.02 * activeAmount;
            }
            
            const corners = [[40, 40], [w - 40, 40], [40, h - 40], [w - 40, h - 40]];
            corners.forEach(([x, y], i) => {
                ctx.save();
                ctx.translate(x, y);
                ctx.rotate(glyphRotation * (i % 2 === 0 ? 1 : -1));
                
                // Interpolate color based on active amount
                const r = Math.round(59 + (96 - 59) * activeAmount);
                const g = Math.round(130 + (165 - 130) * activeAmount);
                const b = Math.round(246 + (250 - 246) * activeAmount);
                const a = 0.25 + 0.25 * activeAmount;
                ctx.strokeStyle = `rgba(${r}, ${g}, ${b}, ${a})`;
                ctx.globalAlpha = glyphPulse;
                ctx.lineWidth = 1 + 0.5 * activeAmount;
                
                ctx.beginPath();
                ctx.moveTo(-15, 0); ctx.lineTo(15, 0);
                ctx.moveTo(0, -15); ctx.lineTo(0, 15);
                ctx.stroke();
                
                ctx.beginPath();
                ctx.arc(0, 0, 8, 0, Math.PI * 2);
                ctx.stroke();
                
                // Outer ring fades in/out smoothly
                if (activeAmount > 0) {
                    ctx.globalAlpha = glyphPulse * activeAmount;
                    ctx.beginPath();
                    ctx.arc(0, 0, 12, 0, Math.PI * 2);
                    ctx.stroke();
                }
                
                ctx.restore();
            });
            ctx.globalAlpha = 1;
        }
        
        function drawRawStrokes() {
            const now = Date.now();
            
            strokes = strokes.filter(stroke => {
                const age = now - stroke.startTime;
                if (age > stroke.fadeStart + TIMING.strokeFade) return false;
                
                let opacity = 1;
                if (age > stroke.fadeStart) {
                    opacity = 1 - ((age - stroke.fadeStart) / TIMING.strokeFade);
                }
                // Ease out for smoother fade
                const easedOpacity = opacity * opacity;
                
                if (stroke.points.length < 2) return true;
                
                ctx.shadowColor = colors.glow;
                ctx.shadowBlur = 12 * easedOpacity;
                ctx.strokeStyle = `rgba(147, 197, 253, ${0.85 * easedOpacity})`;
                ctx.lineWidth = 2.5;
                ctx.lineCap = 'round';
                ctx.lineJoin = 'round';
                
                ctx.beginPath();
                ctx.moveTo(stroke.points[0].x, stroke.points[0].y);
                for (let i = 1; i < stroke.points.length; i++) {
                    const p0 = stroke.points[i - 1];
                    const p1 = stroke.points[i];
                    ctx.quadraticCurveTo(p0.x, p0.y, (p0.x + p1.x) / 2, (p0.y + p1.y) / 2);
                }
                ctx.stroke();
                ctx.shadowBlur = 0;
                
                return true;
            });
        }
        
        function drawRecognizedShapes() {
            const now = Date.now();
            
            recognizedShapes = recognizedShapes.filter(shape => {
                const age = now - shape.createdAt;
                if (age > TIMING.shapeTotal) return false;
                
                let opacity = 1;
                if (age > TIMING.shapeHold) {
                    opacity = 1 - ((age - TIMING.shapeHold) / TIMING.shapeFade);
                }
                // Ease out for smoother fade
                const easedOpacity = opacity * opacity;
                
                const progress = Math.min(1, age / TIMING.morphDuration);
                const morphEase = 1 - Math.pow(1 - progress, 3); // Ease out cubic
                
                // Interpolate between raw and ideal points
                const points = [];
                const idealLen = shape.idealPoints.length;
                const rawLen = shape.rawPoints.length;
                
                for (let i = 0; i < idealLen; i++) {
                    const rawIdx = Math.floor((i / idealLen) * rawLen);
                    const raw = shape.rawPoints[Math.min(rawIdx, rawLen - 1)];
                    const ideal = shape.idealPoints[i];
                    points.push({
                        x: raw.x + (ideal.x - raw.x) * morphEase,
                        y: raw.y + (ideal.y - raw.y) * morphEase
                    });
                }
                
                // Draw shape
                ctx.shadowColor = 'rgba(96, 165, 250, 0.6)';
                ctx.shadowBlur = 20 * easedOpacity;
                ctx.strokeStyle = `rgba(96, 165, 250, ${0.9 * easedOpacity})`;
                ctx.lineWidth = 2.5;
                ctx.lineCap = 'round';
                ctx.lineJoin = 'round';
                
                // Special handling for dots
                if (shape.type === 'dot') {
                    ctx.fillStyle = `rgba(96, 165, 250, ${0.9 * easedOpacity})`;
                    ctx.beginPath();
                    ctx.arc(shape.center.x, shape.center.y, 5 + 2 * morphEase, 0, Math.PI * 2);
                    ctx.fill();
                    ctx.shadowBlur = 0;
                    return true;
                }
                
                ctx.beginPath();
                ctx.moveTo(points[0].x, points[0].y);
                for (let i = 1; i < points.length; i++) {
                    ctx.lineTo(points[i].x, points[i].y);
                }
                if (shape.type === 'circle' || shape.type === 'rectangle' || shape.type === 'triangle') {
                    ctx.closePath();
                }
                ctx.stroke();
                ctx.shadowBlur = 0;
                
                return true;
            });
        }
        
        function drawGhostSuggestion() {
            if (!isDrawing || !currentStroke || currentStroke.points.length < 10) return;
            
            const shape = detectShape(currentStroke.points);
            if (!shape || shape.confidence < 0.4) return;
            
            const idealPoints = generateIdealShape(shape);
            
            ctx.strokeStyle = colors.ghost;
            ctx.lineWidth = 2;
            ctx.setLineDash([8, 8]);
            ctx.lineCap = 'round';
            
            ctx.beginPath();
            ctx.moveTo(idealPoints[0].x, idealPoints[0].y);
            for (let i = 1; i < idealPoints.length; i++) {
                ctx.lineTo(idealPoints[i].x, idealPoints[i].y);
            }
            if (shape.type === 'circle' || shape.type === 'rectangle' || shape.type === 'triangle') {
                ctx.closePath();
            }
            ctx.stroke();
            ctx.setLineDash([]);
        }
        
        function drawWhisperLabels() {
            const now = Date.now();
            
            whisperLabels = whisperLabels.filter(label => {
                const age = now - label.createdAt;
                if (age > TIMING.labelTotal) return false;
                
                let opacity = 1;
                if (age < TIMING.labelFadeIn) {
                    opacity = age / TIMING.labelFadeIn;
                } else if (age > TIMING.labelFadeIn + TIMING.labelHold) {
                    opacity = 1 - ((age - TIMING.labelFadeIn - TIMING.labelHold) / TIMING.labelFade);
                }
                // Ease out for smoother fade
                const easedOpacity = opacity * opacity;
                
                const yOffset = -20 - (age / 80); // Float upward slightly slower
                
                ctx.font = '500 13px "DM Sans", sans-serif';
                ctx.textAlign = 'center';
                ctx.fillStyle = `rgba(147, 197, 253, ${0.8 * easedOpacity})`;
                ctx.fillText(label.text, label.x, label.y + yOffset);
                
                return true;
            });
        }
        
        // Cursor with glow
        let cursorPos = { x: -100, y: -100 };
        let cursorVisible = false;
        
        function drawCursor() {
            if (!cursorVisible) return;
            
            const x = cursorPos.x;
            const y = cursorPos.y;
            const size = isDrawing ? 14 : 10;
            
            const gradient = ctx.createRadialGradient(x, y, 0, x, y, 60);
            gradient.addColorStop(0, isDrawing ? 'rgba(251, 146, 60, 0.25)' : 'rgba(59, 130, 246, 0.2)');
            gradient.addColorStop(0.5, isDrawing ? 'rgba(251, 146, 60, 0.08)' : 'rgba(59, 130, 246, 0.05)');
            gradient.addColorStop(1, 'rgba(0, 0, 0, 0)');
            ctx.fillStyle = gradient;
            ctx.beginPath();
            ctx.arc(x, y, 60, 0, Math.PI * 2);
            ctx.fill();
            
            ctx.strokeStyle = isDrawing ? colors.cursor : 'rgba(147, 197, 253, 0.7)';
            ctx.lineWidth = 1.5;
            ctx.shadowColor = isDrawing ? colors.cursor : colors.glow;
            ctx.shadowBlur = 8;
            
            ctx.beginPath();
            ctx.moveTo(x - size, y); ctx.lineTo(x - 4, y);
            ctx.moveTo(x + 4, y); ctx.lineTo(x + size, y);
            ctx.moveTo(x, y - size); ctx.lineTo(x, y - 4);
            ctx.moveTo(x, y + 4); ctx.lineTo(x, y + size);
            ctx.stroke();
            ctx.shadowBlur = 0;
            
            if (isDrawing) {
                ctx.beginPath();
                ctx.arc(x, y, 3, 0, Math.PI * 2);
                ctx.fillStyle = colors.cursor;
                ctx.fill();
            }
        }
        
        // Draw ripple effects (for tap/dot feedback)
        function drawRipples() {
            const now = Date.now();
            const rippleDuration = 600;
            
            ripples = ripples.filter(ripple => {
                const age = now - ripple.createdAt;
                if (age > rippleDuration) return false;
                
                const progress = age / rippleDuration;
                const eased = 1 - Math.pow(1 - progress, 3); // ease out cubic
                const radius = ripple.maxRadius * eased;
                const opacity = 1 - progress;
                
                ctx.strokeStyle = `rgba(96, 165, 250, ${0.6 * opacity})`;
                ctx.lineWidth = 2 * (1 - progress * 0.5);
                ctx.beginPath();
                ctx.arc(ripple.x, ripple.y, radius, 0, Math.PI * 2);
                ctx.stroke();
                
                // Second ring
                if (progress > 0.15) {
                    const innerProgress = (progress - 0.15) / 0.85;
                    const innerRadius = ripple.maxRadius * 0.6 * innerProgress;
                    ctx.strokeStyle = `rgba(147, 197, 253, ${0.4 * (1 - innerProgress)})`;
                    ctx.lineWidth = 1.5 * (1 - innerProgress * 0.5);
                    ctx.beginPath();
                    ctx.arc(ripple.x, ripple.y, innerRadius, 0, Math.PI * 2);
                    ctx.stroke();
                }
                
                return true;
            });
        }
        
        // Draw particle burst (for shape recognition feedback)
        function drawParticles() {
            const now = Date.now();
            const particleDuration = 800;
            
            particles = particles.filter(p => {
                const age = now - p.createdAt;
                if (age > particleDuration) return false;
                
                const progress = age / particleDuration;
                const opacity = 1 - progress;
                
                // Update position with slight gravity
                p.x += p.vx;
                p.y += p.vy + progress * 0.5;
                p.vx *= 0.97;
                p.vy *= 0.97;
                
                // Draw particle
                const size = 3 * (1 - progress * 0.5);
                ctx.fillStyle = `rgba(251, 191, 36, ${0.8 * opacity})`;
                ctx.beginPath();
                ctx.arc(p.x, p.y, size, 0, Math.PI * 2);
                ctx.fill();
                
                return true;
            });
        }
        
        // Draw ambient floating particles
        function initAmbientParticles() {
            const w = canvas.width / (window.devicePixelRatio || 1);
            const h = canvas.height / (window.devicePixelRatio || 1);
            
            for (let i = 0; i < 15; i++) {
                ambientParticles.push({
                    x: Math.random() * w,
                    y: Math.random() * h,
                    vx: (Math.random() - 0.5) * 0.3,
                    vy: (Math.random() - 0.5) * 0.3,
                    size: 1 + Math.random() * 2,
                    phase: Math.random() * Math.PI * 2
                });
            }
        }
        initAmbientParticles();
        
        function drawAmbientParticles() {
            const now = Date.now();
            const w = canvas.width / (window.devicePixelRatio || 1);
            const h = canvas.height / (window.devicePixelRatio || 1);
            
            ambientParticles.forEach(p => {
                // Drift movement
                p.x += p.vx;
                p.y += p.vy;
                
                // Wrap around edges
                if (p.x < 0) p.x = w;
                if (p.x > w) p.x = 0;
                if (p.y < 0) p.y = h;
                if (p.y > h) p.y = 0;
                
                // Pulsing opacity
                const pulse = 0.15 + 0.1 * Math.sin(now / 2000 + p.phase);
                
                ctx.fillStyle = `rgba(147, 197, 253, ${pulse})`;
                ctx.beginPath();
                ctx.arc(p.x, p.y, p.size, 0, Math.PI * 2);
                ctx.fill();
            });
        }
        
        // Animation loop
        function animate() {
            drawGrid();
            drawAmbientParticles();
            drawRawStrokes();
            drawRawIntersections();
            drawRecognizedShapes();
            drawRelationships();
            drawGhostSuggestion();
            drawWhisperLabels();
            drawRipples();
            drawParticles();
            drawCursor();
            requestAnimationFrame(animate);
        }
        animate();
        
        // ============================================
        // Event Handlers
        // ============================================
        function getPos(e) {
            const rect = canvas.getBoundingClientRect();
            if (e.touches) {
                return { x: e.touches[0].clientX - rect.left, y: e.touches[0].clientY - rect.top };
            }
            return { x: e.clientX - rect.left, y: e.clientY - rect.top };
        }
        
        function startDraw(e) {
            e.preventDefault();
            isDrawing = true;
            hero.classList.add('drawing');
            
            const pos = getPos(e);
            currentStroke = {
                points: [pos],
                startTime: Date.now(),
                fadeStart: TIMING.strokeHold
            };
            strokes.push(currentStroke);
        }
        
        function draw(e) {
            const pos = getPos(e);
            cursorPos = pos;
            cursorVisible = true;
            
            if (!isDrawing || !currentStroke) return;
            e.preventDefault();
            currentStroke.points.push(pos);
        }
        
        function endDraw() {
            // Detect tap/dot (few points, small area)
            if (currentStroke && currentStroke.points.length >= 1 && currentStroke.points.length <= 8) {
                const bounds = getBounds(currentStroke.points);
                const size = Math.max(bounds.maxX - bounds.minX, bounds.maxY - bounds.minY);
                
                if (size < 25) {
                    // It's a tap/dot!
                    const center = currentStroke.points.length === 1 
                        ? currentStroke.points[0]
                        : { x: (bounds.minX + bounds.maxX) / 2, y: (bounds.minY + bounds.maxY) / 2 };
                    
                    // Add dot as recognized shape
                    recognizedShapes.push({
                        type: 'dot',
                        center,
                        radius: 6,
                        confidence: 1,
                        rawPoints: [...currentStroke.points],
                        idealPoints: [center],
                        createdAt: Date.now()
                    });
                    
                    // Add whisper label
                    whisperLabels.push({
                        text: 'Point',
                        x: center.x,
                        y: center.y,
                        createdAt: Date.now()
                    });
                    
                    // Add ripple effect
                    ripples.push({
                        x: center.x,
                        y: center.y,
                        createdAt: Date.now(),
                        maxRadius: 40
                    });
                    
                    lastShapeTime = Date.now();
                    currentStroke.fadeStart = 0;
                }
            }
            
            if (currentStroke && currentStroke.points.length > 5) {
                const shape = detectShape(currentStroke.points);
                
                if (shape && shape.confidence > 0.5) {
                    // Create recognized shape with ideal points
                    recognizedShapes.push({
                        ...shape,
                        rawPoints: [...currentStroke.points],
                        idealPoints: generateIdealShape(shape),
                        createdAt: Date.now()
                    });
                    
                    // Add whisper label
                    const labelText = shape.type.charAt(0).toUpperCase() + shape.type.slice(1);
                    whisperLabels.push({
                        text: labelText,
                        x: shape.center.x,
                        y: shape.center.y,
                        createdAt: Date.now()
                    });
                    
                    // Add particle burst for recognition feedback
                    for (let i = 0; i < 8; i++) {
                        const angle = (i / 8) * Math.PI * 2;
                        particles.push({
                            x: shape.center.x,
                            y: shape.center.y,
                            vx: Math.cos(angle) * (1.5 + Math.random()),
                            vy: Math.sin(angle) * (1.5 + Math.random()),
                            life: 1,
                            createdAt: Date.now()
                        });
                    }
                    
                    lastShapeTime = Date.now();
                    
                    // Hide raw stroke quickly since we're showing recognized version
                    currentStroke.fadeStart = 0;
                }
            }
            
            isDrawing = false;
            currentStroke = null;
            setTimeout(() => {
                if (!isDrawing) hero.classList.remove('drawing');
            }, 500);
        }
        
        // Event listeners
        canvas.addEventListener('mousedown', startDraw);
        canvas.addEventListener('mousemove', draw);
        canvas.addEventListener('mouseup', endDraw);
        canvas.addEventListener('mouseleave', () => { endDraw(); cursorVisible = false; });
        canvas.addEventListener('mouseenter', () => { cursorVisible = true; });
        
        canvas.addEventListener('touchstart', startDraw, { passive: false });
        canvas.addEventListener('touchmove', draw, { passive: false });
        canvas.addEventListener('touchend', endDraw);
        canvas.addEventListener('touchcancel', endDraw);

        // ============================================
        // Demo Canvas - Interactive Fish Animation
        // ============================================
        const demoCanvas = document.getElementById('demoCanvas');
        if (demoCanvas) {
            const demoCtx = demoCanvas.getContext('2d');
            const demoPrompt = document.getElementById('demoPrompt');
            const demoStatus = document.getElementById('demoStatus');
            const demoResetBtn = document.getElementById('demoResetBtn');
            const fishLabel = document.getElementById('fishLabel');
            const foodLabel = document.getElementById('foodLabel');

            // State
            let demoState = 'draw-fish'; // draw-fish, draw-food, animating
            let demoIsDrawing = false;
            let demoIsDraggingFood = false;
            let demoCurrentStroke = [];
            let demoFish = null;
            let demoFood = null;
            let demoAnimationId = null;
            let demoWigglePhase = 0;

            // High DPI support
            function setupDemoCanvas() {
                const rect = demoCanvas.getBoundingClientRect();
                const dpr = window.devicePixelRatio || 1;
                demoCanvas.width = rect.width * dpr;
                demoCanvas.height = rect.height * dpr;
                demoCtx.scale(dpr, dpr);
                demoCanvas.style.width = rect.width + 'px';
                demoCanvas.style.height = rect.height + 'px';
            }
            setupDemoCanvas();
            window.addEventListener('resize', () => {
                setupDemoCanvas();
                redrawDemo();
            });

            // Get canvas dimensions
            function getDemoCanvasSize() {
                return {
                    width: demoCanvas.getBoundingClientRect().width,
                    height: demoCanvas.getBoundingClientRect().height
                };
            }

            // Get position from event
            function getDemoPos(e) {
                const rect = demoCanvas.getBoundingClientRect();
                if (e.touches) {
                    return {
                        x: e.touches[0].clientX - rect.left,
                        y: e.touches[0].clientY - rect.top
                    };
                }
                return {
                    x: e.clientX - rect.left,
                    y: e.clientY - rect.top
                };
            }

            function demoBounds(points) {
                let minX = Infinity, minY = Infinity, maxX = -Infinity, maxY = -Infinity;
                points.forEach(p => {
                    minX = Math.min(minX, p.x);
                    minY = Math.min(minY, p.y);
                    maxX = Math.max(maxX, p.x);
                    maxY = Math.max(maxY, p.y);
                });
                return { minX, minY, maxX, maxY };
            }

            function updateDemoLabel(labelEl, pos, visible) {
                const rect = demoCanvas.getBoundingClientRect();
                labelEl.style.left = pos.x + 'px';
                labelEl.style.top = pos.y + 'px';
                labelEl.classList.toggle('visible', visible);
            }

            // Drawing handlers
            function demoStartDraw(e) {
                e.preventDefault();
                const pos = getDemoPos(e);

                // Check if clicking on food to drag it
                if (demoFood && demoState === 'animating') {
                    const dx = pos.x - demoFood.center.x;
                    const dy = pos.y - demoFood.center.y;
                    const dist = Math.sqrt(dx * dx + dy * dy);
                    if (dist < 40) {
                        demoIsDraggingFood = true;
                        demoCanvas.classList.add('grabbing');
                        return;
                    }
                }

                if (demoState === 'animating') return;
                demoIsDrawing = true;
                demoCurrentStroke = [pos];
            }

            function demoDraw(e) {
                e.preventDefault();
                const pos = getDemoPos(e);

                // Handle dragging food
                if (demoIsDraggingFood && demoFood) {
                    const dx = pos.x - demoFood.center.x;
                    const dy = pos.y - demoFood.center.y;
                    demoFood.center.x = pos.x;
                    demoFood.center.y = pos.y;
                    demoFood.points = demoFood.points.map(p => ({
                        x: p.x + dx,
                        y: p.y + dy
                    }));
                    updateDemoLabel(foodLabel, demoFood.center, true);
                    return;
                }

                if (!demoIsDrawing) return;
                demoCurrentStroke.push(pos);
                redrawDemo();

                // Draw current stroke
                if (demoCurrentStroke.length > 1) {
                    demoCtx.strokeStyle = demoState === 'draw-fish' ? 'rgba(59, 130, 246, 0.8)' : 'rgba(74, 222, 128, 0.8)';
                    demoCtx.lineWidth = 3;
                    demoCtx.lineCap = 'round';
                    demoCtx.lineJoin = 'round';
                    demoCtx.beginPath();
                    demoCtx.moveTo(demoCurrentStroke[0].x, demoCurrentStroke[0].y);
                    for (let i = 1; i < demoCurrentStroke.length; i++) {
                        demoCtx.lineTo(demoCurrentStroke[i].x, demoCurrentStroke[i].y);
                    }
                    demoCtx.stroke();
                }
            }

            function demoEndDraw() {
                if (demoIsDraggingFood) {
                    demoIsDraggingFood = false;
                    demoCanvas.classList.remove('grabbing');
                    return;
                }

                if (!demoIsDrawing) return;
                demoIsDrawing = false;

                if (demoCurrentStroke.length < 5) {
                    demoCurrentStroke = [];
                    return;
                }

                // Calculate bounds and center
                const bounds = demoBounds(demoCurrentStroke);
                const center = {
                    x: (bounds.minX + bounds.maxX) / 2,
                    y: (bounds.minY + bounds.maxY) / 2
                };

                if (demoState === 'draw-fish') {
                    demoFish = {
                        points: [...demoCurrentStroke],
                        originalPoints: [...demoCurrentStroke],
                        center: { ...center },
                        bounds: { ...bounds },
                        velocity: { x: 0, y: 0 }
                    };

                    demoPrompt.innerHTML = 'Draw some <span class="highlight">food</span>';
                    demoStatus.textContent = 'Your fish is waiting...';
                    updateDemoLabel(fishLabel, demoFish.center, true);
                    demoState = 'draw-food';

                } else if (demoState === 'draw-food') {
                    demoFood = {
                        points: [...demoCurrentStroke],
                        center: { ...center },
                        bounds: { ...bounds }
                    };

                    demoPrompt.innerHTML = '🎬 <span class="highlight">Drag the food!</span>';
                    demoStatus.textContent = 'Your fish will follow...';
                    updateDemoLabel(foodLabel, demoFood.center, true);
                    demoResetBtn.classList.add('visible');

                    demoState = 'animating';
                    startDemoAnimation();
                }

                demoCurrentStroke = [];
                redrawDemo();
            }

            // Animation
            function startDemoAnimation() {
                function animate() {
                    if (demoState !== 'animating') return;

                    demoWigglePhase += 0.15;

                    // Calculate direction to food
                    const dx = demoFood.center.x - demoFish.center.x;
                    const dy = demoFood.center.y - demoFish.center.y;
                    const dist = Math.sqrt(dx * dx + dy * dy);

                    // Move fish toward food
                    if (dist > 5) {
                        const speed = Math.min(2.5, dist * 0.05);
                        const vx = (dx / dist) * speed;
                        const vy = (dy / dist) * speed;

                        demoFish.center.x += vx;
                        demoFish.center.y += vy;

                        demoFish.originalPoints = demoFish.originalPoints.map(p => ({
                            x: p.x + vx,
                            y: p.y + vy
                        }));
                    }

                    // Apply wiggle
                    const wiggleAmount = dist < 20 ? 2 : 1.5;
                    const wiggleSpeed = dist < 20 ? 1.5 : 1;
                    demoFish.points = demoFish.originalPoints.map((p, i) => {
                        const offset = Math.sin(demoWigglePhase * wiggleSpeed + i * 0.3) * wiggleAmount;
                        const perpX = -dy / (dist || 1);
                        const perpY = dx / (dist || 1);
                        return {
                            x: p.x + perpX * offset,
                            y: p.y + perpY * offset
                        };
                    });

                    updateDemoLabel(fishLabel, demoFish.center, true);
                    redrawDemo();

                    demoAnimationId = requestAnimationFrame(animate);
                }

                demoAnimationId = requestAnimationFrame(animate);
            }

            // Redraw everything
            function redrawDemo() {
                const { width, height } = getDemoCanvasSize();
                demoCtx.clearRect(0, 0, width, height);

                // Draw grid
                demoCtx.strokeStyle = 'rgba(59, 130, 246, 0.06)';
                demoCtx.lineWidth = 1;
                const gridSize = 20;
                for (let x = 0; x < width; x += gridSize) {
                    demoCtx.beginPath();
                    demoCtx.moveTo(x, 0);
                    demoCtx.lineTo(x, height);
                    demoCtx.stroke();
                }
                for (let y = 0; y < height; y += gridSize) {
                    demoCtx.beginPath();
                    demoCtx.moveTo(0, y);
                    demoCtx.lineTo(width, y);
                    demoCtx.stroke();
                }

                // Draw food
                if (demoFood) {
                    demoCtx.strokeStyle = 'rgba(74, 222, 128, 0.8)';
                    demoCtx.lineWidth = 3;
                    demoCtx.lineCap = 'round';
                    demoCtx.lineJoin = 'round';
                    demoCtx.beginPath();
                    demoCtx.moveTo(demoFood.points[0].x, demoFood.points[0].y);
                    for (let i = 1; i < demoFood.points.length; i++) {
                        demoCtx.lineTo(demoFood.points[i].x, demoFood.points[i].y);
                    }
                    demoCtx.stroke();

                    // Draw drag hint circle
                    if (demoState === 'animating') {
                        demoCtx.strokeStyle = 'rgba(74, 222, 128, 0.3)';
                        demoCtx.lineWidth = 2;
                        demoCtx.setLineDash([4, 4]);
                        demoCtx.beginPath();
                        demoCtx.arc(demoFood.center.x, demoFood.center.y, 35, 0, Math.PI * 2);
                        demoCtx.stroke();
                        demoCtx.setLineDash([]);
                    }
                }

                // Draw fish
                if (demoFish) {
                    demoCtx.strokeStyle = 'rgba(59, 130, 246, 0.8)';
                    demoCtx.lineWidth = 3;
                    demoCtx.lineCap = 'round';
                    demoCtx.lineJoin = 'round';
                    demoCtx.beginPath();
                    demoCtx.moveTo(demoFish.points[0].x, demoFish.points[0].y);
                    for (let i = 1; i < demoFish.points.length; i++) {
                        demoCtx.lineTo(demoFish.points[i].x, demoFish.points[i].y);
                    }
                    demoCtx.stroke();
                }
            }

            // Reset
            function resetDemo() {
                if (demoAnimationId) cancelAnimationFrame(demoAnimationId);

                demoState = 'draw-fish';
                demoFish = null;
                demoFood = null;
                demoCurrentStroke = [];
                demoIsDraggingFood = false;
                demoWigglePhase = 0;

                demoPrompt.innerHTML = 'Draw a <span class="highlight">fish</span>';
                demoStatus.textContent = '';
                demoResetBtn.classList.remove('visible');
                fishLabel.classList.remove('visible');
                foodLabel.classList.remove('visible');

                redrawDemo();
            }

            // Event listeners
            demoCanvas.addEventListener('mousedown', demoStartDraw);
            demoCanvas.addEventListener('mousemove', (e) => {
                demoDraw(e);
                // Update cursor when hovering over food
                if (demoFood && demoState === 'animating' && !demoIsDraggingFood) {
                    const pos = getDemoPos(e);
                    const dx = pos.x - demoFood.center.x;
                    const dy = pos.y - demoFood.center.y;
                    const dist = Math.sqrt(dx * dx + dy * dy);
                    demoCanvas.style.cursor = dist < 40 ? 'grab' : 'crosshair';
                }
            });
            demoCanvas.addEventListener('mouseup', demoEndDraw);
            demoCanvas.addEventListener('mouseleave', demoEndDraw);

            demoCanvas.addEventListener('touchstart', demoStartDraw, { passive: false });
            demoCanvas.addEventListener('touchmove', demoDraw, { passive: false });
            demoCanvas.addEventListener('touchend', demoEndDraw);
            demoCanvas.addEventListener('touchcancel', demoEndDraw);

            demoResetBtn.addEventListener('click', resetDemo);

            // Initial draw
            redrawDemo();
        }
    </script>

    <!-- Image Modal / Lightbox -->
    <div id="imageModal" class="image-modal">
        <div class="modal-backdrop"></div>
        <div class="modal-content">
            <div class="modal-image-container">
                <img id="modalImage" src="" alt="">
            </div>
            <figcaption id="modalCaption"></figcaption>
            <div class="modal-controls">
                <div class="modal-nav-group">
                    <button class="modal-prev" aria-label="Previous image">‹</button>
                    <div class="modal-counter" id="modalCounter"></div>
                    <button class="modal-next" aria-label="Next image">›</button>
                </div>
                <button class="modal-close" aria-label="Close">&times;</button>
            </div>
        </div>
    </div>

    <script>
        // Gallery Modal / Lightbox functionality
        (function() {
            const modal = document.getElementById('imageModal');
            const modalImage = document.getElementById('modalImage');
            const modalCaption = document.getElementById('modalCaption');
            const modalCounter = document.getElementById('modalCounter');
            const closeBtn = document.querySelector('.modal-close');
            const prevBtn = document.querySelector('.modal-prev');
            const nextBtn = document.querySelector('.modal-next');
            const backdrop = document.querySelector('.modal-backdrop');

            let galleryImages = [];
            let currentIndex = 0;

            // Collect all gallery images
            function initGallery() {
                const galleryItems = document.querySelectorAll('.gallery-item');
                galleryImages = Array.from(galleryItems).map(item => ({
                    src: item.querySelector('img').src,
                    alt: item.querySelector('img').alt,
                    caption: item.querySelector('figcaption').textContent
                }));

                // Add click listeners to gallery images
                galleryItems.forEach((item, index) => {
                    const imageContainer = item.querySelector('.image');
                    imageContainer.style.cursor = 'pointer';
                    imageContainer.addEventListener('click', () => openModal(index));
                });
            }

            // Open modal at specific index
            function openModal(index) {
                currentIndex = index;
                updateModalContent();
                modal.classList.add('active');
                document.body.style.overflow = 'hidden';
            }

            // Close modal
            function closeModal() {
                modal.classList.remove('active');
                document.body.style.overflow = '';
            }

            // Update modal content
            function updateModalContent() {
                const current = galleryImages[currentIndex];
                modalImage.src = current.src;
                modalImage.alt = current.alt;
                modalCaption.textContent = current.caption;
                modalCounter.textContent = `${currentIndex + 1} / ${galleryImages.length}`;

                // Update button states
                prevBtn.disabled = currentIndex === 0;
                nextBtn.disabled = currentIndex === galleryImages.length - 1;
            }

            // Navigation
            function showPrevious() {
                if (currentIndex > 0) {
                    currentIndex--;
                    updateModalContent();
                }
            }

            function showNext() {
                if (currentIndex < galleryImages.length - 1) {
                    currentIndex++;
                    updateModalContent();
                }
            }

            // Event listeners
            closeBtn.addEventListener('click', closeModal);
            backdrop.addEventListener('click', closeModal);
            prevBtn.addEventListener('click', showPrevious);
            nextBtn.addEventListener('click', showNext);

            // Keyboard navigation
            document.addEventListener('keydown', (e) => {
                if (!modal.classList.contains('active')) return;

                if (e.key === 'Escape') closeModal();
                else if (e.key === 'ArrowLeft') showPrevious();
                else if (e.key === 'ArrowRight') showNext();
            });

            // Initialize when DOM is ready
            if (document.readyState === 'loading') {
                document.addEventListener('DOMContentLoaded', initGallery);
            } else {
                initGallery();
            }
        })();
    </script>
</body>
</html>
